---
title: "Normalization vol. 2"
author: "Kacper Kaszuba"
date: "`r Sys.Date()`"
output: 
    html_document:
        toc: true
        toc_float: 
            collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, fig.align='center')

transform_M <- function(i, df) {
    if (df[i,2] == 22) {
        return(median(df[df[,2] == 22,1]))
    } else if (df[i,2] == 23) {
        return(median(df[df[,2] == 23,1]))
    } else {
        return(median(df[df[,2] == 24,1]))
    }
}

plothist <- function(df, title='') {
    if (!is.data.frame(df)) { df <- as.data.frame(df)}
    
    # Assuming LFQ_KO is our data frame with at least 3 columns
    # Reshape the data to a long format
    df <- df %>%
      pivot_longer(cols = 1:3, names_to = "Rep", values_to = "LFQValue")
    
    # Plot all histograms on the same plot using ggplot
    ggplot(df, aes(x = LFQValue, fill = Rep)) +
      geom_histogram(alpha = 0.4, position = "identity", bins = 30) +
      labs(title = paste("Histograms of Columns", title), x = "Values", y = "Frequency") +
      theme(legend.title = element_blank())
}

ordernorm <- function(df) {
    LFQ.22 <- bestNormalize::orderNorm(df[,1])$x.t
    LFQ.23 <- bestNormalize::orderNorm(df[,2])$x.t
    LFQ.24 <- bestNormalize::orderNorm(df[,3])$x.t
    
    if (startsWith(x = colnames(df)[1], 'KO')) {
        return(data.frame(KO_TOTALS_22 = LFQ.22,
                          KO_TOTALS_23 = LFQ.23,
                          KO_TOTALS_24 = LFQ.24))
    } else {
        return(data.frame(WT_TOTALS_22 = LFQ.22,
                          WT_TOTALS_23 = LFQ.23,
                          WT_TOTALS_24 = LFQ.24))
    }
}

library(ggplot2)
library(ggpubr)
library(dplyr)
library(tidyr)
library(vsn)
```

# Load Data

```{r data loading}
lfq <- read.csv('./data/LFQ_raw_totals_imp.csv')
head(lfq)
```

# Data Preparation

```{r TOTALS data}
# Extracting only TOTALS data for knockout
LFQ_KO <- lfq %>% select(contains('KO_TOTALS')) #%>% mutate(celltype = 'KO')

# Extracting only TOTALS data for wild type
LFQ_WT <- lfq %>% select(contains('WT_TOTALS')) #%>% mutate(celltype = 'WT')
```

# Data Mining

First of all, we have to check the distribution of our data.

## Knockout

```{r}
plothist(LFQ_KO)
```

As we see on the historam of konckout columns, the data are slightly skewed to the right.

```{r}
moments::skewness(LFQ_KO)
```

The asymmetry score tells use that we need to make some transformation of our data
and confirms the conclusions drawn from the histogram. 

## Wild type

```{r}
plothist(LFQ_WT)
```

As we see on the historam of wild type columns, the data are slightly skewed to the right.

```{r}
moments::skewness(LFQ_WT[,1:3])
```

The asymmetry score tells use that we have to make some transformation of our data 
and confirms the conclusions drawn from the histogram. 

# Transformation - KO

Based on the graphic below, we will first use the natural logarithm.

<center><img src='drabinka_eng.png'></center>

## Logarithm

```{r KO logarithmic transformation}
LFQ_KO.log <- log(LFQ_KO)

plothist(LFQ_KO.log, 'Transformed by Logarithm')
```

The data distributions looks better, but they are still skewed to the right side.

```{r}
moments::skewness(LFQ_KO.log)
```

The skewness score is smaler. This is good, but check another transformation.

## Fraction $1/x$

```{r KO fraction trasnformation}
LFQ_KO.frac <- 1/LFQ_KO

plothist(LFQ_KO.frac, 'Transformed by Fraction 1/x')
```

```{r}
moments::skewness(LFQ_KO.frac)
```

As we see above the plot and scores of the skewness aren't close to normal distribution.
Our result shows us that with simple transformation we can't transform data and 
make their distribution closer to the Gaussian.

## Box-Cos Transformation

The transformation is based on the formula:

$$x' = \Bigg\{ \matrix{\frac{x^{\lambda}-1}{\lambda} & \lambda \neq0 \\
log(x) & \lambda = 0}$$

```{r KO Box-Cox}
# At first we have to calculate the lambda
lambda.22 <- car::powerTransform(LFQ_KO$KO_TOTALS_22)$lambda
lambda.23 <- car::powerTransform(LFQ_KO$KO_TOTALS_23)$lambda
lambda.24 <- car::powerTransform(LFQ_KO$KO_TOTALS_24)$lambda
cbind(lambda.22, lambda.23, lambda.24)
```

The lambda is not equal to 0, so will make the first transformation.

```{r}
LFQ_KO.boxcox.22 <- car::bcPower(LFQ_KO$KO_TOTALS_22, lambda.22)
LFQ_KO.boxcox.23 <- car::bcPower(LFQ_KO$KO_TOTALS_23, lambda.23)
LFQ_KO.boxcox.24 <- car::bcPower(LFQ_KO$KO_TOTALS_24, lambda.24)
LFQ_KO.boxcox <- data.frame(LFQ_KO.boxcox.22, LFQ_KO.boxcox.23, LFQ_KO.boxcox.24)
colnames(LFQ_KO.boxcox) <- c('KO_TOTALS_22', 'KO_TOTALS_23', 'KO_TOTALS_24')
```

```{r echo=FALSE}
plots <- ggarrange(ggplot(LFQ_KO) + geom_histogram(aes(x=KO_TOTALS_22), bins = 10, fill='red', alpha=0.4) + ylab('Frequency'),
                   ggplot(LFQ_KO) + geom_histogram(aes(x=KO_TOTALS_23), bins = 10, fill='green', alpha=0.4) + ylab(''),
                   ggplot(LFQ_KO) + geom_histogram(aes(x=KO_TOTALS_24), bins = 10, fill='blue', alpha=0.4) + ylab(''),
                   ncol=3)

annotate_figure(plots, top=text_grob('Histogram of Transformed Data with Box-Cox', 
                                     face='bold', size=13))
```

```{r}
moments::skewness(LFQ_KO.boxcox)
```

Finally we have got expected results. The distributions are similar to a normal distribution. ðŸŽ‰

# Normalization - KO

## Z-score Normalization

Z-score normalization standardizes data by subtracting the mean and dividing 
by the standard deviation. This technique transforms data into a distribution 
with a mean of 0 and a standard deviation of 1.

Formula: $\tilde{y}_{ij} = \frac{y_ij - \bar{y_j}}{\theta_j}$, where:

* $y_ij$ - value of the LFQ;
* $\bar{y_j}$ - mean of the LFQ values;
* $\theta_j$ - standard deviation of the LFQ values.

```{r KO standardization}
LFQ_KO.standard <- as.data.frame(scale(LFQ_KO))
plothist(LFQ_KO.standard, 'Transformed by Z-score')
```

```{r}
moments::skewness(LFQ_KO.standard)
```

Unfortunately the standardization didn't help to bring the distribution closer to the normal dist.

## Min-max Normalization

Min-max normalization scales data to a specified range (usually [0, 1]) by 
subtracting the minimum value and dividing by the range of values.

```{r KO min-max normalization}
min_max_norm <- function(df) {
    ret <- scale(df, center = min(df), scale = max(df) - min(df))
    return(ret)
}

LFQ_KO.minmax <- as.data.frame(lapply(LFQ_KO, function(col) min_max_norm(col)))
plothist(LFQ_KO.minmax, '- Min-Max Normalization')
```

```{r}
moments::skewness(LFQ_KO.minmax)
```

Unfortunately, again the normalization didn't help. ðŸ˜’

## Median Scaling

```{r KO median scaling}
LFQ_KO.med <- as.data.frame(DescTools::RobScale(LFQ_KO, scale = FALSE))
plothist(LFQ_KO.med, '- Median Scaling')
```

```{r}
moments::skewness(LFQ_KO.med)
```

## MAD Scaling

```{r KO mad scaling}
LFQ_KO.mad <- as.data.frame(DescTools::RobScale(LFQ_KO))
plothist(LFQ_KO.mad, '- MAD Scaling')
```

```{r}
moments::skewness(LFQ_KO.mad)
```

Bove Median and Mad Scaling didn't give us the expected result.

## Linear Regression Normalization

```{r KO lm}
LFQ_KO_long <- LFQ_KO %>%
    pivot_longer(cols = 1:3, names_to = "Rep", values_to = "LFQValue") 
    #%>% mutate(celltype = rep(c(rep('KO', 3), rep('WT', 3)), nrow(lfq)))

model <- lm(LFQValue ~ Rep, data=LFQ_KO_long)
LFQ_KO_long$normalized <- residuals(model)

LFQ_KO.lm <- LFQ_KO_long %>% 
    mutate(row = rep(1:nrow(LFQ_KO), each=3)) %>%
    reshape2::dcast(row ~ Rep, value.var='normalized') %>%
    select(-row)

plothist(LFQ_KO.lm, '- Linear Normalization')
```

```{r}
moments::skewness(LFQ_KO.lm)
```

Linear Regression also didn't give us expected result.

# Transformation - WT

Like earlier we will first use the logarithmic transformation.

## Logarithm

```{r WT logarightmic transformation}
LFQ_WT.log <- log(LFQ_WT)
plothist(LFQ_WT.log, 'Transformed by Logarithm')
```

```{r}
moments::skewness(LFQ_WT.log)
```

Unfortunantely the result isn't closer to the Gaussian distribution.

## Fraction $1/x$

```{r WT fraction transformation}
LFQ_WT.frac <- 1/LFQ_WT
plothist(LFQ_WT.frac, 'Transformed by Fraction 1/x')
```

As we can see above, the results are the same like for the knockout cell type.
With simple methods we can't make the distribution closer to Gaussian dist.


## Box-Cos Transformation

The transformation is based on the formula:

$$x' = \Bigg\{ \matrix{\frac{x^{\lambda}-1}{\lambda} & \lambda \neq0 \\
log(x) & \lambda = 0}$$

```{r WT Box-Cox}
# At first we have to calculate the lambda
lambda.22 <- car::powerTransform(LFQ_WT$WT_TOTALS_22)$lambda
lambda.23 <- car::powerTransform(LFQ_WT$WT_TOTALS_23)$lambda
lambda.24 <- car::powerTransform(LFQ_WT$WT_TOTALS_24)$lambda
cbind(lambda.22, lambda.23, lambda.24)
```

The lambda is not equal to 0, so will make the first transformation.

```{r}
LFQ_WT.boxcox.22 <- car::bcPower(LFQ_WT$WT_TOTALS_22, lambda.22)
LFQ_WT.boxcox.23 <- car::bcPower(LFQ_WT$WT_TOTALS_23, lambda.23)
LFQ_WT.boxcox.24 <- car::bcPower(LFQ_WT$WT_TOTALS_24, lambda.24)
LFQ_WT.boxcox <- data.frame(LFQ_WT.boxcox.22, LFQ_WT.boxcox.23, LFQ_WT.boxcox.24)
colnames(LFQ_WT.boxcox) <- c('WT_TOTALS_22', 'WT_TOTALS_23', 'WT_TOTALS_24')
```

```{r echo=FALSE}
plots <- ggarrange(ggplot(LFQ_WT) + geom_histogram(aes(x=WT_TOTALS_22), bins = 10, fill='red', alpha=0.4) + ylab('Frequency'),
                   ggplot(LFQ_WT) + geom_histogram(aes(x=WT_TOTALS_23), bins = 10, fill='green', alpha=0.4) + ylab(''),
                   ggplot(LFQ_WT) + geom_histogram(aes(x=WT_TOTALS_24), bins = 10, fill='blue', alpha=0.4) + ylab(''),
                   ncol=3)

annotate_figure(plots, top=text_grob('Histogram of Transformed Data with Box-Cox', 
                                     face='bold', size=13))
```

```{r}
moments::skewness(LFQ_WT.boxcox)
```

And again the Box-Cox Tranformation gave us expected result. 
The distributions are similar to a normal distribution.

# Normalization - WT

## Z-score Normalization

```{r WT standardization}
LFQ_KO.standard <- as.data.frame(scale(LFQ_KO))
plothist(LFQ_KO.standard, 'Transformed by Z-score')
```

```{r}
moments::skewness(LFQ_KO.standard)
```

Unfortunately the standardization didn't help to bring the distribution closer to the normal dist.

## Min-max Normalization

```{r WT min-max normalization}
min_max_norm <- function(df) {
    ret <- scale(df, center = min(df), scale = max(df) - min(df))
    return(ret)
}

LFQ_KO.minmax <- as.data.frame(lapply(LFQ_KO, function(col) min_max_norm(col)))
plothist(LFQ_KO.minmax, '- Min-Max Normalization')
```

```{r}
moments::skewness(LFQ_KO.minmax)
```

Unfortunately, again the normalization didn't help. ðŸ˜’

## Median Scaling

```{r WT median scaling}
LFQ_KO.med <- as.data.frame(DescTools::RobScale(LFQ_KO, scale = FALSE))
plothist(LFQ_KO.med, '- Median Scaling')
```

```{r}
moments::skewness(LFQ_KO.med)
```

## MAD Scaling

```{r WT mad scaling}
LFQ_KO.mad <- as.data.frame(DescTools::RobScale(LFQ_KO))
plothist(LFQ_KO.mad, '- MAD Scaling')
```

```{r}
moments::skewness(LFQ_KO.mad)
```

## Linear Regression Normalization

```{r WT lm}
LFQ_WT_long <- LFQ_WT %>%
    pivot_longer(cols = 1:3, names_to = "Rep", values_to = "LFQValue") 
    #%>% mutate(celltype = rep(c(rep('KO', 3), rep('WT', 3)), nrow(lfq)))

model2 <- lm(LFQValue ~ Rep, data=LFQ_WT_long)
LFQ_WT_long$normalized <- residuals(model)

LFQ_WT.lm <- LFQ_WT_long %>% 
    mutate(row = rep(1:nrow(LFQ_WT), each=3)) %>%
    reshape2::dcast(row ~ Rep, value.var='normalized') %>%
    select(-row)

plothist(LFQ_WT.lm, '- Linear Normalization')
```

```{r}
moments::skewness(LFQ_WT.lm)
```


# VSN 
VSN - Variance Stabilization Normalization

```{r vsn}
lfq_rglist<- new('RGList', list(
    R = as.matrix(LFQ_KO),
    G = as.matrix(LFQ_WT)))

LFQ_ALL.vsn <- justvsn(lfq_rglist)
```

```{r}
plothist(LFQ_ALL.vsn@assayData[['R']], 'Transformed by VSN')
```

```{r}
plothist(LFQ_ALL.vsn@assayData[['G']], 'Transformed by VSN')
```

This transformation didn't help us too. Furthermore we lost differences between 
knockout and wild type intensity.

# `bestNormalize` Package for Automatic Normalization

## KO

**Experiment 22**

```{r autonorm KO 22}
best.22 <- bestNormalize::bestNormalize(LFQ_KO$KO_TOTALS_22)
```

**Experiment 23**

```{r autonorm KO 23}
best.23 <- bestNormalize::bestNormalize(LFQ_KO$KO_TOTALS_23)
```

**Experiment 24**

```{r autonorm KO 24}
best.24 <- bestNormalize::bestNormalize(LFQ_KO$KO_TOTALS_24)
```

**Summary**

```{r echo=FALSE}
autonorm_ko <- data.frame(Exp_22=environment(best.22[["norm_stat_fn"]])[["best_idx"]],
                          Exp_23=environment(best.23[["norm_stat_fn"]])[["best_idx"]],
                          Exp_24=environment(best.24[["norm_stat_fn"]])[["best_idx"]])

rownames(autonorm_ko) <- 'Method'
autonorm_ko
```

Based on the result for each column, the best method to normalize the data is Order Quntile Normalization.

```{r}
LFQ_KO.ordernorm <- ordernorm(LFQ_KO)
plothist(LFQ_KO.ordernorm, '- Order Normalization')
```

## WT

**Experiment 22**

```{r autonorm WT 22}
best.22 <- bestNormalize::bestNormalize(LFQ_WT$WT_TOTALS_22)
```

**Experiment 23**

```{r autonorm WT 23}
best.23 <- bestNormalize::bestNormalize(LFQ_WT$WT_TOTALS_23)
```

**Experiment 24**

```{r autonorm WT 24}
best.24 <- bestNormalize::bestNormalize(LFQ_WT$WT_TOTALS_24)
```

**Summary**

```{r echo=FALSE}
autonorm_wt <- data.frame(Exp_22=environment(best.22[["norm_stat_fn"]])[["best_idx"]],
                          Exp_23=environment(best.23[["norm_stat_fn"]])[["best_idx"]],
                          Exp_24=environment(best.24[["norm_stat_fn"]])[["best_idx"]])

rownames(autonorm_wt) <- 'Method'
autonorm_wt
```

Like earlier the best method for each column is Order Quantile Normalization.

```{r}
LFQ_WT.ordernorm <- ordernorm(LFQ_WT)
plothist(LFQ_WT.ordernorm, '- Order Normalization')
```

---
# References

1. [A systematic evaluation of normalization methods in quantitative label-free proteomics](https://academic.oup.com/bib/article/19/1/1/2562889)
2. [bestNormalize R Package](https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html#the-ordered-quantile-technique)
3. [How to Normalize Data in R for my Data: Methods and Examples](https://rpubs.com/zubairishaq9/how-to-normalize-data-r-my-data)
