---
title: "Normalization vol. 2"
author: "Kacper Kaszuba"
date: "`r Sys.Date()`"
output: 
    html_document:
        css: mystyle.css
        toc: true
        toc_float: 
            collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, fig.align='center')

transform_M <- function(i, df) {
    if (df[i,2] == 22) {
        return(median(df[df[,2] == 22,1]))
    } else if (df[i,2] == 23) {
        return(median(df[df[,2] == 23,1]))
    } else {
        return(median(df[df[,2] == 24,1]))
    }
}

plothist <- function(df, title='', plot.title.and.legend=TRUE) {
    if (!is.data.frame(df)) { df <- as.data.frame(df)}
    
    # Assuming LFQ_KO is our data frame with at least 3 columns
    # Reshape the data to a long format
    df <- df %>%
      pivot_longer(cols = 1:3, names_to = "Rep", values_to = "LFQValue")
    
    # Plot all histograms on the same plot using ggplot
    if (plot.title.and.legend) {
        ret_plot <- ggplot(df, aes(x = LFQValue, fill = Rep)) +
            geom_histogram(alpha = 0.4, position = "identity", bins = 30) +
            labs(title = paste("Histograms of Columns", title), x = "Values", y = "Frequency") +
            theme(legend.title = element_blank())
    } else {
        ret_plot <- ggplot(df, aes(x = LFQValue, fill = Rep)) +
            geom_histogram(alpha = 0.4, position = "identity", bins = 30) +
            labs(x = "Values", y = "Frequency") +
            theme(legend.title = element_blank(), legend.position = 'none')
    }
    return(ret_plot)
}

ordernorm <- function(df) {
    LFQ.22 <- bestNormalize::orderNorm(df[,1])$x.t
    LFQ.23 <- bestNormalize::orderNorm(df[,2])$x.t
    LFQ.24 <- bestNormalize::orderNorm(df[,3])$x.t
    
    if (startsWith(x = colnames(df)[1], 'KO')) {
        return(data.frame(KO_TOTALS_22 = LFQ.22,
                          KO_TOTALS_23 = LFQ.23,
                          KO_TOTALS_24 = LFQ.24))
    } else {
        return(data.frame(WT_TOTALS_22 = LFQ.22,
                          WT_TOTALS_23 = LFQ.23,
                          WT_TOTALS_24 = LFQ.24))
    }
}

library(ggplot2)
library(ggpubr)
library(dplyr)
library(tidyr)
library(vsn)
source('EigenMS/EigenMS/EigenMS.R')

writeLines("td, th { padding:6px ; text-align:center} th { background-color:black ; color:white ; border:1px solid black; } td { color:black ; border:1px solid black ; text-align:center}", con = "mystyle.css")
```

# Load Data

```{r data loading}
lfq <- read.csv('./data/LFQ_raw_totals_imp.csv')
knitr::kable(head(lfq), format='html')
```

# Data Preparation

```{r TOTALS data}
# Extracting only TOTALS data for knockout
LFQ_KO <- lfq %>% select(contains('KO_TOTALS')) #%>% mutate(celltype = 'KO')

# Extracting only TOTALS data for wild type
LFQ_WT <- lfq %>% select(contains('WT_TOTALS')) #%>% mutate(celltype = 'WT')
```

# Data Mining

First of all, we have to check the distribution of our data.

## Knockout

```{r}
plothist(LFQ_KO)
```

As we see on the historam of konckout columns, the data are slightly skewed to the right.

```{r}
moments::skewness(LFQ_KO)
```

The asymmetry score tells use that we need to make some transformation of our data
and confirms the conclusions drawn from the histogram. 

## Wild type

```{r}
plothist(LFQ_WT)
```

As we see on the historam of wild type columns, the data are slightly skewed to the right.

```{r}
moments::skewness(LFQ_WT[,1:3])
```

The asymmetry score tells use that we have to make some transformation of our data 
and confirms the conclusions drawn from the histogram. 

# Transformation - KO

Based on the graphic below, we will first use the natural logarithm.

<center><img src='drabinka_eng.png'></center>

## Logarithm

```{r KO logarithmic transformation}
LFQ_KO.log <- log(LFQ_KO)

plothist(LFQ_KO.log, 'Transformed by Logarithm')
```

The data distributions looks better, but they are still skewed to the right side.

```{r}
moments::skewness(LFQ_KO.log)
```

The skewness score is smaler. This is good, but check another transformation.

## Fraction $1/x$

```{r KO fraction trasnformation}
LFQ_KO.frac <- 1/LFQ_KO

plothist(LFQ_KO.frac, 'Transformed by Fraction 1/x')
```

```{r}
moments::skewness(LFQ_KO.frac)
```

As we see above the plot and scores of the skewness aren't close to normal distribution.
Our result shows us that with simple transformation we can't transform data and 
make their distribution closer to the Gaussian.

## Box-Cos Transformation

The transformation is based on the formula:

$$x' = \Bigg\{ \matrix{\frac{x^{\lambda}-1}{\lambda} & \lambda \neq0 \\
log(x) & \lambda = 0}$$

```{r KO lambda}
# At first we have to calculate the lambda
lambda.22 <- car::powerTransform(LFQ_KO$KO_TOTALS_22)$lambda
lambda.23 <- car::powerTransform(LFQ_KO$KO_TOTALS_23)$lambda
lambda.24 <- car::powerTransform(LFQ_KO$KO_TOTALS_24)$lambda
knitr::kable(data.frame(lambda.22, lambda.23, lambda.24, row.names = 'lambda'), format = 'html')
```

<br>
The lambda is not 0, so the first transformation is performed.

```{r KO Box-Cox}
LFQ_KO.boxcox.22 <- car::bcPower(LFQ_KO$KO_TOTALS_22, lambda.22)
LFQ_KO.boxcox.23 <- car::bcPower(LFQ_KO$KO_TOTALS_23, lambda.23)
LFQ_KO.boxcox.24 <- car::bcPower(LFQ_KO$KO_TOTALS_24, lambda.24)
LFQ_KO.boxcox <- data.frame(LFQ_KO.boxcox.22, LFQ_KO.boxcox.23, LFQ_KO.boxcox.24)
colnames(LFQ_KO.boxcox) <- c('KO_TOTALS_22', 'KO_TOTALS_23', 'KO_TOTALS_24')
```

```{r echo=FALSE}
plots1 <- ggarrange(ggplot(LFQ_KO.boxcox) + 
                       geom_histogram(aes(x=KO_TOTALS_22), bins = 10, fill='red', alpha=0.4) + 
                       labs(x='Box-Cox.22',y='Frequency'),
                   ggplot(LFQ_KO.boxcox) + 
                       geom_histogram(aes(x=KO_TOTALS_23), bins = 10, fill='green', alpha=0.4) + 
                       labs(x='Box-Cox.23',y=''),
                   ggplot(LFQ_KO.boxcox) + 
                       geom_histogram(aes(x=KO_TOTALS_24), bins = 10, fill='blue', alpha=0.4) + 
                       labs(x='Box-Cox.24',y=''),
                   ncol=3)

annotate_figure(plots1, top=text_grob('Histogram of Transformed Data with Box-Cox', 
                                     face='bold', size=13))
```

```{r}
moments::skewness(LFQ_KO.boxcox)
```

Finally we have got expected results. The distributions are similar to a normal distribution. ðŸŽ‰

# Normalization - KO

## Z-score Normalization

Z-score normalization standardizes data by subtracting the mean and dividing 
by the standard deviation. This technique transforms data into a distribution 
with a mean of 0 and a standard deviation of 1.

Formula: $\tilde{y}_{ij} = \frac{y_ij - \bar{y_j}}{\theta_j}$, where:

* $y_ij$ - value of the LFQ;
* $\bar{y_j}$ - mean of the LFQ values;
* $\theta_j$ - standard deviation of the LFQ values.

```{r KO standardization}
LFQ_KO.standard <- as.data.frame(scale(LFQ_KO))
plothist(LFQ_KO.standard, 'Transformed by Z-score')
```

```{r}
moments::skewness(LFQ_KO.standard)
```

Unfortunately the standardization didn't help to bring the distribution closer to the normal dist.

## Min-max Normalization

Min-max normalization scales data to a specified range (usually [0, 1]) by 
subtracting the minimum value and dividing by the range of values.

```{r KO min-max normalization}
min_max_norm <- function(df) {
    ret <- scale(df, center = min(df), scale = max(df) - min(df))
    return(ret)
}

LFQ_KO.minmax <- as.data.frame(lapply(LFQ_KO, function(col) min_max_norm(col)))
plothist(LFQ_KO.minmax, '- Min-Max Normalization')
```

```{r}
moments::skewness(LFQ_KO.minmax)
```

Unfortunately, again the normalization didn't help. ðŸ˜’

## Median Scaling

```{r KO median scaling}
LFQ_KO.med <- as.data.frame(DescTools::RobScale(LFQ_KO, scale = FALSE))
plothist(LFQ_KO.med, '- Median Scaling')
```

```{r}
moments::skewness(LFQ_KO.med)
```

## MAD Scaling

```{r KO mad scaling}
LFQ_KO.mad <- as.data.frame(DescTools::RobScale(LFQ_KO))
plothist(LFQ_KO.mad, '- MAD Scaling')
```

```{r}
moments::skewness(LFQ_KO.mad)
```

Bove Median and Mad Scaling didn't give us the expected result.

## Linear Regression Normalization

```{r KO lm}
LFQ_KO_long <- LFQ_KO %>%
    pivot_longer(cols = 1:3, names_to = "Rep", values_to = "LFQValue") 
    #%>% mutate(celltype = rep(c(rep('KO', 3), rep('WT', 3)), nrow(lfq)))

model <- lm(LFQValue ~ Rep, data=LFQ_KO_long)
LFQ_KO_long$normalized <- residuals(model)

LFQ_KO.lm <- LFQ_KO_long %>% 
    mutate(row = rep(1:nrow(LFQ_KO), each=3)) %>%
    reshape2::dcast(row ~ Rep, value.var='normalized') %>%
    select(-row)

plothist(LFQ_KO.lm, '- Linear Normalization')
```

```{r}
moments::skewness(LFQ_KO.lm)
```

Linear Regression also didn't give us expected result.

# Transformation - WT

Like earlier we will first use the logarithmic transformation.

## Logarithm

```{r WT logarightmic transformation}
LFQ_WT.log <- log(LFQ_WT)
plothist(LFQ_WT.log, 'Transformed by Logarithm')
```

```{r}
moments::skewness(LFQ_WT.log)
```

Unfortunantely the result isn't closer to the Gaussian distribution.

## Fraction $1/x$

```{r WT fraction transformation}
LFQ_WT.frac <- 1/LFQ_WT
plothist(LFQ_WT.frac, 'Transformed by Fraction 1/x')
```

As we can see above, the results are the same like for the knockout cell type.
With simple methods we can't make the distribution closer to Gaussian dist.


## Box-Cos Transformation

The transformation is based on the formula:

$$x' = \Bigg\{ \matrix{\frac{x^{\lambda}-1}{\lambda} & \lambda \neq0 \\
log(x) & \lambda = 0}$$

```{r WT lambda}
# At first we have to calculate the lambda
lambda.22 <- car::powerTransform(LFQ_WT$WT_TOTALS_22)$lambda
lambda.23 <- car::powerTransform(LFQ_WT$WT_TOTALS_23)$lambda
lambda.24 <- car::powerTransform(LFQ_WT$WT_TOTALS_24)$lambda
knitr::kable(data.frame(lambda.22, lambda.23, lambda.24, row.names = 'lambda'), format = 'html')
```

<br>
The lambda is not 0, so the first transformation is performed.

```{r WT Box-Cox}
LFQ_WT.boxcox.22 <- car::bcPower(LFQ_WT$WT_TOTALS_22, lambda.22)
LFQ_WT.boxcox.23 <- car::bcPower(LFQ_WT$WT_TOTALS_23, lambda.23)
LFQ_WT.boxcox.24 <- car::bcPower(LFQ_WT$WT_TOTALS_24, lambda.24)
LFQ_WT.boxcox <- data.frame(LFQ_WT.boxcox.22, LFQ_WT.boxcox.23, LFQ_WT.boxcox.24)
colnames(LFQ_WT.boxcox) <- c('WT_TOTALS_22', 'WT_TOTALS_23', 'WT_TOTALS_24')
```

```{r echo=FALSE}
plots2 <- ggarrange(ggplot(LFQ_WT.boxcox) + 
                        geom_histogram(aes(x=WT_TOTALS_22), bins = 10, fill='red', alpha=0.4) + 
                        labs(x='Box-Cox.22',y='Frequency'),
                    ggplot(LFQ_WT.boxcox) + 
                        geom_histogram(aes(x=WT_TOTALS_23), bins = 10, fill='green', alpha=0.4) + 
                        labs(x='Box-Cox.23',y=''),
                    ggplot(LFQ_WT.boxcox) + 
                        geom_histogram(aes(x=WT_TOTALS_24), bins = 10, fill='blue', alpha=0.4) + 
                        labs(x='Box-Cox.24',y=''),
                   ncol=3)

annotate_figure(plots2, top=text_grob('Histogram of Transformed Data with Box-Cox', 
                                     face='bold', size=13))
```

```{r}
moments::skewness(LFQ_WT.boxcox)
```

And again the Box-Cox Tranformation gave us expected result. 
The distributions are similar to a normal distribution.

# Normalization - WT

## Z-score Normalization

```{r WT standardization}
LFQ_WT.standard <- as.data.frame(scale(LFQ_WT))
plothist(LFQ_WT.standard, 'Transformed by Z-score')
```

```{r}
moments::skewness(LFQ_KO.standard)
```

Unfortunately the standardization didn't help to bring the distribution closer to the normal dist.

## Min-max Normalization

```{r WT min-max normalization}
min_max_norm <- function(df) {
    ret <- scale(df, center = min(df), scale = max(df) - min(df))
    return(ret)
}

LFQ_WT.minmax <- as.data.frame(lapply(LFQ_WT, function(col) min_max_norm(col)))
plothist(LFQ_WT.minmax, '- Min-Max Normalization')
```

```{r}
moments::skewness(LFQ_WT.minmax)
```

Unfortunately, again the normalization didn't help. ðŸ˜’

## Median Scaling

```{r WT median scaling}
LFQ_WT.med <- as.data.frame(DescTools::RobScale(LFQ_WT, scale = FALSE))
plothist(LFQ_WT.med, '- Median Scaling')
```

```{r}
moments::skewness(LFQ_WT.med)
```

## MAD Scaling

```{r WT mad scaling}
LFQ_WT.mad <- as.data.frame(DescTools::RobScale(LFQ_WT))
plothist(LFQ_WT.mad, '- MAD Scaling')
```

```{r}
moments::skewness(LFQ_WT.mad)
```

## Linear Regression Normalization

```{r WT lm}
LFQ_WT_long <- LFQ_WT %>%
    pivot_longer(cols = 1:3, names_to = "Rep", values_to = "LFQValue") 
    #%>% mutate(celltype = rep(c(rep('WT', 3), rep('WT', 3)), nrow(lfq)))

model2 <- lm(LFQValue ~ Rep, data=LFQ_WT_long)
LFQ_WT_long$normalized <- residuals(model)

LFQ_WT.lm <- LFQ_WT_long %>% 
    mutate(row = rep(1:nrow(LFQ_WT), each=3)) %>%
    reshape2::dcast(row ~ Rep, value.var='normalized') %>%
    select(-row)

plothist(LFQ_WT.lm, '- Linear Normalization')
```

```{r}
moments::skewness(LFQ_WT.lm)
```


# VSN 
VSN - Variance Stabilization Normalization

```{r vsn}
lfq_rglist<- new('RGList', list(
    R = as.matrix(LFQ_KO),
    G = as.matrix(LFQ_WT)))

LFQ_ALL.vsn <- justvsn(lfq_rglist)
```

```{r}
plothist(LFQ_ALL.vsn@assayData[['R']], 'Transformed by VSN')
```

```{r}
plothist(LFQ_ALL.vsn@assayData[['G']], 'Transformed by VSN')
```

This transformation didn't help us too. Furthermore we lost differences between 
knockout and wild type intensity.

# `bestNormalize` Package for Automatic Normalization

## KO

**Experiment 22**

```{r autonorm KO 22}
best.22 <- bestNormalize::bestNormalize(LFQ_KO$KO_TOTALS_22)
```

**Experiment 23**

```{r autonorm KO 23}
best.23 <- bestNormalize::bestNormalize(LFQ_KO$KO_TOTALS_23)
```

**Experiment 24**

```{r autonorm KO 24}
best.24 <- bestNormalize::bestNormalize(LFQ_KO$KO_TOTALS_24)
```

**Summary**

```{r echo=FALSE}
autonorm_ko <- data.frame(Exp_22=environment(best.22[["norm_stat_fn"]])[["best_idx"]],
                          Exp_23=environment(best.23[["norm_stat_fn"]])[["best_idx"]],
                          Exp_24=environment(best.24[["norm_stat_fn"]])[["best_idx"]])

rownames(autonorm_ko) <- 'Method'
autonorm_ko
```

Based on the result for each column, the best method to normalize the data is Order Quntile Normalization.

```{r}
LFQ_KO.ordernorm <- ordernorm(LFQ_KO)
plothist(LFQ_KO.ordernorm, '- Order Normalization')
```

## WT

**Experiment 22**

```{r autonorm WT 22}
best.22 <- bestNormalize::bestNormalize(LFQ_WT$WT_TOTALS_22)
```

**Experiment 23**

```{r autonorm WT 23}
best.23 <- bestNormalize::bestNormalize(LFQ_WT$WT_TOTALS_23)
```

**Experiment 24**

```{r autonorm WT 24}
best.24 <- bestNormalize::bestNormalize(LFQ_WT$WT_TOTALS_24)
```

**Summary**

```{r echo=FALSE}
autonorm_wt <- data.frame(Exp_22=environment(best.22[["norm_stat_fn"]])[["best_idx"]],
                          Exp_23=environment(best.23[["norm_stat_fn"]])[["best_idx"]],
                          Exp_24=environment(best.24[["norm_stat_fn"]])[["best_idx"]])

rownames(autonorm_wt) <- 'Method'
autonorm_wt
```

Like earlier the best method for each column is Order Quantile Normalization.

```{r}
LFQ_WT.ordernorm <- ordernorm(LFQ_WT)
plothist(LFQ_WT.ordernorm, '- Order Normalization')
```

# EigenMS

```{r EigenMS 1, results='hide', fig.keep='all'}
protein.groups <- readr::read_tsv('data/proteinGroups.txt', show_col_types = FALSE)
protein.groups <- protein.groups %>% filter(is.na(`Only identified by site`),
                         is.na(Reverse),
                         is.na(`Potential contaminant`))

lfq.copy <- lfq
colnames(lfq.copy) <- c('KO.22', 'KO.23', 'KO.24', 'WT.22', 'WT.23', 'WT.24')

# I needed unique values for each peptide, so I create artificial names `prot_1:5905`
prot.info <- data.frame(protein.groups[,"Peptide sequences"], paste('prot_', 1:nrow(lfq), sep = ''))
LFQ.eig1 <- eig_norm1(lfq.copy, treatment = as.factor(c('KO', 'KO', 'KO', 'WT', 'WT', 'WT')), prot.info = prot.info)
```

```{r EigenMS 2, results='hide', fig.keep='all'}
# Performing eig normalization
LFQ.eig_norm <- eig_norm2(LFQ.eig1)
```

```{r boxplot EigenNorm}
par(mfcol=c(1,2))
boxplot(lfq.copy, las=2, main='Raw intensities')
boxplot(LFQ.eig_norm$norm_m, las=2, main='Normalized intensities')
```

```{r}
plothist(LFQ.eig_norm$norm_m[,1:3], '- EigenMS KO')
```

```{r}
plothist(LFQ.eig_norm$norm_m[,4:6], '- EigenMS WT')
```

```{r}
moments::skewness(LFQ.eig_norm$norm_m)
```

The eigen normalization perfectly shows the differences between cell type, but
distributions are skewed all the time.

# Comparison

<center><b style='font-size:26px'>TRANSFORMATIONS</b></center>

## Visualizations

```{r comparison trans KO, echo=FALSE}
plots_tKO <- ggarrange(ggarrange(plothist(LFQ_KO, '', FALSE) + xlab('Original'),
                                 plothist(LFQ_KO.log, '', FALSE) + labs(x='Logarithm',y=''),
                                 plothist(LFQ_KO.frac, '', FALSE) + labs(x='Fraction 1/x',y=''),
                                 nrow=1, ncol=3),
                       plots1 + ggtitle('Box-Cox'), nrow=2,ncol=1)

annotate_figure(plots_tKO, top=text_grob('Comparison of Transformation Methods - KO', 
                                     face='bold', size=13))
```

```{r comparison trans WT, echo=FALSE}
plots_tWT <- ggarrange(ggarrange(plothist(LFQ_WT, '', FALSE) + xlab('Original'),
                                 plothist(LFQ_WT.log, '', FALSE) + labs(x='Logarithm',y=''),
                                 plothist(LFQ_WT.frac, '', FALSE) + labs(x='Fraction 1/x',y=''),
                                 nrow=1, ncol=3),
                       plots2 + ggtitle('Box-Cox'), nrow=2,ncol=1)

annotate_figure(plots_tWT, top=text_grob('Comparison of Transformation Methods - WT', 
                                     face='bold', size=13))
```



<center><b style='font-size:26px'>NORMALIZATIONS</b></center>

```{r comparison KO, echo=FALSE}
plots_KO <- ggarrange(plothist(LFQ_KO, '', FALSE) + xlab('Original'),
                      plothist(LFQ_KO.standard, '', FALSE) + labs(x='Z-score', y=''), 
                      plothist(LFQ_KO.minmax, '', FALSE) + labs(x='Min-Max', y=''),
                      plothist(LFQ_KO.med, '', FALSE) + xlab('Median Scaling'),
                      plothist(LFQ_KO.mad, '', FALSE) + labs(x='MAD Scaling', y=''),
                      plothist(LFQ_KO.lm, '', FALSE) + labs(x='Linear Norm', y=''),
                      plothist(LFQ_ALL.vsn@assayData[['R']], '', FALSE) + xlab('VSN'),
                      plothist(LFQ_KO.ordernorm, '', FALSE) + labs(x='Order Quantile', y=''),
                      plothist(LFQ.eig_norm$norm_m[,1:3], '', FALSE) + labs(x='EigenMS', y=''),
                      nrow=3, ncol=3)

annotate_figure(plots_KO, top=text_grob('Comparison of Normalization Methods - KO', 
                                     face='bold', size=13))
```

```{r comparison WT, echo=FALSE}
plots_WT <- ggarrange(plothist(LFQ_WT, '', FALSE) + xlab('Original'),
                      plothist(LFQ_WT.standard, '', FALSE) + labs(x='Z-score', y=''), 
                      plothist(LFQ_WT.minmax, '', FALSE) + labs(x='Min-Max', y=''),
                      plothist(LFQ_WT.med, '', FALSE) + xlab('Median Scaling'),
                      plothist(LFQ_WT.mad, '', FALSE) + labs(x='MAD Scaling', y=''),
                      plothist(LFQ_WT.lm, '', FALSE) + labs(x='Linear Norm', y=''),
                      plothist(LFQ_ALL.vsn@assayData[['G']], '', FALSE) + xlab('VSN'),
                      plothist(LFQ_WT.ordernorm, '', FALSE) + labs(x='Order Quantile', y=''),
                      plothist(LFQ.eig_norm$norm_m[,4:6], '', FALSE) + labs(x='EigenMS', y=''),
                      nrow=3, ncol=3)

annotate_figure(plots_WT, top=text_grob('Comparison of Normalization Methods - WT', 
                                     face='bold', size=13))
```

## Statistical Metrics

**RELATIVE STANDARD DEVIATION (RSD)** - a lower RSD indicates better normalization.

```{r echo=FALSE}
rsd <- function(x) (sd(x) / mean(x)) * 100
exp <- c('KO.22', 'KO.23', 'KO.24', 'WT.22', 'WT.23', 'WT.24')

rsd_before <- apply(cbind(LFQ_KO, LFQ_WT), 2, rsd)
rsd.standard <- apply(cbind(LFQ_KO.standard,LFQ_WT.standard), 2, rsd)
rsd.minmax <- apply(cbind(LFQ_KO.minmax, LFQ_WT.minmax), 2, rsd)
rsd.med <- apply(cbind(LFQ_KO.med,LFQ_WT.med), 2, rsd)
rsd.mad <- apply(cbind(LFQ_KO.mad,LFQ_WT.mad), 2, rsd)
rsd.lm <- apply(cbind(LFQ_KO.lm,LFQ_WT.lm), 2, rsd)
rsd.vsn <- apply(cbind(LFQ_ALL.vsn@assayData[['R']],LFQ_ALL.vsn@assayData[['G']]), 2, rsd)
rsd.ordernorm <- apply(cbind(LFQ_KO.ordernorm,LFQ_WT.ordernorm), 2, rsd)
rsd.eig <- apply(LFQ.eig_norm$norm_m, 2, rsd)

rsd_all <- data.frame(exp, rsd_before, rsd.standard, rsd.minmax, rsd.med, rsd.mad, rsd.lm, rsd.vsn, rsd.ordernorm, rsd.eig)
colnames(rsd_all) <- c('Exp', 'Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 'VSN', 'OrderQuantile', 'EigenMS')
knitr::kable(rsd_all, format='html', row.names = FALSE)
```

<br>
**REPRODUCIBILITY OF BIOLOGICAL REPLICATES**

After normalization, biological replicates should group more tightly. You can 
assess this by measuring the intraclass correlation coefficient (ICC) to see if 
replicates cluster together.

A guidelines for interpretation by [Koo and Li (2016)](https://doi.org/10.1016%2Fj.jcm.2016.02.012):

* below 0.50: poor
* between 0.50 and 0.75: moderate
* between 0.75 and 0.90: good
* above 0.90: excellent


```{r include=FALSE}
icc_before <- psych::ICC(lfq)$results['ICC']
icc.standard <- psych::ICC(cbind(LFQ_KO.standard,LFQ_WT.standard))$results['ICC']
icc.minmax <- psych::ICC(cbind(LFQ_KO.standard,LFQ_WT.standard))$results['ICC']
icc.med <- psych::ICC(cbind(LFQ_KO.med,LFQ_WT.med))$results['ICC']
icc.mad <- psych::ICC(cbind(LFQ_KO.mad,LFQ_WT.mad))$results['ICC']
icc.lm <- psych::ICC(cbind(LFQ_KO.lm,LFQ_WT.lm))$results['ICC']
icc.vsn <- psych::ICC(cbind(LFQ_ALL.vsn@assayData[['R']],LFQ_ALL.vsn@assayData[['G']]))$results['ICC']
icc.ordernorm <- psych::ICC(cbind(LFQ_KO.ordernorm,LFQ_WT.ordernorm))$results['ICC']
icc.eig <- psych::ICC(LFQ.eig_norm$norm_m)$results['ICC']
```
```{r echo=FALSE}
icc_all <- data.frame(icc_before, icc.standard, icc.minmax, icc.med, icc.mad, icc.lm, icc.vsn, icc.ordernorm, icc.eig)
colnames(icc_all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 'VSN', 'OrderQuantile', 'EigenMS')
knitr::kable(icc_all, format='html')
```


# Conclusion

All normalization methods worked quite well. The skewness problem was solved 
with the Box-Cox transformation, and with transformed data we can normalize 
it with all methods.

The method chosen with `bestNormalize` gave us a perfect shape of a distribution,
but we lost the main differences between samples. In my opinion the best result
gave the **Eigen Normalization** method, because this method kept a characteristic
of a sample and this is very important in the analysis of biological data.

---
# References

1. [A systematic evaluation of normalization methods in quantitative label-free proteomics](https://academic.oup.com/bib/article/19/1/1/2562889)
2. [bestNormalize R Package](https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html#the-ordered-quantile-technique)
3. [How to Normalize Data in R for my Data: Methods and Examples](https://rpubs.com/zubairishaq9/how-to-normalize-data-r-my-data)
4. [EigenMS](https://sourceforge.net/projects/eigenms/)
5. [Normalization of peak intensities in bottom-up MS-based proteomics using singular value decomposition](https://pubmed.ncbi.nlm.nih.gov/19602524/)
