---
title: "Non-imputed Normalization"
author: "Kacper Kaszuba"
date: "`r Sys.Date()`"
output: 
    html_document:
        toc: true
        toc_float: 
            collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = 'center')

plothist <- function(df, title='', plot.title.and.legend=TRUE) {
    if (!is.data.frame(df)) { df <- as.data.frame(df)}
    
    # Assuming LFQ_KO is our data frame with at least 3 columns
    # Reshape the data to a long format
    df <- df %>%
      pivot_longer(cols = 1:3, names_to = "Rep", values_to = "LFQValue")
    
    # Plot all histograms on the same plot using ggplot
    if (plot.title.and.legend) {
        ret_plot <- ggplot(df, aes(x = LFQValue, fill = Rep)) +
            geom_histogram(alpha = 0.4, position = "identity", bins = 30) +
            labs(title = title, x = "Values", y = "Frequency") +
            theme(legend.title = element_blank())
    } else {
        ret_plot <- ggplot(df, aes(x = LFQValue, fill = Rep)) +
            geom_histogram(alpha = 0.4, position = "identity", bins = 30) +
            labs(x = "Values", y = "Frequency") +
            theme(legend.title = element_blank(), legend.position = 'none')
    }
    return(ret_plot)
}

plotoneviolin <- function(object, xlab, fill = 'green', colour='black') {
    object <- as.data.frame(object) %>%
        pivot_longer(1, names_to = 'Method', values_to = 'LFQ_CV')
    
    ggplot(data=object, aes(x=Method, y=LFQ_CV)) +
        geom_boxplot(width=0.3, fill=fill, colour=colour)+
        geom_violin(alpha=0.4, fill=fill, colour=colour)+
        labs(title='',x=xlab,y='LFQ CV[%]')+
        theme(legend.position = 'none', axis.text.x=element_blank(), axis.ticks.x=element_blank(),
              panel.background = element_rect(fill='white', colour = 'grey'),
              panel.grid = element_line(colour = 'grey'))
}


library(ggplot2)
library(ggpubr)
library(dplyr)
library(tidyr)
options(scipen=123)
```

# Load Data

```{r load data}
lfq <- read.csv('./data/nonimputed_lfq.csv')
```

<center>
```{r echo=FALSE}
DT::datatable(lfq)
```
</center>

# Data Preprocessing

```{r extracting data}
LFQ_KO <- lfq %>%
    select(contains('KO'))

LFQ_WT <- lfq %>%
    select(contains('WT'))
```

# Data Mining

```{r plotting hists}
ggarrange(
    plothist(LFQ_KO, '- KO'),
    plothist(LFQ_WT, '- WT'),
    nrow=2,ncol=1
)
```

```{r skewnes nonimputed}
# skewness of nonimputed data
moments::skewness(lfq, na.rm = TRUE)
```

```{r skewnes imputed}
# skewnes of imputed data
lfq.imp <- read.csv('./data/LFQ_raw_totals_imp.csv')
moments::skewness(lfq.imp)
```


The imputation doesn't have influence on skewness of data. ðŸ•º Furthermore the 
imputation fixed it in some level, because the skewness coeff is lower for imputed data.


# `protti` library

```{r}
library(protti)
```

## Data Preprocessing

Thanks to Mateusz, we had data almost ready for analysis with `protti`, but there
were some important columns missing. So, based on Matis' code, I transformed and
selected features based on 
[The Input Preparation Workflow](https://jpquast.github.io/protti/articles/input_preparation_workflow.html) 
from the `protti` documentation.

```{r preprocessing data for protti}
proteingroups <- readr::read_tsv("data/proteinGroups.txt", show_col_types = FALSE)

proteingroups <- proteingroups %>% filter(is.na(`Only identified by site`),
                         is.na(Reverse),
                         is.na(`Potential contaminant`))

to_protti <- proteingroups %>%
    select(`Protein IDs`, `Peptide sequences`, 
           contains('LFQ intensity') & contains('TOTALS') & (ends_with('22') | ends_with('23') | ends_with('24'))) %>%
    mutate(`Protein IDs`= paste('prot_', 1:nrow(proteingroups), sep='')) %>%
    pivot_longer(3:8, names_to = 'Sample', values_to = 'Intensity')%>%
    mutate(Sample = gsub('LFQ intensity ', '', Sample)) %>%
    mutate(Sample = gsub('_TOTALS_', '_', Sample)) %>%
    separate(col =  Sample, into = c("celltype","rep"), sep = "_", remove = F) %>%
    mutate(Condition = ifelse(celltype == 'KO', 'treated', 'control'),
           Intensity = ifelse(Intensity == 0, NA, log2(Intensity))) %>% 
    select(Sample, `Protein IDs`, `Peptide sequences`, Condition, Intensity) 
```

<center>
```{r echo=FALSE}
DT::datatable(head(to_protti), rownames = FALSE)
```
</center>

<br>**We can do most of the calculations from the `protti` library with data structured as above.**

## Normalization

The `protti` library gives us fxn to normalise data, but the function has only 
median method which is the original intensity minus the run median plus the global median. 

```{r normalisation}
normalised_data <- to_protti %>%
    protti::normalise(sample = Sample,
              intensity_log2 = Intensity,
              method = 'median')
```

```{r echo=FALSE}
ggarrange(
    ggplot(data=normalised_data[normalised_data$Sample %in% c('KO_22', 'KO_23', 'KO_24'),]) + 
        geom_histogram(aes(x=normalised_intensity_log2, fill = Sample),bins=20, alpha=0.5, position = "identity") +
        labs(title='Normalised LFQ Intensity - KO', x=' Normalised LFQ Values'),
    ggplot(data=normalised_data[normalised_data$Sample %in% c('WT_22', 'WT_23', 'WT_24'),]) + 
        geom_histogram(aes(x=normalised_intensity_log2, fill = Sample),bins=20, alpha=0.5, position = "identity") +
        labs(title='Normalised LFQ Intensity - WT', x=' Normalised LFQ Values'),
    nrow=2,ncol=1
)
```


```{r echo=FALSE}
ggplot(data=normalised_data, mapping=aes(x=Sample, y = normalised_intensity_log2, fill=Sample)) + 
    geom_boxplot(width=0.2)+
    geom_violin(alpha=0.4)+
    theme(legend.position = 'none') +
    labs(title='Violin plot of Normalised LFQ Intensity', y=' Normalised LFQ Values', x='')
```

```{r skewness after normalisation}
normalised_data %>%
    group_by(Sample) %>%
    summarise(skewness = moments::skewness(normalised_intensity_log2, na.rm=TRUE))
```


## Imputation

The library gives us 2 methods for imputation:

1. `method = "ludovic"`, MNAR missingness is sampled from a normal distribution 
around a value that is three lower (log2) than the lowest intensity value recorded
for the precursor/peptide and that has a spread of the mean standard deviation for the precursor/peptide.
2. `method = "noise"`, MNAR missingness is sampled from a normal distribution around 
the mean noise for the precursor/peptide and that has a spread of the mean standard 
deviation (from each condition) for the precursor/peptide.

Before imputation, we have assign the missigness rows in our data. We can do that
with [`assign_missingess()`](https://jpquast.github.io/protti/reference/assign_missingness.html)
fxn from `protti` library. This fxn returns a inputed dataframe extended by:

1. the `comparison` column contains the comparison name for the specific treatment/reference pair.
2. the `missingness` column reports the type of missingness.

Types of missingness:

* `"complete"`: No missing values for every replicate of this reference/treatment 
pair for the specific grouping variable.
* `"MNAR"`: Missing not at random. All replicates of either the reference or 
treatment condition have missing values for the specific grouping variable.
* `"MAR"`: Missing at random. At least n-1 replicates have missing values for the 
reference/treatment pair for the specific grouping varible.
* `NA`: The comparison is not complete enough to fall into any other category. 
It will not be imputed if imputation is performed.

```{r handle with missing data}
# Assigning missing rows
data_missing <- normalised_data %>%
    assign_missingness(sample=Sample,
                       condition = Condition,
                       grouping = `Protein IDs`,
                       intensity = normalised_intensity_log2,
                       ref_condition = 'all')
```

<center>**Summary of assigning missigness rows**</center>

```{r echo=FALSE}
DT::datatable(data.frame(MNAR = length(which(data_missing$missingness == 'MNAR')),
                         MAR = length(which(data_missing$missingness == 'MAR')),
                         complete = length(which(data_missing$missingness == 'complete')),
                         'NA' = sum(is.na(data_missing$missingness)), row.names = 'Entries'),
              options=list(searching=FALSE, paging=FALSE, info=FALSE))
```

We can see that in the data there is still a lot of missing rows.

```{r echo=FALSE}
DT::datatable(data_missing, rownames = FALSE, options=list(pageLength=6))
```



### Method `ludovic`

```{r ludovic method}
ludovic_imp <- impute(
    data_missing,
    sample = Sample,
    grouping = `Protein IDs`,
    intensity_log2 = normalised_intensity_log2,
    condition = Condition,
    comparison = comparison,
    missingness = missingness,
    method = 'ludovic',
    skip_log2_transform_error = TRUE
)
```


The imputation is done, but we have too much missing data in the column and the 
fxn can't impute a value there. The conclusion is that we still have a lot of 
missing values for peptides that didn't have an LFQ value.

### Method `noise`

```{r noise method}
noise_imp <- impute(
    data_missing,
    sample = Sample,
    grouping = `Protein IDs`,
    intensity_log2 = normalised_intensity_log2,
    condition = Condition,
    comparison = comparison,
    missingness = missingness,
    method = 'noise',
    skip_log2_transform_error = TRUE
)
```

### Visualizations

```{r echo=FALSE}
ludovic_to_plot_KO <- ludovic_imp[ludovic_imp$Sample %in% c('KO_22', 'KO_23', 'KO_24'), c("Sample", "imputed_intensity")] 
ludovic_to_plot_WT <- ludovic_imp[ludovic_imp$Sample %in% c('WT_22', 'WT_23', 'WT_24'), c("Sample", "imputed_intensity")] 

noise_to_plot_KO <- noise_imp[noise_imp$Sample %in% c('KO_22', 'KO_23', 'KO_24'), c("Sample", "imputed_intensity")] 
noise_to_plot_WT <- noise_imp[noise_imp$Sample %in% c('WT_22', 'WT_23', 'WT_24'), c("Sample", "imputed_intensity")] 

ggarrange(
    plothist(LFQ_KO, 'Non-Imputed Values') + theme(legend.position = 'none') + xlab('KO'),
    plothist(LFQ_WT, plot.title.and.legend = FALSE) + labs(title='', x='WT'),
    ggplot(data=ludovic_to_plot_KO)+
        geom_histogram(aes(x=imputed_intensity,fill=Sample), alpha=0.4, bins=20, position = 'identity')+
        labs(title='Imputed Values with Ludovic Method', x='KO', y='Frequency')+theme(legend.position = 'none'),
    ggplot(data=ludovic_to_plot_WT)+
        geom_histogram(aes(x=imputed_intensity,fill=Sample), alpha=0.4, bins=20, position = 'identity')+
        labs(title='', x='WT', y='')+theme(legend.position = 'none'),
    ggplot(data=noise_to_plot_KO)+
        geom_histogram(aes(x=imputed_intensity,fill=Sample), alpha=0.4, bins=20, position = 'identity')+
        labs(title='Imputed Values with Noise Method', x='KO', y='Frequency')+theme(legend.position = 'none'),
    ggplot(data=noise_to_plot_WT)+
        geom_histogram(aes(x=imputed_intensity,fill=Sample), alpha=0.4, bins=20, position = 'identity')+
        labs(title='', x='WT', y='')+theme(legend.position = 'none'),
    nrow=3, ncol=2
)
```

```{r skewness after imp by group, echo=FALSE}
df_to_skew <- data.frame(ludovic_imp$Sample,ludovic_imp$imputed_intensity, noise_imp$imputed_intensity)
colnames(df_to_skew) <- c('sample', 'ludovic', 'noise')

skew <- df_to_skew %>%
    group_by(sample) %>%
    summarise(skewness_ludovic = moments::skewness(ludovic, na.rm=TRUE),
              skewness_noise = moments::skewness(noise, na.rm=TRUE))

DT::datatable(skew, rownames = FALSE, options=list(searching=FALSE, paging=FALSE, info=FALSE))
```

Ludovic imputation method has less skewness coefficient. Important is that both 
imputation methods decreases the skewness coefficient.

### Statistics

**DIFFERENTIAL ABUNDANCE AND SIGNIFICANCE**

In the tutorial, they calculating Differential Abundance and Significance using 
data prepared like above. We can run calculations on missing or imputed data.
Like in tutorial we are using the moderate t-test to calculate this statistic.

All methods:

1. `t-test` - Welch test.
2. `t-test_mean_sd` - Welch test on means, standard deviations and number of replicates.
3. `moderated_t-test` - a moderated t-test based on the `limma` package
4. `proDA` - can be used to infer means across samples based on a probabilistic 
dropout model. This eliminates the need for data imputation since missing values 
are inferred from the model.

```{r DAAS t-test}
result <- ludovic_imp %>%
    calculate_diff_abundance(sample=Sample,
                             condition = Condition,
                             grouping = `Protein IDs`,
                             intensity_log2 = imputed_intensity,
                             missingness = missingness,
                             comparison = comparison,
                             filter_NA_missingness = TRUE,
                             method = 'moderated_t-test')
```

The result can be visualised with the volcano plot, which can also be interactive, 
but we can't change it to suit our needs. Below is a manually generated volcano plot
showing the significant proteins split by type of missigness, which also has an 
influence on the type of imputation.

```{r echo=FALSE}
result$significance  <- ifelse(result$adj_pval < 0.05, TRUE, FALSE)

ggplot(data=result, aes(x=diff, y=-log10(pval), colour=missingness , fill=significance, shape=significance)) + 
    geom_point(size=ifelse(result$significance == TRUE, 3, 1)) +
    geom_vline(xintercept=c(-1, 1), linetype = 2) +
    geom_hline(data=subset(result, significance  == TRUE), aes(yintercept=-log10(max(pval))-0.05), linetype=2) +
    #geom_text(data=subset(result, significance == TRUE), aes(label=`Protein IDs`), vjust=-1.1, hjust=0.5, size=2) +
    labs(title='Volcano plot for Ludovic Imputed Data', x='log2(fold change)')
```

```{r DAAS t-test_mean_sd}
ludovic_imp$mean <- ifelse(ludovic_imp$Condition == 'control', 
                           mean(ludovic_imp$imputed_intensity[which(ludovic_imp$Condition == 'control')], na.rm = TRUE),
                           mean(ludovic_imp$imputed_intensity[which(ludovic_imp$Condition == 'treated')], na.rm = TRUE))

ludovic_imp$sd <- ifelse(ludovic_imp$Condition == 'control', 
                           sd(ludovic_imp$imputed_intensity[which(ludovic_imp$Condition == 'control')], na.rm = TRUE),
                           sd(ludovic_imp$imputed_intensity[which(ludovic_imp$Condition == 'treated')], na.rm = TRUE))

ludovic_imp$n_samples <- ifelse(ludovic_imp$Condition == 'control', 
                           length(ludovic_imp$imputed_intensity[which(ludovic_imp$Condition == 'control')]),
                           length(ludovic_imp$imputed_intensity[which(ludovic_imp$Condition == 'treated')]))

result <- ludovic_imp %>%
    calculate_diff_abundance(sample=Sample,
                             condition = Condition,
                             grouping = `Protein IDs`,
                             intensity_log2 = imputed_intensity,
                             missingness = missingness,
                             comparison = comparison,
                             filter_NA_missingness = TRUE,
                             method = 't-test_mean_sd',
                             mean = mean,
                             sd = sd,
                             n_samples = n_samples,
                             ref_condition = 'all')
```

The volcano plot look strange for the result of the DAAS with `t-test_mean_sd`.

**CORELATION OF VARIATION**

```{r echo=FALSE}
cv <- function(x) (sd(x) / mean(x)) * 100

# extract data with values for all samples
all_imp <- as.data.frame(ludovic_imp)[which(ludovic_imp$`Protein IDs` %in% result$`Protein IDs`),]

lfq_wider <- all_imp %>%
    select(Sample, imputed_intensity) %>%
    pivot_wider(names_from = Sample, values_from = imputed_intensity, values_fn = list) %>%
    unnest(cols = c(KO_22, KO_23, KO_24, WT_22, WT_23, WT_24))

lfq_imp_KO <- lfq_wider %>% select(contains('KO'))
lfq_imp_WT <- lfq_wider %>% select(contains('WT'))

cv_imp_KO <- apply(2^lfq_imp_KO, 1, cv)
cv_imp_WT <- apply(2^lfq_imp_WT, 1, cv)
cv_all <- as.data.frame(cbind(KO=cv_imp_KO, WT=cv_imp_WT))
```

```{r echo=FALSE}
cv_to_plot <- cv_all %>%
    pivot_longer(everything(), names_to = 'Sample', values_to = 'LFQ_CV')

ggplot(data=cv_to_plot, aes(x=Sample, y=LFQ_CV, fill=Sample))+
    geom_boxplot(width=0.4)+
    geom_violin(alpha=0.5)+
    labs(title='Distribution of LFQ CV',x='', y='LFQ CV [%]')+
    theme(plot.title = element_text(hjust=0.5), legend.position = 'none')
```

<center>
```{r echo=FALSE}
base_cv_stats <- cv_to_plot %>%
    group_by(Sample) %>%
    summarise(mean=mean(LFQ_CV), median=median(LFQ_CV), sd=sd(LFQ_CV))

DT::datatable(base_cv_stats, rownames = FALSE, options=list(searching=FALSE, paging=FALSE, info=FALSE))
```
</center>

# Interesting Observation

When we run code bellow, we have data frame with 5138 rows.

```{r}
lfq_long <- read.csv('./data/LFQ_long_raw.csv')


lfq_long %>%
    select(Sample) %>%
    filter(grepl('TOTALS', Sample) & (grepl('22', Sample) | grepl('23', Sample) | grepl('24', Sample)))%>%
    group_by(Sample) %>%
    summarise(test = length(Sample))
```

But, when you read the original data from `proteingroups.txt` and make some basic 
filtration we have 5905 rows.