---
title: "Comparison"
author: "Kacper Kaszuba"
date: "`r Sys.Date()`"
output: 
    html_document:
        toc: true
        toc_float: 
            collapsed: false
        code_download: true
fontsize: 14pt
css: document_style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = "center", fig.keep = "all", echo=FALSE)

if (!require("randomForest", quietly = TRUE))
    install.packages("randomForest")
if (!require("caret", quietly = TRUE))
    install.packages("caret")
if (!require('pcaMethods', quietly = TRUE))
    install.packages('pcaMethods')
if (!require('DT', quietly = TRUE))
    install.packages('DT')
if (!require('ggpubr', quietly = TRUE))
    install.packages('ggpubr')
if (!require('ggplot2', quietly = TRUE))
    install.packages('ggplot2')
if (!require('dplyr', quietly = TRUE))
    install.packages('dplyr')
if (!require('tidyr', quietly = TRUE))
    install.packages('tidyr')
if (!require('vsn', quietly = TRUE))
    install.packages('vsn')

show_cv_table <- function(df) {
    DT::datatable(df, rownames = FALSE, 
              options=list(searching=FALSE, paging=FALSE, info=FALSE)) |>
    DT::formatStyle(columns = c('mean_KO', 'mean_WT', 'median_KO', 'median_WT'), 
                    background = DT::styleInterval(c(0, 25), c('white','lightgreen', 'white'))) |>
    DT::formatStyle(columns = 'method', fontWeight = 'bold')
}

assign_missing <- function(protein.ids, condition, lfq_intensity) {
    # verification of the fxn assumptions
    if (!is.factor(condition)) {rlang::abort('The condition are not factor!')}

    # occurences of protein IDs per condition
    occur <- dplyr::tibble(protein.ids) |>
        dplyr::group_by(protein.ids) |>
        dplyr::summarise(len = length(protein.ids)) |>
        dplyr::distinct(len)

    # occurences of condition
    occur_per_cond <- dplyr::tibble(protein.ids, condition) |>
        dplyr::group_by(protein.ids, condition) |>
        dplyr::summarise(len = length(protein.ids))
    occur_per_cond <- unique(occur_per_cond$len)

    if (length(protein.ids)/occur != length(unique(protein.ids))) {rlang::abort('The protein IDs are not unique!')}
    if (!is.numeric(lfq_intensity)) {rlang::abort('The lfq intensities are not numeric!')}

    df <- data.frame('prot.IDs'=protein.ids, 'condition'=condition, 'lfq'=lfq_intensity)

    number_missing <- df |>
        dplyr::group_by(prot.IDs, condition) |>
        dplyr::summarise(no_NAs = sum(is.na(lfq)))

    missingness <- purrr::map(number_missing$no_NAs, function(no) {
            if (no == occur_per_cond) {
              rep('all_NA', occur_per_cond)
            } else if (no == occur_per_cond-1) {
              rep('MNAR', occur_per_cond)
            } else if (no < occur_per_cond-1 & no != 0) {
              rep('MAR', occur_per_cond)
            } else {
              rep('complete', occur_per_cond)
            }
        })
    missingness <- data.frame(do.call(c,missingness))

    prot.id.miss <- purrr::map(unique(number_missing$prot.IDs), function(id) rep(id, occur))
    prot.id.miss <- data.frame(do.call(c,prot.id.miss))

    df_miss <- data.frame(prot.id.miss, missingness)
    colnames(df_miss) <- c('prot.IDs', 'missingness')


    missingness <- lapply(unique(protein.ids), function(id) df_miss[which(df_miss$prot.IDs == id),'missingness'])
    df$missingness <- do.call(c,missingness)

    ret_list <- list(df = df, missingness=df$missingness)
}

ttest <- function(df, grp1, grp2){ 
  x = df[grp1]
  y = df[grp2]
  x = as.numeric((x))
  y = as.numeric((y))
  results = t.test(x,y, 
                   alternative = 'two.sided', #one-sided: 'greater' is x > y
                   paired = T,
                   na.action=na.omit)
  results$p.value
}

p.cutoff = 0.05 #set p value cutoff
FC.cutoff = 1 # set fold change cutoff

plothist <- function(df, title='', plot.title.and.legend=TRUE) {
    if (!is.data.frame(df)) { df <- as.data.frame(df)}
    
    # Assuming LFQ_KO is our data frame with at least 3 columns
    # Reshape the data to a long format
    df <- df %>%
        pivot_longer(cols = 1:ncol(df), names_to = "Rep", values_to = "LFQValue")
    
    # Plot all histograms on the same plot using ggplot
    if (plot.title.and.legend) {
        ret_plot <- ggplot(df, aes(x = LFQValue, fill = Rep)) +
            geom_histogram(alpha = 0.4, position = "identity", bins = 30) +
            labs(title = paste(title), x = "Values", y = "Frequency") +
            theme(legend.title = element_blank())
    } else {
        ret_plot <- ggplot(df, aes(x = LFQValue, fill = Rep)) +
            geom_histogram(alpha = 0.4, position = "identity", bins = 30) +
            labs(x = "Values", y = "Frequency") +
            theme(legend.title = element_blank(), legend.position = 'none')
    }
    return(ret_plot)
}

plotoneviolin <- function(object, title='') {
    object <- as.data.frame(object) %>%
        pivot_longer(everything(), names_to = 'Sample', values_to = 'LFQ_CV')
    
    ggplot(data=object, aes(x=Sample, y=LFQ_CV, fill=Sample))+
        geom_boxplot(width=0.2)+
        geom_violin(alpha=0.4)+
        labs(title=title,x=NULL,y='LFQ CV[%]')+
        theme(legend.position = 'none',
              panel.background = element_rect(fill='white', colour = 'grey'),
              panel.grid = element_line(colour = 'grey'))
}

library(ggplot2)
library(ggpubr)
library(dplyr)
library(tidyr)
library(vsn)
library(protti)
source('EigenMS/EigenMS/EigenMS.R')

writeLines('table {
  width: 100%;
  border-collapse: collapse;
}
th, td {
  width: 14.28%; /* Distribute width equally (100% / 7 columns) */
  border: 1px #f2f2f2;
  text-align: center; /* Center the text */
  padding: 8px;
}
th {
  background-color: #f2f2f2;
  border: 0px;
}
body {
    text-align: justify
}', con = 'document_style.css')
```

# Introduction

The results below shows the comparison of several normalization and imputation methods.
We decided to use 4 imputation methods:

1. Shifted Distribution based on the data;
2. Imputation using the Random Forest algorithm;
3. Imputation using the k-Nearest Neighbors algorithm;
4. Imputation using the ludovic method.

and 8 normalization methods:

1. Z-Score Normalization
2. Min-Max Normalization
3. Median Scaling
4. MAD (Median Absolute Deviation) Scaling
5. Linear Regression Normalization
6. VSN (Variance Stabilization Normalization)
7. EigenMS

We used the following parameters for the K-Nearest Neighbors algorithm (bold row):

| k | RMSE | Rsquared | MAE |
|:--:|:--:|:--:|:--:|
| **5** | **560141411** | **0.5764903** | **83499878** |
| 7 | 586141572 | 0.5110148 | 89345187 |
| 9 | 615649021 | 0.4549313 | 93976048 |

For the Random Forest algorithm we used the Grid Search method to find the best hyperparameters.
The table belows shows the output of the top 10 models.

<center><b style='font-size:20px'>GRID SEARCH FOR RANDOM FOREST MODEL</b></center>
<br>

|     | num.trees | mtry | min.node.size | replace | sample.fraction |        rmse         | perc_gain |
|:---:|:---------:|:----:|:-------------:|:-------:|:---------------:|:-------------------:|:---------:|
|**1**|  **100**  | **4**|         **5** |**false**|         **0.8** |  **36184321.6466**  | **2.8631**|
|  2  |       500 |    4 |             5 | false   |            0.8  |    367032968.8757   |     1.4713|
|  3  |       300 |    4 |             5 | false   |            0.8  |    367963900.30284  |     1.2214|
|  4  |       500 |    4 |             1 | true    |            0.8  |    370084110.1524   |     0.6523|
|  5  |       500 |    4 |            10 | false   |            0.8  |    370380581.2619   |     0.5727|
|  6  |       300 |    4 |             1 | true    |           0.63  |    371446664.9562   |     0.2865|
|  7  |       100 |    4 |             5 | false   |            0.8  |    372131148.9947   |     0.1027|
|  8  |       300 |    4 |            10 | false   |            0.8  |    372169999.3198   |     0.0923|
|  9  |       300 |    4 |             4 | true    |           0.63  |    372767670.8139   |    -0.0681|
| 10  |       300 |    4 |             3 | true    |            0.8  |    373586116.5828   |    -0.2878|


Base on the output of the grid search, we can see that there is better model than default and 
it is random forest model with the following hyperparameters:

* num.trees = 100
* mtry = 4
* min.node.size = 5
* replace = FALSE
* sample.fraction = 0.80

We train Linear Regression Model which, after the stepwise selection, has the following formula:

$$LFQvalue = 8631815 \cdot no.proteins -18172773 \cdot peptides + 1794237 \cdot
seq.coverage +\\ + 807933 \cdot mol.weight.kDa + 430143 \cdot score + 1490175 \cdot ms.count$$

Where:

* `no.proteins` - Number of proteins contained within the group.
* `peptides` - The total number of peptide sequences associated with the protein 
group (i.e. for all the proteins in the group).
* `seq.coverage` - Percentage of the sequence that is covered by the identified
peptides of the first protein sequence contained in the group.
* `mol.wieght.kDa` - Molecular weight of the leading protein sequence contained 
in the protein group.
* `score` - Andromeda score for the best associated MS/MS spectrum.
* `ms.count` - The number of MS/MS spectra recorded in this raw file.

We train and compare 4 machine learning models and based on the performance (table 
below), we decided to use 2 of them:

1. Random Forest
2. kNN

|     Model     | NRMSE |  MSE  | RMSE  |  MAE  |   R2   |
|:-------------:|:-----:|:-----:|:-----:|:-----:|:------:|
| **RF**        | 0.5724 | 0.8044 | 0.8969 | 0.6663 | 0.7615 |
| **kNN**       | 0.6070 | 1.0182 | 1.0091 | 0.7590 | 0.6701 |
| **LM**        | 0.8737 | 3.8825 | 1.9704 | 1.6091 | 0.2623 |
| **PCA**       | 1      | 6.8863 | 2.6242 | 2.2380 | 0      |


For the Ludovic method first we had to assign the missing values using the
[`protti::assign_missingness()`](https://jpquast.github.io/protti/reference/assign_missingness.html) 
fxn, which added the **`comparison`** column containing the comparison name for the specific 
treatment/reference pair and the **`missingness`** column with 4 types of missingness:

* **`"complete"`**: No missing values for every replicate of this reference/treatment 
pair for the specific grouping variable.
* **`"MNAR"`**: Missing not at random. All replicates of either the reference or 
treatment condition have missing values for the specific grouping variable.
* **`"MAR"`**: Missing at random. At least n-1 replicates have missing values for the 
reference/treatment pair for the specific grouping variable.
* **`NA`**: The comparison is not complete enough to fall into any other category. 
It will not be imputed if imputation is performed.

Then with additional column we could enter our data into 
[`protti::impute()`](https://jpquast.github.io/protti/reference/impute.html) 
fxn, which performs the Ludovic imputation method based on the **`missingness`** column
using the following instructions:

* **`"MNAR"`** missingness is sampled from a normal distribution around a value that 
is three lower (log2) than the lowest intensity value recorded for the 
precursor/peptide and that has a spread of the mean standard deviation for the precursor/peptide.
* **`"MAR"`** data is imputed using the mean and variance of the condition with the missing data.
* **`NA`** data is not imputed.

```{r load data}
# load raw data
protein.groups <- readr::read_tsv('./data/proteinGroups.txt',show_col_types = FALSE)

protein.groups <- protein.groups %>% filter(is.na(`Only identified by site`),
                         is.na(Reverse),
                         is.na(`Potential contaminant`))

# non-imputed data
lfq <- read.csv('./data/nonimputed_lfq.csv')

# imputed data with Mateusz fxn
lfq_imp <- read.csv('./data/LFQ_raw_totals_imp.csv')
colnames(lfq_imp) <- gsub('_TOTALS_', '.I.', colnames(lfq_imp))
```

```{r nonimp TOTALS data}
# Extracting only TOTALS data for knockout
LFQ_KO <- lfq %>% select(contains('KO'))

# Extracting only TOTALS data for wild type
LFQ_WT <- lfq %>% select(contains('WT'))
```

```{r imp TOTALS data}
# Extracting only TOTALS data for knockout
LFQ_KO_imp <- lfq_imp %>% select(contains('KO'))

# Extracting only TOTALS data for wild type
LFQ_WT_imp <- lfq_imp %>% select(contains('WT'))
```

```{r data to RF and kNN}
# extracting columns to machine learning models
df_ml <- protein.groups |>
    dplyr::select(`Number of proteins`, Peptides, `Sequence coverage [%]`, `Sequence length`, 
                  `Mol. weight [kDa]`, `Q-value`, Score, `MS/MS count`, 
                  dplyr::starts_with('LFQ Intensity') & (ends_with('22') | ends_with('23') | ends_with('24')) & dplyr::contains('TOTALS'))

# changing the colnames
colnames(df_ml)[9:14] <- gsub('LFQ intensity ', '', colnames(df_ml)[9:14])

# making longer data frame with one column containing all LFQ values
df_ml <- df_ml |>
    dplyr::mutate(prot.id = paste('prot',1:nrow(df_ml),sep='_')) |>
    tidyr::pivot_longer(9:14, names_to = 'Sample', values_to = 'LFQvalue') |>
    tidyr::separate(col=Sample, into=c("celltype","sampletype","rep"), sep = "_", remove = FALSE) |>
    dplyr::mutate(celltype = as.factor(celltype), sampletype = as.factor(sampletype),
                  rep = as.factor(rep))

# changing values with 0 to NA
df_ml$LFQvalue[df_ml$LFQvalue==0] <- NA

df_ml$missingness <- assign_missing(df_ml$prot.id, df_ml$celltype, df_ml$LFQvalue)$missingness

# removing blank spaces and all separators from colnames
colnames(df_ml)[1:8] <- c('no.proteins', 'peptides','seq.coverage','seq.len',
                                      'mol.weight.kDa','q.value','score','ms.count')

# Remove unnecessary columns and reordering columns
df_ml.to.models <- df_ml |>
    dplyr::select(-prot.id, -Sample, -sampletype) |> dplyr::select(1:10, 12, 11) |>
    dplyr::mutate(missingness = as.numeric(as.factor(missingness)),
                  celltype = as.numeric(celltype),
                  rep = as.numeric(rep))

df_ml <- df_ml |>
    dplyr::select(-prot.id, -Sample) |> dplyr::select(1:11, 13, 12) |>
    dplyr::mutate(missingness = as.factor(missingness))
```

```{r data to protti}
df_to_protti <- protein.groups %>%
    select(`Protein IDs`, `Peptide sequences`, 
           contains('LFQ intensity') & contains('TOTALS') & (ends_with('22') | ends_with('23') | ends_with('24'))) %>%
    mutate(`Protein IDs`= paste('prot_', 1:nrow(protein.groups), sep='')) %>%
    pivot_longer(3:8, names_to = 'Sample', values_to = 'Intensity')%>%
    mutate(Sample = gsub('LFQ intensity ', '', Sample)) %>%
    mutate(Sample = gsub('_TOTALS_', '_', Sample)) %>%
    separate(col =  Sample, into = c("celltype","rep"), sep = "_", remove = F) %>%
    mutate(Condition = ifelse(celltype == 'KO', 'treated', 'control'),
           Intensity = ifelse(Intensity == 0, NA, log2(Intensity))) %>% 
    select(Sample, `Protein IDs`, `Peptide sequences`, Condition, Intensity) 
```

```{r train/test split}
# creating data with no NA to train models
df_ml.no.na <- na.omit(df_ml.to.models)

# set seed to avoid different models and test/train data with each run
set.seed(123)
# split data to train and test
train_data_idx <- sample(1:nrow(df_ml.no.na), ceiling(0.7*nrow(df_ml.no.na)))
train_data <- df_ml.no.na[train_data_idx,]
```

```{r imputation RF}
# training random forest model
rf.model <- ranger::ranger(LFQvalue ~ .,
                          data=train_data,
                          num.trees = 100,
                          mtry = 4,
                          min.node.size = 5,
                          replace = FALSE,
                          sample.fraction = 0.8,
                          seed = 123,
                          importance = 'impurity')

# extracting missing rows - can be usefull later
missing <- which(is.na(df_ml$LFQvalue))

# predict the values for NA rows
predictions <- predict(rf.model, df_ml.to.models[missing,])

# data frame with predicted LFQ values by random forest model
imputed_RF <- df_ml
imputed_RF[missing,13] <- predictions$predictions
imputed_RF <- imputed_RF |>
    dplyr::select(celltype, rep, LFQvalue) |>
    dplyr::mutate('Batch' = paste(celltype, rep, sep='.'),
                  LFQvalue = log2(LFQvalue)) |>
    dplyr::select(Batch, LFQvalue) |>
    tidyr::pivot_wider(everything(), names_from = Batch, values_from = LFQvalue) |>
    unnest()

# extracting KO and WT
LFQ_KO_RF <- imputed_RF |> dplyr::select(contains('KO'))
LFQ_WT_RF <- imputed_RF |> dplyr::select(contains('WT'))
```

```{r imputation kNN}
# calculating kNN model
kNN.model <- caret::train(LFQvalue ~ ., data=train_data, method='knn')

# predict the values for NA rows
predictions2 <- predict(kNN.model, df_ml.to.models[missing,1:12])

# data frame with predicted LFQ values by random forest model
imputed_kNN <- df_ml
imputed_kNN[missing,13] <- predictions2
imputed_kNN <- imputed_kNN |>
    dplyr::select(celltype, rep, LFQvalue) |>
    dplyr::mutate('Batch' = paste(celltype, rep, sep='.'),
                  LFQvalue = log2(LFQvalue)) |>
    dplyr::select(Batch, LFQvalue) |>
    tidyr::pivot_wider(everything(), names_from = Batch, values_from = LFQvalue) |>
    unnest()

# extracting KO and WT
LFQ_KO_kNN <- imputed_kNN |> dplyr::select(contains('KO'))
LFQ_WT_kNN <- imputed_kNN |> dplyr::select(contains('WT'))
```

```{r ludovic imputation}
data_missing <- df_to_protti |>
    assign_missingness(sample=Sample,
                       condition = Condition,
                       grouping = `Protein IDs`,
                       intensity = Intensity,
                       ref_condition = 'all')

imputed_ludovic <- impute(
    data_missing,
    sample = Sample,
    grouping = `Protein IDs`,
    intensity_log2 = Intensity,
    condition = Condition,
    comparison = comparison,
    missingness = missingness,
    method = 'ludovic',
    skip_log2_transform_error = TRUE
)
rm(data_missing)

# droping the NA values 
imputed_ludovic <- imputed_ludovic[which(!is.na(imputed_ludovic$missingness)),]

# extracting KO and WT
imputed_ludovic <- imputed_ludovic |>
    dplyr::select(Sample, imputed_intensity, `Protein IDs`) |>
    tidyr::pivot_wider(everything(), names_from = Sample, values_from = imputed_intensity) |>
    unnest()

LFQ_KO_ludovic <- imputed_ludovic |> dplyr::select(contains('KO'))
LFQ_WT_ludovic <- imputed_ludovic |> dplyr::select(contains('WT'))
```

# Distributions {.tabset}

**Summary**<br>
Below we can see the distribution of non-imputed data and data imputed with 4 methods:

1. Shifted Distribution based on the data;
2. Imputation using the Random Forest algorithm;
3. Imputation using the k-Nearest Neighbors algorithm;
4. Imputation using the ludovic method.

For each histogram, we can see that the 24th replicate (blue colour) of the KO 
cell type  has a distribution shift to the right. Our goal is to get this 
distribution closer to the others, and we can do this using different 
normalization methods.

## Mati Code

```{r distributions mati code}
ggarrange(
    plothist(LFQ_KO, '', FALSE) + xlab('KO') + ggtitle('Non-Imputed'), 
    plothist(LFQ_WT, '', FALSE) + xlab('WT') + ggtitle(''),
    plothist(LFQ_KO_imp, '', FALSE) + xlab('KO') + ggtitle('Imputed'), 
    plothist(LFQ_WT_imp, '', FALSE) + xlab('WT') + ggtitle(''),
    nrow=2,ncol=2
)
```

## Random Forest

```{r distribution rf}
ggarrange(
    plothist(LFQ_KO, '', FALSE) + xlab('KO') + ggtitle('Non-Imputed'), 
    plothist(LFQ_WT, '', FALSE) + xlab('WT') + ggtitle(''),
    plothist(LFQ_KO_RF, '', FALSE) + xlab('KO') + ggtitle('Imputed - Random Forest'), 
    plothist(LFQ_WT_RF, '', FALSE) + xlab('WT') + ggtitle(''),
    nrow=2,ncol=2
)
```

## k-Nearest Neighbors

```{r distribution kNN}
ggarrange(
    plothist(LFQ_KO, '', FALSE) + xlab('KO') + ggtitle('Non-Imputed'), 
    plothist(LFQ_WT, '', FALSE) + xlab('WT') + ggtitle(''),
    plothist(LFQ_KO_kNN, '', FALSE) + xlab('KO') + ggtitle('Imputed - kNN'), 
    plothist(LFQ_WT_kNN, '', FALSE) + xlab('WT') + ggtitle(''),
    nrow=2,ncol=2
)
```

## Ludovic

```{r distribution ludovic}
ggarrange(
    plothist(LFQ_KO, '', FALSE) + xlab('KO') + ggtitle('Non-Imputed'), 
    plothist(LFQ_WT, '', FALSE) + xlab('WT') + ggtitle(''),
    plothist(LFQ_KO_ludovic, '', FALSE) + xlab('KO') + ggtitle('Imputed - Ludovic'), 
    plothist(LFQ_WT_ludovic, '', FALSE) + xlab('WT') + ggtitle(''),
    nrow=2,ncol=2
)
```

# Normalization {.tabset}

The 24th replicate of the KO cell type has the distribution shifted to the right and 
should be more similar to the other replicates. Therefore we tried several
normalization methods to achieve this goal. We used the following methods:

1. Z-Score
2. Min-Max
3. Median Scaling
4. MAD (Median Absolute Deviation) Scaling
5. Linear Regression
6. VSN (Variance Stabilization Normalization)
7. EigenMS

## Z-Score

Z-score normalization standardizes data by subtracting the mean and dividing 
by the standard deviation. This technique transforms data into a distribution 
with a mean of 0 and a standard deviation of 1.

Formula: $\tilde{y}_{ij} = \frac{y_ij - \bar{y_j}}{\theta_j}$, where:

* $y_ij$ - value of the LFQ;
* $\bar{y_j}$ - mean of the LFQ values;
* $\theta_j$ - standard deviation of the LFQ values.

```{r KO standardization}
LFQ_KO.standard <- as.data.frame(scale(LFQ_KO)) # Non-Imputed
LFQ_KO_imp.standard <- as.data.frame(scale(LFQ_KO_imp)) # Imputed - Mati Code
LFQ_KO_RF.standard <- as.data.frame(scale(LFQ_KO_RF)) # Imputed RF
LFQ_KO_kNN.standard <- as.data.frame(scale(LFQ_KO_kNN)) # Imputed kNN
LFQ_KO_ludovic.standard <- as.data.frame(scale(LFQ_KO_ludovic)) # Imputed ludovic
```

```{r WT standardization}
LFQ_WT.standard <- as.data.frame(scale(LFQ_WT)) # Non-Imputed
LFQ_WT_imp.standard <- as.data.frame(scale(LFQ_WT_imp)) # Imputed - Mati Code
LFQ_WT_RF.standard <- as.data.frame(scale(LFQ_WT_RF)) # Imputed RF
LFQ_WT_kNN.standard <- as.data.frame(scale(LFQ_WT_kNN)) # Imputed kNN
LFQ_WT_ludovic.standard <- as.data.frame(scale(LFQ_WT_ludovic)) # Imputed ludovic
```

**Summary** <br>
The Z-SCORE method brought the distribution closer together, but we lost
the original scale of the data and the differences are harder to see after normalization.

<center><b style='font-size:20px'>DISTRIBUTIONS</b></center>

```{r dist standardization, fig.height=4}
annotate_figure(ggarrange(
    plothist(LFQ_KO.standard, NULL, TRUE) + xlab('KO'), 
    plothist(LFQ_WT.standard, NULL, TRUE) + xlab('WT'),
    nrow=1,ncol=2
    ), top = text_grob('Non-Imputed Data', size = 16)
)
```

```{r}
ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_imp.standard, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_imp.standard, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Mati Code', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_RF.standard, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_RF.standard, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Random Forest', size = 16)
    ),
    nrow=2,ncol=1
)
ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_kNN.standard, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_kNN.standard, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - kNN', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_ludovic.standard, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_ludovic.standard, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Ludovic', size = 16)
    ),
    nrow=2,ncol=1
)
```

## Min-Max

Min-max normalization scales data to a specified range (usually [0, 1]) by 
subtracting the minimum value and dividing by the range of values.

```{r minmax}
min_max_norm <- function(df) {
    df_no_na <- na.omit(df)
    ret <- scale(df_no_na, center = min(df_no_na), scale = max(df_no_na) - min(df_no_na))
    df[which(!is.na(df))] <- ret
    return(df)
}

LFQ_KO.minmax <- as.data.frame(lapply(LFQ_KO, function(col) min_max_norm(col)))  # Non-Imputed
LFQ_KO_imp.minmax <- as.data.frame(lapply(LFQ_KO_imp, function(col) min_max_norm(col)))  # Imputed
LFQ_KO_RF.minmax <- as.data.frame(lapply(LFQ_KO_RF, function(col) min_max_norm(col)))  # Imputed RF
LFQ_KO_kNN.minmax <- as.data.frame(lapply(LFQ_KO_kNN, function(col) min_max_norm(col)))  # Imputed kNN
LFQ_KO_ludovic.minmax <- as.data.frame(lapply(LFQ_KO_ludovic, function(col) min_max_norm(col)))  # Imputed ludovic

LFQ_WT.minmax <- as.data.frame(lapply(LFQ_WT, function(col) min_max_norm(col)))  # Non-Imputed
LFQ_WT_imp.minmax <- as.data.frame(lapply(LFQ_WT_imp, function(col) min_max_norm(col)))  # Imputed
LFQ_WT_RF.minmax <- as.data.frame(lapply(LFQ_WT_RF, function(col) min_max_norm(col)))  # Imputed RF
LFQ_WT_kNN.minmax <- as.data.frame(lapply(LFQ_WT_kNN, function(col) min_max_norm(col)))  # Imputed kNN
LFQ_WT_ludovic.minmax <- as.data.frame(lapply(LFQ_WT_ludovic, function(col) min_max_norm(col)))  # Imputed ludovic
```

**Summary** <br>
The MIN-MAX method didn't succeed in bringing the distribution closer together. 
In conclusion, we probably shouldn't use this method.

<center><b style='font-size:20px'>DISTRIBUTIONS</b></center>

```{r dist minmax, fig.height=4}
annotate_figure(ggarrange(
    plothist(LFQ_KO.minmax, NULL, TRUE) + xlab('KO'), 
    plothist(LFQ_WT.minmax, NULL, TRUE) + xlab('WT'),
    nrow=1,ncol=2
    ), top = text_grob('Non-Imputed Data', size = 16)
)
```

```{r}
ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_imp.minmax, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_imp.minmax, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Mati Code', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_RF.minmax, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_RF.minmax, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Random Forest', size = 16)
    ),
    nrow=2,ncol=1
)

ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_kNN.minmax, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_kNN.minmax, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - kNN', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_ludovic.minmax, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_ludovic.minmax, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Ludovic', size = 16)
    ),
    nrow=2,ncol=1
)
```

## Median Scaling

Median Scaling, also called the Robust Scalar, transforms x to x’ by subtracting each 
value of features by the median and dividing it by the interquartile range between 
the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). The formula:

$$x' = \frac{x-median(x)}{(Q3-Q1)}$$

```{r median scaling}
LFQ_KO.med <- as.data.frame(DescTools::RobScale(LFQ_KO, scale=FALSE)) # Non-Imputed
LFQ_KO_imp.med <- as.data.frame(DescTools::RobScale(LFQ_KO_imp, scale=FALSE)) # Imputed
LFQ_KO_RF.med <- as.data.frame(DescTools::RobScale(LFQ_KO_RF, scale=FALSE)) # Imputed
LFQ_KO_kNN.med <- as.data.frame(DescTools::RobScale(LFQ_KO_kNN, scale=FALSE)) # Imputed
LFQ_KO_ludovic.med <- as.data.frame(DescTools::RobScale(LFQ_KO_ludovic, scale=FALSE)) # Imputed

LFQ_WT.med <- as.data.frame(DescTools::RobScale(LFQ_WT, scale=FALSE)) # Non-Imputed
LFQ_WT_imp.med <- as.data.frame(DescTools::RobScale(LFQ_WT_imp, scale=FALSE)) # Imputed
LFQ_WT_RF.med <- as.data.frame(DescTools::RobScale(LFQ_WT_RF, scale=FALSE)) # Imputed
LFQ_WT_kNN.med <- as.data.frame(DescTools::RobScale(LFQ_WT_kNN, scale=FALSE)) # Imputed
LFQ_WT_ludovic.med <- as.data.frame(DescTools::RobScale(LFQ_WT_ludovic, scale=FALSE)) # Imputed
```

**Summary**<br>
The MEDIAN scaling method brought the distributions closer together, but again we lost
the original scale of the data and the differences are harder to see after normalization.

<center><b style='font-size:20px'>DISTRIBUTIONS</b></center>

```{r dist median, fig.height=4}
annotate_figure(ggarrange(
    plothist(LFQ_KO.med, NULL, TRUE) + xlab('KO'), 
    plothist(LFQ_WT.med, NULL, TRUE) + xlab('WT'),
    nrow=1,ncol=2
    ), top = text_grob('Non-Imputed Data', size = 16)
)
```

```{r}
ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_imp.med, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_imp.med, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Mati Code', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_RF.med, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_RF.med, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Random Forest', size = 16)
    ),
    nrow=2,ncol=1
)

ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_kNN.med, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_kNN.med, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - kNN', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_ludovic.med, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_ludovic.med, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Ludovic', size = 16)
    ),
    nrow=2,ncol=1
)
```

## MAD Scaling

MAD (**M**edian **A**bsoulte **D**eviation) Scaling transforms $x$ to $x'$ in 
similar way like Median Scaling, but it substractingthe MAD value from the $x$. 
The formula of MAD:

$$\tilde{X} = median(X) \\ MAD = median\Big(|X_i - \tilde{X}|\Big)$$

and the formula of scaling is:

$$x' = \frac{x - MAD}{(Q3-Q1)}$$

```{r mad scaling}
LFQ_KO.mad <- as.data.frame(DescTools::RobScale(LFQ_KO)) # Non-Imputed
LFQ_KO_imp.mad <- as.data.frame(DescTools::RobScale(LFQ_KO_imp)) # Imputed
LFQ_KO_RF.mad <- as.data.frame(DescTools::RobScale(LFQ_KO_RF)) # Imputed RF
LFQ_KO_kNN.mad <- as.data.frame(DescTools::RobScale(LFQ_KO_kNN)) # Imputed kNN
LFQ_KO_ludovic.mad <- as.data.frame(DescTools::RobScale(LFQ_KO_ludovic)) # Imputed ludovic

LFQ_WT.mad <- as.data.frame(DescTools::RobScale(LFQ_WT)) # Non-Imputed
LFQ_WT_imp.mad <- as.data.frame(DescTools::RobScale(LFQ_WT_imp)) # Imputed
LFQ_WT_RF.mad <- as.data.frame(DescTools::RobScale(LFQ_WT_RF)) # Imputed RF
LFQ_WT_kNN.mad <- as.data.frame(DescTools::RobScale(LFQ_WT_kNN)) # Imputed kNN
LFQ_WT_ludovic.mad <- as.data.frame(DescTools::RobScale(LFQ_WT_ludovic)) # Imputed ludovic
```

**Summary**<br>
The MAD scaling method brought the distributions closer together, but again we lost
the original scale of the data and the differences are harder to see after normalization.

<center><b style='font-size:20px'>DISTRIBUTIONS</b></center>

```{r dist mad, fig.height=4}
annotate_figure(ggarrange(
    plothist(LFQ_KO.mad, NULL, TRUE) + xlab('KO'), 
    plothist(LFQ_WT.mad, NULL, TRUE) + xlab('WT'),
    nrow=1,ncol=2
    ), top = text_grob('Non-Imputed Data', size = 16)
)
```

```{r}
ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_imp.mad, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_imp.mad, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Mati Code', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_RF.mad, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_RF.mad, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Random Forest', size = 16)
    ),
    nrow=2,ncol=1
)

ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_kNN.mad, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_kNN.mad, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - kNN', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_ludovic.mad, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_ludovic.mad, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Ludovic', size = 16)
    ),
    nrow=2,ncol=1
)
```

## Linear Regression

Normalization ny linear regression using a residual of the linear model $y = ax + b$
to normalize the data. In our project we have two variables: independent and dependent.
The independent is **Batch** and the dependent is **LFQ**.

```{r KO lm}
# Non-Imputed
LFQ_KO_long <- LFQ_KO %>%
    pivot_longer(cols = 1:3, names_to = "Batch", values_to = "LFQValue")

model <- lm(LFQValue ~ Batch, data=LFQ_KO_long)
LFQ_KO_long$normalized <- NA
LFQ_KO_long[which(!is.na(LFQ_KO_long$LFQValue)), 'normalized'] <- residuals(model)

LFQ_KO.lm <- LFQ_KO_long %>% 
    mutate(row = rep(1:nrow(LFQ_KO), each=3)) %>%
    reshape2::dcast(row ~ Batch, value.var='normalized') %>%
    select(-row)

# Imputed Mati Code
LFQ_KO_long <- LFQ_KO_imp %>%
    pivot_longer(cols = 1:3, names_to = "Batch", values_to = "LFQValue")

model <- lm(LFQValue ~ Batch, data=LFQ_KO_long)
LFQ_KO_long$normalized <- residuals(model)

LFQ_KO_imp.lm <- LFQ_KO_long %>% 
    mutate(row = rep(1:nrow(LFQ_KO_imp), each=3)) %>%
    reshape2::dcast(row ~ Batch, value.var='normalized') %>%
    select(-row)

# Imputed RF
LFQ_KO_long <- LFQ_KO_RF %>%
    pivot_longer(cols = 1:3, names_to = "Batch", values_to = "LFQValue")

model <- lm(LFQValue ~ Batch, data=LFQ_KO_long)
LFQ_KO_long$normalized <- residuals(model)

LFQ_KO_RF.lm <- LFQ_KO_long %>% 
    mutate(row = rep(1:nrow(LFQ_KO_RF), each=3)) %>%
    reshape2::dcast(row ~ Batch, value.var='normalized') %>%
    select(-row)

# Imputed kNN
LFQ_KO_long <- LFQ_KO_kNN %>%
    pivot_longer(cols = 1:3, names_to = "Batch", values_to = "LFQValue")

model <- lm(LFQValue ~ Batch, data=LFQ_KO_long)
LFQ_KO_long$normalized <- residuals(model)

LFQ_KO_kNN.lm <- LFQ_KO_long %>% 
    mutate(row = rep(1:nrow(LFQ_KO_kNN), each=3)) %>%
    reshape2::dcast(row ~ Batch, value.var='normalized') %>%
    select(-row)

# Imputed ludovic
LFQ_KO_long <- LFQ_KO_ludovic %>%
    pivot_longer(cols = 1:3, names_to = "Batch", values_to = "LFQValue")

model <- lm(LFQValue ~ Batch, data=LFQ_KO_long)
LFQ_KO_long$normalized <- residuals(model)

LFQ_KO_ludovic.lm <- LFQ_KO_long %>% 
    mutate(row = rep(1:nrow(LFQ_KO_ludovic), each=3)) %>%
    reshape2::dcast(row ~ Batch, value.var='normalized') %>%
    select(-row)
```

```{r WT lm}
# Non-Imputed
LFQ_WT_long <- LFQ_WT %>%
    pivot_longer(cols = 1:3, names_to = "Batch", values_to = "LFQValue")

model <- lm(LFQValue ~ Batch, data=LFQ_WT_long)
LFQ_WT_long$normalized <- NA
LFQ_WT_long[which(!is.na(LFQ_WT_long$LFQValue)), 'normalized'] <- residuals(model)

LFQ_WT.lm <- LFQ_WT_long %>% 
    mutate(row = rep(1:nrow(LFQ_WT), each=3)) %>%
    reshape2::dcast(row ~ Batch, value.var='normalized') %>%
    select(-row)

# Imputed Mati Code
LFQ_WT_long <- LFQ_WT_imp %>%
    pivot_longer(cols = 1:3, names_to = "Batch", values_to = "LFQValue")

model <- lm(LFQValue ~ Batch, data=LFQ_WT_long)
LFQ_WT_long$normalized <- residuals(model)

LFQ_WT_imp.lm <- LFQ_WT_long %>% 
    mutate(row = rep(1:nrow(LFQ_WT_imp), each=3)) %>%
    reshape2::dcast(row ~ Batch, value.var='normalized') %>%
    select(-row)

# Imputed RF
LFQ_WT_long <- LFQ_WT_RF %>%
    pivot_longer(cols = 1:3, names_to = "Batch", values_to = "LFQValue")

model <- lm(LFQValue ~ Batch, data=LFQ_WT_long)
LFQ_WT_long$normalized <- residuals(model)

LFQ_WT_RF.lm <- LFQ_WT_long %>% 
    mutate(row = rep(1:nrow(LFQ_WT_RF), each=3)) %>%
    reshape2::dcast(row ~ Batch, value.var='normalized') %>%
    select(-row)

# Imputed kNN
LFQ_WT_long <- LFQ_WT_kNN %>%
    pivot_longer(cols = 1:3, names_to = "Batch", values_to = "LFQValue")

model <- lm(LFQValue ~ Batch, data=LFQ_WT_long)
LFQ_WT_long$normalized <- residuals(model)

LFQ_WT_kNN.lm <- LFQ_WT_long %>% 
    mutate(row = rep(1:nrow(LFQ_WT_kNN), each=3)) %>%
    reshape2::dcast(row ~ Batch, value.var='normalized') %>%
    select(-row)

# Imputed ludovic
LFQ_WT_long <- LFQ_WT_ludovic %>%
    pivot_longer(cols = 1:3, names_to = "Batch", values_to = "LFQValue")

model <- lm(LFQValue ~ Batch, data=LFQ_WT_long)
LFQ_WT_long$normalized <- residuals(model)

LFQ_WT_ludovic.lm <- LFQ_WT_long %>% 
    mutate(row = rep(1:nrow(LFQ_WT_ludovic), each=3)) %>%
    reshape2::dcast(row ~ Batch, value.var='normalized') %>%
    select(-row)
```

**Summary**<br>
The linear regression method brought the distributions closer together, but again we lost
the original scale of the data and the differences are harder to see after normalization.

<center><b style='font-size:20px'>DISTRIBUTIONS</b></center>

```{r dist lm, fig.height=4}
annotate_figure(ggarrange(
    plothist(LFQ_KO.lm, NULL, TRUE) + xlab('KO'), 
    plothist(LFQ_WT.lm, NULL, TRUE) + xlab('WT'),
    nrow=1,ncol=2
    ), top = text_grob('Non-Imputed Data', size = 16)
)
```

```{r}
ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_imp.lm, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_imp.lm, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Mati Code', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_RF.lm, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_RF.lm, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Random Forest', size = 16)
    ),
    nrow=2,ncol=1
)

ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_kNN.lm, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_kNN.lm, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - kNN', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_ludovic.lm, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_ludovic.lm, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Ludovic', size = 16)
    ),
    nrow=2,ncol=1
)
```

## VSN

VSN - **V**ariance **S**tabilization **N**ormalization

The data are returned on a $glog$ scale to base 2. More precisely, the transformed 
data are subject to the transformation $glog_2(f(b)*x+a)+c$, where the function
$glog_2(u)=log_2(u+\sqrt{u*u+1}) = asinh(u)/log(2)$ is called the generalised logarithm, 
the offset $a$ and the scaling parameter $b$ are the fitted model parameters (see references), 
and $f(x)=exp(x)$ is a&nbsp;parameter transformation that allows ensuring positivity of 
the factor in front of x while using an unconstrained optimization over $b$&nbsp;[[source](https://www.bioconductor.org/packages/devel/bioc/vignettes/vsn/inst/doc/C-likelihoodcomputations.pdf)]. 
The overall offset $c$ is computed from the $b$'s such that for large $x$ the 
transformation approximately corresponds to the $log_2$ function. This is done 
separately for each stratum, but with the same value across arrays. More precisely, 
if the element $b[s,i]$ of the array $b$ is the scaling parameter for the $s$-th stratum
and the $i$-th array, then $c[s]$ is computed as $log2(2*f(mean(b[,i])))$. The&nbsp;offset
$c$ is  inconsequential for all differential expression calculations, but 
many users like to see the data in a range that they are familiar with.

```{r vsn}
lfq_rglist<- new('RGList', list(
    R = as.matrix(2^LFQ_KO),
    G = as.matrix(2^LFQ_WT)))

lfq_rglist_imp<- new('RGList', list(
    R = as.matrix(2^LFQ_KO_imp),
    G = as.matrix(2^LFQ_WT_imp)))

lfq_rglist_RF<- new('RGList', list(
    R = as.matrix(2^LFQ_KO_RF),
    G = as.matrix(2^LFQ_WT_RF)))

lfq_rglist_kNN<- new('RGList', list(
    R = as.matrix(2^LFQ_KO_kNN),
    G = as.matrix(2^LFQ_WT_kNN)))

lfq_rglist_ludovic <- new('RGList', list(
    R = as.matrix(2^LFQ_KO_ludovic),
    G = as.matrix(2^LFQ_WT_ludovic)))

LFQ_KO.vsn <- justvsn(lfq_rglist)@assayData$R  # Non-Imputed
LFQ_KO_imp.vsn <- justvsn(lfq_rglist_imp)@assayData$R # Imputed
LFQ_KO_RF.vsn <- justvsn(lfq_rglist_RF)@assayData$R # Imputed RF
LFQ_KO_kNN.vsn <- justvsn(lfq_rglist_kNN)@assayData$R # Imputed kNN
LFQ_KO_ludovic.vsn <- justvsn(lfq_rglist_ludovic)@assayData$R # Imputed ludovic

LFQ_WT.vsn <- justvsn(lfq_rglist)@assayData$G  # Non-Imputed
LFQ_WT_imp.vsn <- justvsn(lfq_rglist_imp)@assayData$G # Imputed
LFQ_WT_RF.vsn <- justvsn(lfq_rglist_RF)@assayData$G # Imputed RF
LFQ_WT_kNN.vsn <- justvsn(lfq_rglist_kNN)@assayData$G # Imputed kNN
LFQ_WT_ludovic.vsn <- justvsn(lfq_rglist_ludovic)@assayData$G # Imputed ludovic
```

**Summary**<br>
The VSN method brought the distributions closer together and we didn't lose the 
original scale of the data. The method didn't lose the differences between 
KO and WT cells at first glance. In addition, the 24th replicate of KO cell doesn't
stand out only in distribution of imputed data with kNN.

<center><b style='font-size:20px'>DISTRIBUTIONS</b></center>

```{r dist vsn, fig.height=4}
annotate_figure(ggarrange(
    plothist(LFQ_KO.vsn, NULL, TRUE) + xlab('KO'), 
    plothist(LFQ_WT.vsn, NULL, TRUE) + xlab('WT'),
    nrow=1,ncol=2
    ), top = text_grob('Non-Imputed Data', size = 16)
)
```

```{r}
ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_imp.vsn, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_imp.vsn, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Mati Code', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_RF.vsn, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_RF.vsn, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Random Forest', size = 16)
    ),
    nrow=2,ncol=1
)

ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_kNN.vsn, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_kNN.vsn, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - kNN', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_ludovic.vsn, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_ludovic.vsn, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Ludovic', size = 16)
    ),
    nrow=2,ncol=1
)
```

## EigenMS

EigenMS is a normalization method developed for high-throughput mass spectrometry 
(MS) proteomics data. It uses eigenvector-based adjustments to remove unwanted 
variation, such as batch effects, from the data. The method works by identifying 
systematic biases in the dataset and applying a correction while preserving 
biologically relevant signals.

In the EigenMS method, eigentrends are calculated by decomposing the systematic 
variation in the proteomics data matrix using Singular Value Decomposition (SVD). 
This is typically expressed as:

$$X = U \Sigma V'$$
Where:

* $X$ represents the original data matrix.
* $U$ and $v'$ are orthogonal matrices.
* $\Sigma$ is a diagonal matrix of singular values.

The decomposition isolates systematic trends (eigentrends) as columns in $U$, 
which can then be removed to adjust for unwanted variation.

```{r EigenMS, fig.keep='none', results='hide'}
treatment = as.factor(c('KO', 'KO', 'KO', 'WT', 'WT', 'WT'))
# Non-Imputed
prot.info <- data.frame(prot_ID = paste('prot_', 1:nrow(lfq), sep = ''))
LFQ.eig1 <- eig_norm1(lfq, 
                      treatment = treatment, 
                      prot.info = prot.info)
# Performing eig normalization
LFQ.eig_norm <- eig_norm2(LFQ.eig1)

# Imputed
prot.info <- data.frame(prot_ID = paste('prot_', 1:nrow(lfq_imp), sep = ''))
LFQ_imp.eig1 <- eig_norm1(lfq_imp, 
                          treatment = treatment, 
                          prot.info = prot.info)
# Performing eig normalization
LFQ_imp.eig_norm <- eig_norm2(LFQ_imp.eig1)

# Imputed RF
prot.info <- data.frame(prot_ID = paste('prot_', 1:nrow(imputed_RF), sep = ''))
LFQ_RF.eig1 <- eig_norm1(imputed_RF, 
                         treatment = treatment, 
                         prot.info = prot.info)
# Performing eig normalization
LFQ_RF.eig_norm <- eig_norm2(LFQ_RF.eig1)

# Imputed kNN
prot.info <- data.frame(prot_ID = paste('prot_', 1:nrow(imputed_kNN), sep = ''))
LFQ_kNN.eig1 <- eig_norm1(imputed_kNN, 
                          treatment = treatment, 
                          prot.info = prot.info)
# Performing eig normalization
LFQ_kNN.eig_norm <- eig_norm2(LFQ_kNN.eig1)

# Imputed ludovic
prot.info <- data.frame(imputed_ludovic$`Protein IDs`)
LFQ_ludovic.eig1 <- eig_norm1(imputed_ludovic[,-1],
                              treatment = treatment,
                              prot.info = prot.info)
# Performing eig normalization
LFQ_ludovic.eig_norm <- eig_norm2(LFQ_ludovic.eig1)
```

```{r KO and WT eigen}
LFQ_KO.eigen <- LFQ.eig_norm$norm_m[,1:3] # Non-Imputed
LFQ_KO_imp.eigen <- LFQ_imp.eig_norm$norm_m[,1:3] # Imputed Mati Code
LFQ_KO_RF.eigen <- LFQ_RF.eig_norm$norm_m[,1:3] # Imputed RF
LFQ_KO_kNN.eigen <- LFQ_kNN.eig_norm$norm_m[,1:3] # Imputed kNN
LFQ_KO_ludovic.eigen <- LFQ_ludovic.eig_norm$norm_m[,1:3] # Imputed ludovic

LFQ_WT.eigen <- LFQ.eig_norm$norm_m[,4:6] # Non-Imputed
LFQ_WT_imp.eigen <- LFQ_imp.eig_norm$norm_m[,4:6] # Imputed Mati Code
LFQ_WT_RF.eigen <- LFQ_RF.eig_norm$norm_m[,4:6] # Imputed RF
LFQ_WT_kNN.eigen <- LFQ_kNN.eig_norm$norm_m[,4:6] # Imputed kNN
LFQ_WT_ludovic.eigen <- LFQ_ludovic.eig_norm$norm_m[,4:6] # Imputed ludovic
```

**Summary**<br>
The EigenMS method was the one which kept the differences between the replicates 
and cell types (KO and WT). The distribution are close together and we solve the 
problem of 24th replicate. In my opinion the EigenMS method gives the best results.

<center><b style='font-size:20px'>DISTRIBUTIONS</b></center>

```{r dist eigen, fig.height=4}
annotate_figure(ggarrange(
    plothist(LFQ_KO.eigen, NULL, TRUE) + xlab('KO'), 
    plothist(LFQ_WT.eigen, NULL, TRUE) + xlab('WT'),
    nrow=1,ncol=2
    ), top = text_grob('Non-Imputed Data', size = 16)
)
```

```{r}
ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_imp.eigen, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_imp.eigen, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Mati Code', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_RF.eigen, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_RF.eigen, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Random Forest', size = 16)
    ),
    nrow=2,ncol=1
)

ggarrange(
    annotate_figure(ggarrange(
        plothist(LFQ_KO_kNN.eigen, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_kNN.eigen, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - kNN', size = 16)
    ),
    annotate_figure(ggarrange(
        plothist(LFQ_KO_ludovic.eigen, '', FALSE) + xlab('KO'), 
        plothist(LFQ_WT_ludovic.eigen, '', FALSE) + xlab('WT'),
        nrow=1,ncol=2
        ), top = text_grob('Imputed Data - Ludovic', size = 16)
    ),
    nrow=2,ncol=1
)
```

# Statistics Metrics {.tabset}

## CV {.tabset}

**COEFFICIENT OF VARIATION (CV) / RELATIVE STANDARD DEVIATION (RSD)**

This is a way to measure how spread out values are in a dataset relative to the mean.
A lower RSD/CV indicates better normalization. It is calculated as:

$CV = \frac{\sigma}{\mu}$

where:

* $\sigma$: The standard deviation of dataset
* $\mu$: The mean of dataset

```{r CV}
cv <- function (x) sd(x) / mean(x) * 100
```

**Summary**
For the biological data the cut-off level of CV is around 20 (+/-5). After close 
looking at the tables in the `**Summary**` section, we can safely say that the **EigenMS** 
method had the CV around 20 in all imputation methods. In fact, from this point of view 
we can distinguish four such methods that had good coefficient of variation.

1. EigenMS
2. MAD Scaling
3. Min-Max Normalization
4. Z-Score Normalization

When we take a closer look at the violin plots, we can see the strange pattern
in the results for the **kNN** imputation method. Namely, many values have a CV
close to the 0, which can mean that the standard deviation of the values is also close
to the 0, which means that the values are very similar, which shouldn't be the case.

The violin plots for the other methods look better and show the distribution/density
of the CV correctly.

### Non-Imputed

```{r CV1}
cv.before <- data.frame(KO=apply(2^LFQ_KO, 1, cv), WT=apply(2^LFQ_WT,1,cv))
cv.standard <- data.frame(KO=apply(2^LFQ_KO.standard, 1, cv), WT=apply(2^LFQ_WT.standard, 1, cv))
cv.minmax <- data.frame(KO=apply(2^LFQ_KO.minmax, 1, cv), WT= apply(2^LFQ_WT.minmax, 1, cv))
cv.med <- data.frame(KO=apply(2^LFQ_KO.med, 1, cv), WT=apply(2^LFQ_WT.med, 1, cv))
cv.mad <- data.frame(KO=apply(2^LFQ_KO.mad, 1, cv), WT=apply(2^LFQ_WT.mad, 1, cv))
cv.lm <- data.frame(KO=apply(2^LFQ_KO.lm, 1, cv), WT=apply(2^LFQ_WT.lm, 1, cv))
cv.vsn <- data.frame(KO=apply(2^LFQ_KO.vsn, 1, cv), WT=apply(2^LFQ_WT.vsn, 1, cv))
cv.eig <- data.frame(KO=apply(2^LFQ_KO.eigen, 1, cv), WT=apply(2^LFQ_WT.eigen, 1, cv))

ggarrange(
    plotoneviolin(cv.before, 'Original'),
    plotoneviolin(cv.standard, 'Z-Score'),
    plotoneviolin(cv.minmax, 'Min-Max'),
    plotoneviolin(cv.med, 'Median Scaling'),
    nrow=2,ncol=2
)

ggarrange(
    plotoneviolin(cv.mad, 'MAD Scaling'),
    plotoneviolin(cv.lm, 'Linear Regression'),
    plotoneviolin(cv.vsn, 'VSN'),
    plotoneviolin(cv.eig, 'EigenMS'),
    nrow=2,ncol=2
)
```

### Mati Code

```{r CV2}
cv_imp.before <- data.frame(KO=apply(2^LFQ_KO_imp, 1, cv), WT=apply(2^LFQ_WT_imp,1,cv))
cv_imp.standard <- data.frame(KO=apply(2^LFQ_KO_imp.standard, 1, cv), WT=apply(2^LFQ_WT_imp.standard, 1, cv))
cv_imp.minmax <- data.frame(KO=apply(2^LFQ_KO_imp.minmax, 1, cv), WT= apply(2^LFQ_WT_imp.minmax, 1, cv))
cv_imp.med <- data.frame(KO=apply(2^LFQ_KO_imp.med, 1, cv), WT=apply(2^LFQ_WT_imp.med, 1, cv))
cv_imp.mad <- data.frame(KO=apply(2^LFQ_KO_imp.mad, 1, cv), WT=apply(2^LFQ_WT_imp.mad, 1, cv))
cv_imp.lm <- data.frame(KO=apply(2^LFQ_KO_imp.lm, 1, cv), WT=apply(2^LFQ_WT_imp.lm, 1, cv))
cv_imp.vsn <- data.frame(KO=apply(2^LFQ_KO_imp.vsn, 1, cv), WT=apply(2^LFQ_WT_imp.vsn, 1, cv))
cv_imp.eig <- data.frame(KO=apply(2^LFQ_KO_imp.eigen, 1, cv), WT=apply(2^LFQ_WT_imp.eigen, 1, cv))

ggarrange(
    plotoneviolin(cv_imp.before, 'Original'),
    plotoneviolin(cv_imp.standard, 'Z-Score'),
    plotoneviolin(cv_imp.minmax, 'Min-Max'),
    plotoneviolin(cv_imp.med, 'Median Scaling'),
    nrow=2,ncol=2
)

ggarrange(
    plotoneviolin(cv_imp.mad, 'MAD Scaling'),
    plotoneviolin(cv_imp.lm, 'Linear Regression'),
    plotoneviolin(cv_imp.vsn, 'VSN'),
    plotoneviolin(cv_imp.eig, 'EigenMS'),
    nrow=2,ncol=2
)
```

### Random Forest

```{r CV3}
cv_RF.before <- data.frame(KO=apply(2^LFQ_KO_RF, 1, cv), WT=apply(2^LFQ_WT_RF,1,cv))
cv_RF.standard <- data.frame(KO=apply(2^LFQ_KO_RF.standard, 1, cv), WT=apply(2^LFQ_WT_RF.standard, 1, cv))
cv_RF.minmax <- data.frame(KO=apply(2^LFQ_KO_RF.minmax, 1, cv), WT= apply(2^LFQ_WT_RF.minmax, 1, cv))
cv_RF.med <- data.frame(KO=apply(2^LFQ_KO_RF.med, 1, cv), WT=apply(2^LFQ_WT_RF.med, 1, cv))
cv_RF.mad <- data.frame(KO=apply(2^LFQ_KO_RF.mad, 1, cv), WT=apply(2^LFQ_WT_RF.mad, 1, cv))
cv_RF.lm <- data.frame(KO=apply(2^LFQ_KO_RF.lm, 1, cv), WT=apply(2^LFQ_WT_RF.lm, 1, cv))
cv_RF.vsn <- data.frame(KO=apply(2^LFQ_KO_RF.vsn, 1, cv), WT=apply(2^LFQ_WT_RF.vsn, 1, cv))
cv_RF.eig <- data.frame(KO=apply(2^LFQ_KO_RF.eigen, 1, cv), WT=apply(2^LFQ_WT_RF.eigen, 1, cv))

ggarrange(
    plotoneviolin(cv_RF.before, 'Original'),
    plotoneviolin(cv_RF.standard, 'Z-Score'),
    plotoneviolin(cv_RF.minmax, 'Min-Max'),
    plotoneviolin(cv_RF.med, 'Median Scaling'),
    nrow=2,ncol=2
)

ggarrange(
    plotoneviolin(cv_RF.mad, 'MAD Scaling'),
    plotoneviolin(cv_RF.lm, 'Linear Regression'),
    plotoneviolin(cv_RF.vsn, 'VSN'),
    plotoneviolin(cv_RF.eig, 'EigenMS'),
    nrow=2,ncol=2
)
```

### K-Nearest Neighbors

```{r CV4}
cv_kNN.before <- data.frame(KO=apply(2^LFQ_KO_kNN, 1, cv), WT=apply(2^LFQ_WT_kNN,1,cv))
cv_kNN.standard <- data.frame(KO=apply(2^LFQ_KO_kNN.standard, 1, cv), WT=apply(2^LFQ_WT_kNN.standard, 1, cv))
cv_kNN.minmax <- data.frame(KO=apply(2^LFQ_KO_kNN.minmax, 1, cv), WT= apply(2^LFQ_WT_kNN.minmax, 1, cv))
cv_kNN.med <- data.frame(KO=apply(2^LFQ_KO_kNN.med, 1, cv), WT=apply(2^LFQ_WT_kNN.med, 1, cv))
cv_kNN.mad <- data.frame(KO=apply(2^LFQ_KO_kNN.mad, 1, cv), WT=apply(2^LFQ_WT_kNN.mad, 1, cv))
cv_kNN.lm <- data.frame(KO=apply(2^LFQ_KO_kNN.lm, 1, cv), WT=apply(2^LFQ_WT_kNN.lm, 1, cv))
cv_kNN.vsn <- data.frame(KO=apply(2^LFQ_KO_kNN.vsn, 1, cv), WT=apply(2^LFQ_WT_kNN.vsn, 1, cv))
cv_kNN.eig <- data.frame(KO=apply(2^LFQ_KO_kNN.eigen, 1, cv), WT=apply(2^LFQ_WT_kNN.eigen, 1, cv))

ggarrange(
    plotoneviolin(cv_kNN.before, 'Original'),
    plotoneviolin(cv_kNN.standard, 'Z-Score'),
    plotoneviolin(cv_kNN.minmax, 'Min-Max'),
    plotoneviolin(cv_kNN.med, 'Median Scaling'),
    nrow=2,ncol=2
)

ggarrange(
    plotoneviolin(cv_kNN.mad, 'MAD Scaling'),
    plotoneviolin(cv_kNN.lm, 'Linear Regression'),
    plotoneviolin(cv_kNN.vsn, 'VSN'),
    plotoneviolin(cv_kNN.eig, 'EigenMS'),
    nrow=2,ncol=2
)
```

### Ludovic

```{r CV5}
cv_ludovic.before <- data.frame(KO=apply(2^LFQ_KO_ludovic, 1, cv), WT=apply(2^LFQ_WT_ludovic,1,cv))
cv_ludovic.standard <- data.frame(KO=apply(2^LFQ_KO_ludovic.standard, 1, cv), WT=apply(2^LFQ_WT_ludovic.standard, 1, cv))
cv_ludovic.minmax <- data.frame(KO=apply(2^LFQ_KO_ludovic.minmax, 1, cv), WT= apply(2^LFQ_WT_ludovic.minmax, 1, cv))
cv_ludovic.med <- data.frame(KO=apply(2^LFQ_KO_ludovic.med, 1, cv), WT=apply(2^LFQ_WT_ludovic.med, 1, cv))
cv_ludovic.mad <- data.frame(KO=apply(2^LFQ_KO_ludovic.mad, 1, cv), WT=apply(2^LFQ_WT_ludovic.mad, 1, cv))
cv_ludovic.lm <- data.frame(KO=apply(2^LFQ_KO_ludovic.lm, 1, cv), WT=apply(2^LFQ_WT_ludovic.lm, 1, cv))
cv_ludovic.vsn <- data.frame(KO=apply(2^LFQ_KO_ludovic.vsn, 1, cv), WT=apply(2^LFQ_WT_ludovic.vsn, 1, cv))
cv_ludovic.eig <- data.frame(KO=apply(2^LFQ_KO_ludovic.eigen, 1, cv), WT=apply(2^LFQ_WT_ludovic.eigen, 1, cv))

ggarrange(
    plotoneviolin(cv_ludovic.before, 'Original'),
    plotoneviolin(cv_ludovic.standard, 'Z-Score'),
    plotoneviolin(cv_ludovic.minmax, 'Min-Max'),
    plotoneviolin(cv_ludovic.med, 'Median Scaling'),
    nrow=2,ncol=2
)

ggarrange(
    plotoneviolin(cv_ludovic.mad, 'MAD Scaling'),
    plotoneviolin(cv_ludovic.lm, 'Linear Regression'),
    plotoneviolin(cv_ludovic.vsn, 'VSN'),
    plotoneviolin(cv_ludovic.eig, 'EigenMS'),
    nrow=2,ncol=2
)
```

### Summary

```{r}
cv_all <- data.frame(cv.before, cv.standard, cv.minmax, cv.med,
                     cv.mad, cv.lm, cv.vsn)
colnames(cv_all) <- c('Before_KO', 'Before_WT', 'Z-score_KO', 'Z-score_WT',
                          'MinMax_KO', 'MinMax_WT', 'Median_KO', 'Median_WT', 
                          'MAD_KO', 'MAD_WT', 'Linear_KO', 'Linear_WT', 
                          'VSN_KO', 'VSN_WT')
colnames(cv.eig) <- c('EigenMS_KO', 'EigenMS_WT')

cv_imp_all <- data.frame(cv_imp.before, cv_imp.standard, cv_imp.minmax, cv_imp.med,
                         cv_imp.mad, cv_imp.lm, cv_imp.vsn, cv_imp.eig)
colnames(cv_imp_all) <- c('Before_KO', 'Before_WT', 'Z-score_KO', 'Z-score_WT',
                          'MinMax_KO', 'MinMax_WT', 'Median_KO', 'Median_WT', 
                          'MAD_KO', 'MAD_WT', 'Linear_KO', 'Linear_WT', 
                          'VSN_KO', 'VSN_WT', 'EigenMS_KO', 'EigenMS_WT')

cv_RF_all <- data.frame(cv_RF.before, cv_RF.standard, cv_RF.minmax, cv_RF.med,
                         cv_RF.mad, cv_RF.lm, cv_RF.vsn, cv_RF.eig)
colnames(cv_RF_all) <- c('Before_KO', 'Before_WT', 'Z-score_KO', 'Z-score_WT',
                          'MinMax_KO', 'MinMax_WT', 'Median_KO', 'Median_WT', 
                          'MAD_KO', 'MAD_WT', 'Linear_KO', 'Linear_WT', 
                          'VSN_KO', 'VSN_WT', 'EigenMS_KO', 'EigenMS_WT')

cv_kNN_all <- data.frame(cv_kNN.before, cv_kNN.standard, cv_kNN.minmax, cv_kNN.med,
                         cv_kNN.mad, cv_kNN.lm, cv_kNN.vsn, cv_kNN.eig)
colnames(cv_kNN_all) <- c('Before_KO', 'Before_WT', 'Z-score_KO', 'Z-score_WT',
                          'MinMax_KO', 'MinMax_WT', 'Median_KO', 'Median_WT', 
                          'MAD_KO', 'MAD_WT', 'Linear_KO', 'Linear_WT', 
                          'VSN_KO', 'VSN_WT', 'EigenMS_KO', 'EigenMS_WT')

cv_ludovic_all <- data.frame(cv_ludovic.before, cv_ludovic.standard, cv_ludovic.minmax, cv_ludovic.med,
                         cv_ludovic.mad, cv_ludovic.lm, cv_ludovic.vsn, cv_ludovic.eig)
colnames(cv_ludovic_all) <- c('Before_KO', 'Before_WT', 'Z-score_KO', 'Z-score_WT',
                          'MinMax_KO', 'MinMax_WT', 'Median_KO', 'Median_WT', 
                          'MAD_KO', 'MAD_WT', 'Linear_KO', 'Linear_WT', 
                          'VSN_KO', 'VSN_WT', 'EigenMS_KO', 'EigenMS_WT')
```

```{r}
#### NON-IMPUTED ####
cv_summary_KO <- cv_all %>%
    select(contains('KO')) %>%
    pivot_longer(everything(), names_to = 'method KO', values_to = 'LFQ_CV') %>%
    group_by(`method KO`) %>%
    summarise('mean_KO'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_KO'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_KO'=round(sd(LFQ_CV, na.rm=TRUE),5))

cv_summary_eigen_KO <- tibble(cv.eig) %>%
    select(contains('KO')) %>%
    pivot_longer(everything(), names_to = 'method KO', values_to = 'LFQ_CV') %>%
    group_by(`method KO`) %>%
    summarise('mean_KO'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_KO'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_KO'=round(sd(LFQ_CV, na.rm=TRUE),5))

cv_summary_WT <- cv_all %>%
    select(contains('WT')) %>%
    pivot_longer(everything(), names_to = 'method WT', values_to = 'LFQ_CV') %>%
    group_by(`method WT`) %>%
    summarise('mean_WT'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_WT'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_WT'=round(sd(LFQ_CV, na.rm=TRUE),5))

cv_summary_eigen_WT <- tibble(cv.eig) %>%
    select(contains('WT')) %>%
    pivot_longer(everything(), names_to = 'method WT', values_to = 'LFQ_CV') %>%
    group_by(`method WT`) %>%
    summarise('mean_WT'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_WT'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_WT'=round(sd(LFQ_CV, na.rm=TRUE),5))

#### MATI CODE ####
cv_imp_summary_KO <- cv_imp_all %>%
    select(contains('KO')) %>%
    pivot_longer(everything(), names_to = 'method KO', values_to = 'LFQ_CV') %>%
    group_by(`method KO`) %>%
    summarise('mean_KO'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_KO'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_KO'=round(sd(LFQ_CV, na.rm=TRUE),5))

cv_imp_summary_WT <- cv_imp_all %>%
    select(contains('WT')) %>%
    pivot_longer(everything(), names_to = 'method WT', values_to = 'LFQ_CV') %>%
    group_by(`method WT`) %>%
    summarise('mean_WT'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_WT'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_WT'=round(sd(LFQ_CV, na.rm=TRUE),5))

#### RANDOM FOREST ####
cv_RF_summary_KO <- cv_RF_all %>%
    select(contains('KO')) %>%
    pivot_longer(everything(), names_to = 'method KO', values_to = 'LFQ_CV') %>%
    group_by(`method KO`) %>%
    summarise('mean_KO'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_KO'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_KO'=round(sd(LFQ_CV, na.rm=TRUE),5))

cv_RF_summary_WT <- cv_RF_all %>%
    select(contains('WT')) %>%
    pivot_longer(everything(), names_to = 'method WT', values_to = 'LFQ_CV') %>%
    group_by(`method WT`) %>%
    summarise('mean_WT'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_WT'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_WT'=round(sd(LFQ_CV, na.rm=TRUE),5))

#### K-NEAREST NEIGHBORS ####
cv_kNN_summary_KO <- cv_kNN_all %>%
    select(contains('KO')) %>%
    pivot_longer(everything(), names_to = 'method KO', values_to = 'LFQ_CV') %>%
    group_by(`method KO`) %>%
    summarise('mean_KO'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_KO'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_KO'=round(sd(LFQ_CV, na.rm=TRUE),5))

cv_kNN_summary_WT <- cv_kNN_all %>%
    select(contains('WT')) %>%
    pivot_longer(everything(), names_to = 'method WT', values_to = 'LFQ_CV') %>%
    group_by(`method WT`) %>%
    summarise('mean_WT'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_WT'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_WT'=round(sd(LFQ_CV, na.rm=TRUE),5))

#### LUDOVIC ####
cv_ludovic_summary_KO <- cv_ludovic_all %>%
    select(contains('KO')) %>%
    pivot_longer(everything(), names_to = 'method KO', values_to = 'LFQ_CV') %>%
    group_by(`method KO`) %>%
    summarise('mean_KO'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_KO'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_KO'=round(sd(LFQ_CV, na.rm=TRUE),5))

cv_ludovic_summary_WT <- cv_ludovic_all %>%
    select(contains('WT')) %>%
    pivot_longer(everything(), names_to = 'method WT', values_to = 'LFQ_CV') %>%
    group_by(`method WT`) %>%
    summarise('mean_WT'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_WT'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_WT'=round(sd(LFQ_CV, na.rm=TRUE),5))

#### SUMMARY ####
cv_summary <- as.data.frame(cbind(rbind(cv_summary_KO, cv_summary_eigen_KO),
                                  rbind(cv_summary_WT[-1], cv_summary_eigen_WT[-1])))
colnames(cv_summary)[1] <- 'method'
cv_summary$method <- gsub('_KO', '', cv_summary$method)
cv_summary <- cv_summary |> arrange(method)

cv_imp_summary <- as.data.frame(cbind(cv_imp_summary_KO, cv_imp_summary_WT[-1]))
colnames(cv_imp_summary)[1] <- 'method'
cv_imp_summary$method <- gsub('_KO', '', cv_imp_summary$method)

cv_RF_summary <- as.data.frame(cbind(cv_RF_summary_KO, cv_RF_summary_WT[-1]))
colnames(cv_RF_summary)[1] <- 'method'
cv_RF_summary$method <- gsub('_KO', '', cv_RF_summary$method)

cv_kNN_summary <- as.data.frame(cbind(cv_kNN_summary_KO, cv_kNN_summary_WT[-1]))
colnames(cv_kNN_summary)[1] <- 'method'
cv_kNN_summary$method <- gsub('_KO', '', cv_kNN_summary$method)

cv_ludovic_summary <- as.data.frame(cbind(cv_ludovic_summary_KO, cv_ludovic_summary_WT[-1]))
colnames(cv_ludovic_summary)[1] <- 'method'
cv_ludovic_summary$method <- gsub('_KO', '', cv_ludovic_summary$method)
```

#### Non-Imputed 

```{r}
show_cv_table(cv_summary)
```

#### Imputed - Mati Code 

```{r}
show_cv_table(cv_imp_summary)
```

#### Random Forest 

```{r}
show_cv_table(cv_RF_summary)
```

#### K-Nearest Neighbors 

```{r}
show_cv_table(cv_kNN_summary)
```

#### Ludovic

```{r}
show_cv_table(cv_ludovic_summary)
```

## ICC {.tabset}

After normalization, biological replicates should group more tightly. You can 
assess this by measuring the intraclass correlation coefficient (ICC) to see if 
replicates cluster together.

A guidelines for interpretation by [Koo and Li (2016)](https://doi.org/10.1016%2Fj.jcm.2016.02.012):

* below 0.50: poor <span style="color:#D3D3D3;font-size:26px">■</span>
* between 0.50 and 0.75: moderate <span style="color:#aefda1;font-size:26px">■</span>
* between 0.75 and 0.90: good <span style="color:#6dff54;font-size:26px">■</span>
* above 0.90: excellent <span style="color:#1bb400;font-size:26px">■</span>

**Summary**
We can clearly see that the replicates are the most tight after the normalization 
using the **EigenMS** method. Almost all methods group replicates more tightly, 
except the **Min-Max** and **Median** methods, where the ICCs were moderate or good.


### Non-Imputed

```{r ICC1}
icc.KO.before <- irr::icc(2^LFQ_KO)$value
icc.KO.standard <- irr::icc(2^LFQ_KO.standard)$value
icc.KO.minmax <- irr::icc(2^LFQ_KO.minmax)$value
icc.KO.med <- irr::icc(2^LFQ_KO.med)$value
icc.KO.mad <- irr::icc(2^LFQ_KO.mad)$value
icc.KO.lm <- irr::icc(2^LFQ_KO.lm)$value
icc.KO.vsn <- irr::icc(2^LFQ_KO.vsn)$value
icc.KO.eig <- irr::icc(2^LFQ_KO.eigen)$value

icc.KO.all <- data.frame(`ICC KO`=c(icc.KO.before, icc.KO.standard, icc.KO.minmax, icc.KO.med,
                                    icc.KO.mad, icc.KO.lm, icc.KO.vsn, icc.KO.eig))
rownames(icc.KO.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'EigenMS')

icc.WT.before <- irr::icc(2^LFQ_WT)$value
icc.WT.standard <- irr::icc(2^LFQ_WT.standard)$value
icc.WT.minmax <- irr::icc(2^LFQ_WT.minmax)$value
icc.WT.med <- irr::icc(2^LFQ_WT.med)$value
icc.WT.mad <- irr::icc(2^LFQ_WT.mad)$value
icc.WT.lm <- irr::icc(2^LFQ_WT.lm)$value
icc.WT.vsn <- irr::icc(2^LFQ_WT.vsn)$value
icc.WT.eig <- irr::icc(2^LFQ_WT.eigen)$value

icc.WT.all <- data.frame(`ICC WT`=c(icc.WT.before, icc.WT.standard, icc.WT.minmax, icc.WT.med,
                                    icc.WT.mad, icc.WT.lm, icc.WT.vsn, icc.WT.eig))
rownames(icc.WT.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'EigenMS')

DT::datatable(cbind(icc.KO.all, icc.WT.all), 
              options=list(searching=FALSE, paging=FALSE, info=FALSE)) |>
    DT::formatStyle(columns = c('ICC.KO', 'ICC.WT'), 
                    background = DT::styleInterval(c(0.5,0.75,0.9), c('#D3D3D3', '#aefda1', '#6dff54', '#1bb400')))
```

### Mati Code

```{r ICC2}
icc_imp.KO.before <- irr::icc(2^LFQ_KO_imp)$value
icc_imp.KO.standard <- irr::icc(2^LFQ_KO_imp.standard)$value
icc_imp.KO.minmax <- irr::icc(2^LFQ_KO_imp.minmax)$value
icc_imp.KO.med <- irr::icc(2^LFQ_KO_imp.med)$value
icc_imp.KO.mad <- irr::icc(2^LFQ_KO_imp.mad)$value
icc_imp.KO.lm <- irr::icc(2^LFQ_KO_imp.lm)$value
icc_imp.KO.vsn <- irr::icc(2^LFQ_KO_imp.vsn)$value
icc_imp.KO.eig <- irr::icc(2^LFQ_KO_imp.eigen)$value

icc_imp.KO.all <- data.frame(`ICC KO`=c(icc_imp.KO.before, icc_imp.KO.standard, 
                                        icc_imp.KO.minmax, icc_imp.KO.med, 
                                        icc_imp.KO.mad, icc_imp.KO.lm, 
                                        icc_imp.KO.vsn, icc_imp.KO.eig))
rownames(icc_imp.KO.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'EigenMS')

icc_imp.WT.before <- irr::icc(2^LFQ_WT_imp)$value
icc_imp.WT.standard <- irr::icc(2^LFQ_WT_imp.standard)$value
icc_imp.WT.minmax <- irr::icc(2^LFQ_WT_imp.minmax)$value
icc_imp.WT.med <- irr::icc(2^LFQ_WT_imp.med)$value
icc_imp.WT.mad <- irr::icc(2^LFQ_WT_imp.mad)$value
icc_imp.WT.lm <- irr::icc(2^LFQ_WT_imp.lm)$value
icc_imp.WT.vsn <- irr::icc(2^LFQ_WT_imp.vsn)$value
icc_imp.WT.eig <- irr::icc(2^LFQ_WT_imp.eigen)$value

icc_imp.WT.all <- data.frame(`ICC WT`=c(icc_imp.WT.before, icc_imp.WT.standard, 
                                        icc_imp.WT.minmax, icc_imp.WT.med, 
                                        icc_imp.WT.mad, icc_imp.WT.lm, 
                                        icc_imp.WT.vsn, icc_imp.WT.eig))
rownames(icc_imp.WT.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'EigenMS')

DT::datatable(cbind(icc_imp.KO.all, icc_imp.WT.all), 
              options=list(searching=FALSE, paging=FALSE, info=FALSE)) |>
    DT::formatStyle(columns = c('ICC.KO', 'ICC.WT'), 
                    background = DT::styleInterval(c(0.5,0.75,0.9), c('#D3D3D3', '#aefda1', '#6dff54', '#1bb400')))
```

### Random Forest

```{r ICC3}
icc_RF.KO.before <- irr::icc(2^LFQ_KO_RF)$value
icc_RF.KO.standard <- irr::icc(2^LFQ_KO_RF.standard)$value
icc_RF.KO.minmax <- irr::icc(2^LFQ_KO_RF.minmax)$value
icc_RF.KO.med <- irr::icc(2^LFQ_KO_RF.med)$value
icc_RF.KO.mad <- irr::icc(2^LFQ_KO_RF.mad)$value
icc_RF.KO.lm <- irr::icc(2^LFQ_KO_RF.lm)$value
icc_RF.KO.vsn <- irr::icc(2^LFQ_KO_RF.vsn)$value
icc_RF.KO.eig <- irr::icc(2^LFQ_KO_RF.eigen)$value

icc_RF.KO.all <- data.frame(`ICC KO`=c(icc_RF.KO.before, icc_RF.KO.standard, 
                                       icc_RF.KO.minmax, icc_RF.KO.med, 
                                       icc_RF.KO.mad, icc_RF.KO.lm, 
                                       icc_RF.KO.vsn, icc_RF.KO.eig))
rownames(icc_RF.KO.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'EigenMS')

icc_RF.WT.before <- irr::icc(2^LFQ_WT_RF)$value
icc_RF.WT.standard <- irr::icc(2^LFQ_WT_RF.standard)$value
icc_RF.WT.minmax <- irr::icc(2^LFQ_WT_RF.minmax)$value
icc_RF.WT.med <- irr::icc(2^LFQ_WT_RF.med)$value
icc_RF.WT.mad <- irr::icc(2^LFQ_WT_RF.mad)$value
icc_RF.WT.lm <- irr::icc(2^LFQ_WT_RF.lm)$value
icc_RF.WT.vsn <- irr::icc(2^LFQ_WT_RF.vsn)$value
icc_RF.WT.eig <- irr::icc(2^LFQ_WT_RF.eigen)$value

icc_RF.WT.all <- data.frame(`ICC WT`=c(icc_RF.WT.before, icc_RF.WT.standard, 
                                       icc_RF.WT.minmax, icc_RF.WT.med,
                                       icc_RF.WT.mad, icc_RF.WT.lm, 
                                       icc_RF.WT.vsn, icc_RF.WT.eig))
rownames(icc_RF.WT.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'EigenMS')

DT::datatable(cbind(icc_RF.KO.all, icc_RF.WT.all), 
              options=list(searching=FALSE, paging=FALSE, info=FALSE)) |>
    DT::formatStyle(columns = c('ICC.KO', 'ICC.WT'), 
                    background = DT::styleInterval(c(0.5,0.75,0.9), c('#D3D3D3', '#aefda1', '#6dff54', '#1bb400')))
```

### K-Nearest Neighbors

```{r ICC4}
icc_kNN.KO.before <- irr::icc(2^LFQ_KO_kNN)$value
icc_kNN.KO.standard <- irr::icc(2^LFQ_KO_kNN.standard)$value
icc_kNN.KO.minmax <- irr::icc(2^LFQ_KO_kNN.minmax)$value
icc_kNN.KO.med <- irr::icc(2^LFQ_KO_kNN.med)$value
icc_kNN.KO.mad <- irr::icc(2^LFQ_KO_kNN.mad)$value
icc_kNN.KO.lm <- irr::icc(2^LFQ_KO_kNN.lm)$value
icc_kNN.KO.vsn <- irr::icc(2^LFQ_KO_kNN.vsn)$value
icc_kNN.KO.eig <- irr::icc(2^LFQ_KO_kNN.eigen)$value

icc_kNN.KO.all <- data.frame(`ICC KO`=c(icc_kNN.KO.before, icc_kNN.KO.standard, 
                                        icc_kNN.KO.minmax, icc_kNN.KO.med,
                                        icc_kNN.KO.mad, icc_kNN.KO.lm, 
                                        icc_kNN.KO.vsn, icc_kNN.KO.eig))
rownames(icc_kNN.KO.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'EigenMS')

icc_kNN.WT.before <- irr::icc(2^LFQ_WT_kNN)$value
icc_kNN.WT.standard <- irr::icc(2^LFQ_WT_kNN.standard)$value
icc_kNN.WT.minmax <- irr::icc(2^LFQ_WT_kNN.minmax)$value
icc_kNN.WT.med <- irr::icc(2^LFQ_WT_kNN.med)$value
icc_kNN.WT.mad <- irr::icc(2^LFQ_WT_kNN.mad)$value
icc_kNN.WT.lm <- irr::icc(2^LFQ_WT_kNN.lm)$value
icc_kNN.WT.vsn <- irr::icc(2^LFQ_WT_kNN.vsn)$value
icc_kNN.WT.eig <- irr::icc(2^LFQ_WT_kNN.eigen)$value

icc_kNN.WT.all <- data.frame(`ICC WT`=c(icc_kNN.WT.before, icc_kNN.WT.standard, 
                                        icc_kNN.WT.minmax, icc_kNN.WT.med, 
                                        icc_kNN.WT.mad, icc_kNN.WT.lm, 
                                        icc_kNN.WT.vsn, icc_kNN.WT.eig))
rownames(icc_kNN.WT.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'EigenMS')

DT::datatable(cbind(icc_kNN.KO.all, icc_kNN.WT.all), 
              options=list(searching=FALSE, paging=FALSE, info=FALSE)) |>
    DT::formatStyle(columns = c('ICC.KO', 'ICC.WT'), 
                    background = DT::styleInterval(c(0.5,0.75,0.9), c('#D3D3D3', '#aefda1', '#6dff54', '#1bb400')))
```

### Ludovic

```{r ICC5}
icc_ludovic.KO.before <- irr::icc(2^LFQ_KO_ludovic)$value
icc_ludovic.KO.standard <- irr::icc(2^LFQ_KO_ludovic.standard)$value
icc_ludovic.KO.minmax <- irr::icc(2^LFQ_KO_ludovic.minmax)$value
icc_ludovic.KO.med <- irr::icc(2^LFQ_KO_ludovic.med)$value
icc_ludovic.KO.mad <- irr::icc(2^LFQ_KO_ludovic.mad)$value
icc_ludovic.KO.lm <- irr::icc(2^LFQ_KO_ludovic.lm)$value
icc_ludovic.KO.vsn <- irr::icc(2^LFQ_KO_ludovic.vsn)$value
icc_ludovic.KO.eig <- irr::icc(2^LFQ_KO_ludovic.eigen)$value

icc_ludovic.KO.all <- data.frame(`ICC KO`=c(icc_ludovic.KO.before, icc_ludovic.KO.standard,
                                            icc_ludovic.KO.minmax, icc_ludovic.KO.med, 
                                            icc_ludovic.KO.mad, icc_ludovic.KO.lm, 
                                            icc_ludovic.KO.vsn, icc_ludovic.KO.eig))
rownames(icc_ludovic.KO.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'EigenMS')

icc_ludovic.WT.before <- irr::icc(2^LFQ_WT_ludovic)$value
icc_ludovic.WT.standard <- irr::icc(2^LFQ_WT_ludovic.standard)$value
icc_ludovic.WT.minmax <- irr::icc(2^LFQ_WT_ludovic.minmax)$value
icc_ludovic.WT.med <- irr::icc(2^LFQ_WT_ludovic.med)$value
icc_ludovic.WT.mad <- irr::icc(2^LFQ_WT_ludovic.mad)$value
icc_ludovic.WT.lm <- irr::icc(2^LFQ_WT_ludovic.lm)$value
icc_ludovic.WT.vsn <- irr::icc(2^LFQ_WT_ludovic.vsn)$value
icc_ludovic.WT.eig <- irr::icc(2^LFQ_WT_ludovic.eigen)$value

icc_ludovic.WT.all <- data.frame(`ICC WT`=c(icc_ludovic.WT.before, icc_ludovic.WT.standard, 
                                            icc_ludovic.WT.minmax, icc_ludovic.WT.med, 
                                            icc_ludovic.WT.mad, icc_ludovic.WT.lm, 
                                            icc_ludovic.WT.vsn, icc_ludovic.WT.eig))
rownames(icc_ludovic.WT.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'EigenMS')

DT::datatable(cbind(icc_ludovic.KO.all, icc_ludovic.WT.all), 
              options=list(searching=FALSE, paging=FALSE, info=FALSE)) |>
    DT::formatStyle(columns = c('ICC.KO', 'ICC.WT'), 
                    background = DT::styleInterval(c(0.5,0.75,0.9), c('#D3D3D3', '#aefda1', '#6dff54', '#1bb400')))
```

## t-test {.tabset}

### Raw Data

**Summary** <br>
The p-value plots show us that we had to normalize the data, because the output
below doesn't show strong differences between KO and WT cells.

```{r p-value raw data}
df <- lfq
df[is.na(df)] <- 0

pvalue <- df |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(df, 1, ttest, 
                  grp1=grep("KO", colnames(df)), 
                  grp2=grep("WT", colnames(df)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p <- ggplot(data = pvalue, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Non-Imputed - p.value') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 4, color = "black")
```

```{r p-value imp data}
pvalue_imp <- lfq_imp |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(lfq_imp, 1, ttest, 
                  grp1=grep("KO", colnames(lfq_imp)), 
                  grp2=grep("WT", colnames(lfq_imp)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_imp$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_imp <- ggplot(data = pvalue_imp, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed Mati Code - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value RF data}
pvalue_RF <- imputed_RF |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(imputed_RF, 1, ttest, 
                  grp1=grep("KO", colnames(imputed_RF)), 
                  grp2=grep("WT", colnames(imputed_RF)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_RF$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_RF <- ggplot(data = pvalue_RF, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed RF - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value kNN data}
# handling with duplicates
duplicates <- which(lapply(1:nrow(imputed_kNN), function(i) length(unique(as.numeric(imputed_kNN[i,])))) != 6)
df_kNN <- imputed_kNN[-duplicates,]

pvalue_kNN <- df_kNN |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(df_kNN, 1, ttest, 
                  grp1=grep("KO", colnames(df_kNN)), 
                  grp2=grep("WT", colnames(df_kNN)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_kNN$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_kNN <- ggplot(data = pvalue_kNN, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed kNN - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value ludovic data}
pvalue_ludovic <- imputed_ludovic |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(imputed_ludovic, 1, ttest, 
                  grp1=grep("KO", colnames(imputed_ludovic)), 
                  grp2=grep("WT", colnames(imputed_ludovic)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_ludovic$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_ludovic <- ggplot(data = pvalue_ludovic, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed ludovic - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r fig.height=3, fig.width=5}
hist_p
```

```{r}
ggarrange(
    hist_p_imp,
    hist_p_RF,
    hist_p_kNN,
    hist_p_ludovic,
    nrow=2,ncol=2
)
```

### Z-Score

**Summary** <br>
The p-value plots look very bad, which means that we have lost whole differences 
between the samples.

```{r p-value raw data standardization}
standard_df <- data.frame(LFQ_KO.standard, LFQ_WT.standard)
standard_df[is.na(standard_df)] <- 0

pvalue_standard <- standard_df |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(standard_df, 1, ttest, 
                  grp1=grep("KO", colnames(standard_df)), 
                  grp2=grep("WT", colnames(standard_df)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_standard$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_standard <- ggplot(data = pvalue_standard, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Non-Imputed - p.value') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 4, color = "black")
```

```{r p-value imputed data standardization}
standard_df_imp <- data.frame(LFQ_KO_imp.standard, LFQ_WT_imp.standard)

pvalue_standard_imp <- standard_df_imp |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(standard_df_imp, 1, ttest, 
                  grp1=grep("KO", colnames(standard_df_imp)), 
                  grp2=grep("WT", colnames(standard_df_imp)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_standard_imp$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_standard_imp <- ggplot(data = pvalue_standard_imp, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed Mati Code - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value rf data standardization}
standard_df_RF <- data.frame(LFQ_KO_RF.standard, LFQ_WT_RF.standard)

pvalue_standard_RF <- standard_df_RF |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(standard_df_RF, 1, ttest, 
                  grp1=grep("KO", colnames(standard_df_RF)), 
                  grp2=grep("WT", colnames(standard_df_RF)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_standard_RF$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_standard_RF <- ggplot(data = pvalue_standard_RF, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed RF - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value kNN data standardization}
standard_df_kNN <- data.frame(LFQ_KO_kNN.standard, LFQ_WT_kNN.standard)

pvalue_standard_kNN <- standard_df_kNN |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(standard_df_kNN, 1, ttest, 
                  grp1=grep("KO", colnames(standard_df_kNN)), 
                  grp2=grep("WT", colnames(standard_df_kNN)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_standard_kNN$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_standard_kNN <- ggplot(data = pvalue_standard_kNN, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed kNN - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value ludovic data standardization}
standard_df_ludovic <- data.frame(LFQ_KO_ludovic.standard, LFQ_WT_ludovic.standard)

pvalue_standard_ludovic <- standard_df_ludovic |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(standard_df_ludovic, 1, ttest, 
                  grp1=grep("KO", colnames(standard_df_ludovic)), 
                  grp2=grep("WT", colnames(standard_df_ludovic)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_standard_ludovic$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_standard_ludovic <- ggplot(data = pvalue_standard_ludovic, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed ludovic - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r fig.height=3, fig.width=5}
hist_p_standard
```

```{r}
ggarrange(
    hist_p_standard_imp,
    hist_p_standard_RF,
    hist_p_standard_kNN,
    hist_p_standard_ludovic,
    nrow=2,ncol=2
)
```

### Min-Max

**Summary** <br>
Again the p-value plots look very bad, which means that we have again lost whole 
differences between the samples.

```{r p-value raw data minmax}
minmax_df <- data.frame(LFQ_KO.minmax, LFQ_WT.minmax)
minmax_df[is.na(minmax_df)] <- 0

pvalue_minmax <- minmax_df |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(minmax_df, 1, ttest, 
                  grp1=grep("KO", colnames(minmax_df)), 
                  grp2=grep("WT", colnames(minmax_df)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_minmax$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_minmax <- ggplot(data = pvalue_minmax, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Non-Imputed - p.value') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 4, color = "black")
```

```{r p-value imputed data minmax}
minmax_df_imp <- data.frame(LFQ_KO_imp.minmax, LFQ_WT_imp.minmax)

pvalue_minmax_imp <- minmax_df_imp |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(minmax_df_imp, 1, ttest, 
                  grp1=grep("KO", colnames(minmax_df_imp)), 
                  grp2=grep("WT", colnames(minmax_df_imp)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_minmax_imp$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_minmax_imp <- ggplot(data = pvalue_minmax_imp, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed Mati Code - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value rf data minmax}
minmax_df_RF <- data.frame(LFQ_KO_RF.minmax, LFQ_WT_RF.minmax)

pvalue_minmax_RF <- minmax_df_RF |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(minmax_df_RF, 1, ttest, 
                  grp1=grep("KO", colnames(minmax_df_RF)), 
                  grp2=grep("WT", colnames(minmax_df_RF)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_minmax_RF$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_minmax_RF <- ggplot(data = pvalue_minmax_RF, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed RF - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value kNN data minmax}
minmax_df_kNN <- data.frame(LFQ_KO_kNN.minmax, LFQ_WT_kNN.minmax)

pvalue_minmax_kNN <- minmax_df_kNN |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(minmax_df_kNN, 1, ttest, 
                  grp1=grep("KO", colnames(minmax_df_kNN)), 
                  grp2=grep("WT", colnames(minmax_df_kNN)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_minmax_kNN$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_minmax_kNN <- ggplot(data = pvalue_minmax_kNN, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed kNN - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value ludovic data minmax}
minmax_df_ludovic <- data.frame(LFQ_KO_ludovic.minmax, LFQ_WT_ludovic.minmax)

pvalue_minmax_ludovic <- minmax_df_ludovic |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(minmax_df_ludovic, 1, ttest, 
                  grp1=grep("KO", colnames(minmax_df_ludovic)), 
                  grp2=grep("WT", colnames(minmax_df_ludovic)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_minmax_ludovic$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_minmax_ludovic <- ggplot(data = pvalue_minmax_ludovic, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed ludovic - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r fig.height=3, fig.width=5}
hist_p_minmax
```

```{r}
ggarrange(
    hist_p_minmax_imp,
    hist_p_minmax_RF,
    hist_p_minmax_kNN,
    hist_p_minmax_ludovic,
    nrow=2,ncol=2
)
```

### Median Scaling

**Summary**<br>
Again the p-value plots look very bad, which means that we have again lost whole 
differences between the samples.

```{r p-value raw data median}
med_df <- data.frame(LFQ_KO.med, LFQ_WT.med)
med_df[is.na(med_df)] <- 0

pvalue_med <- med_df |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(med_df, 1, ttest, 
                  grp1=grep("KO", colnames(med_df)), 
                  grp2=grep("WT", colnames(med_df)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_med$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_med <- ggplot(data = pvalue_med, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Non-Imputed - p.value') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 4, color = "black")
```

```{r p-value imputed data median}
med_df_imp <- data.frame(LFQ_KO_imp.med, LFQ_WT_imp.med)

pvalue_med_imp <- med_df_imp |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(med_df_imp, 1, ttest, 
                  grp1=grep("KO", colnames(med_df_imp)), 
                  grp2=grep("WT", colnames(med_df_imp)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_med_imp$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_med_imp <- ggplot(data = pvalue_med_imp, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed Mati Code - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value rf data median}
med_df_RF <- data.frame(LFQ_KO_RF.med, LFQ_WT_RF.med)

pvalue_med_RF <- med_df_RF |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(med_df_RF, 1, ttest, 
                  grp1=grep("KO", colnames(med_df_RF)), 
                  grp2=grep("WT", colnames(med_df_RF)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_med_RF$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_med_RF <- ggplot(data = pvalue_med_RF, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed RF - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value kNN data median}
med_df_kNN <- data.frame(LFQ_KO_kNN.med, LFQ_WT_kNN.med)

pvalue_med_kNN <- med_df_kNN |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(med_df_kNN, 1, ttest, 
                  grp1=grep("KO", colnames(med_df_kNN)), 
                  grp2=grep("WT", colnames(med_df_kNN)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_med_kNN$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_med_kNN <- ggplot(data = pvalue_med_kNN, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed kNN - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value ludovic data median}
med_df_ludovic <- data.frame(LFQ_KO_ludovic.med, LFQ_WT_ludovic.med)

pvalue_med_ludovic <- med_df_ludovic |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(med_df_ludovic, 1, ttest, 
                  grp1=grep("KO", colnames(med_df_ludovic)), 
                  grp2=grep("WT", colnames(med_df_ludovic)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_med_ludovic$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_med_ludovic <- ggplot(data = pvalue_med_ludovic, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed ludovic - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r fig.height=3, fig.width=5}
hist_p_med
```

```{r}
ggarrange(
    hist_p_med_imp,
    hist_p_med_RF,
    hist_p_med_kNN,
    hist_p_med_ludovic,
    nrow=2,ncol=2
)
```

### Mad Scaling

**Summary**<br>
Again the p-value plots look very bad, which means that we have again lost whole 
differences between the samples.

```{r p-value raw data mad}
mad_df <- data.frame(LFQ_KO.mad, LFQ_WT.mad)
mad_df[is.na(mad_df)] <- 0

pvalue_mad <- mad_df |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(mad_df, 1, ttest, 
                  grp1=grep("KO", colnames(mad_df)), 
                  grp2=grep("WT", colnames(mad_df)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_mad$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_mad <- ggplot(data = pvalue_mad, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Non-Imputed - p.value') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 4, color = "black")
```

```{r p-value imputed data mad}
mad_df_imp <- data.frame(LFQ_KO_imp.mad, LFQ_WT_imp.mad)

pvalue_mad_imp <- mad_df_imp |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(mad_df_imp, 1, ttest, 
                  grp1=grep("KO", colnames(mad_df_imp)), 
                  grp2=grep("WT", colnames(mad_df_imp)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_mad_imp$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_mad_imp <- ggplot(data = pvalue_mad_imp, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed Mati Code - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value rf data mad}
mad_df_RF <- data.frame(LFQ_KO_RF.mad, LFQ_WT_RF.mad)

pvalue_mad_RF <- mad_df_RF |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(mad_df_RF, 1, ttest, 
                  grp1=grep("KO", colnames(mad_df_RF)), 
                  grp2=grep("WT", colnames(mad_df_RF)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_mad_RF$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_mad_RF <- ggplot(data = pvalue_mad_RF, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed RF - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value kNN data mad}
mad_df_kNN <- data.frame(LFQ_KO_kNN.mad, LFQ_WT_kNN.mad)

pvalue_mad_kNN <- mad_df_kNN |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(mad_df_kNN, 1, ttest, 
                  grp1=grep("KO", colnames(mad_df_kNN)), 
                  grp2=grep("WT", colnames(mad_df_kNN)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_mad_kNN$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_mad_kNN <- ggplot(data = pvalue_mad_kNN, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed kNN - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value ludovic data mad}
mad_df_ludovic <- data.frame(LFQ_KO_ludovic.mad, LFQ_WT_ludovic.mad)

pvalue_mad_ludovic <- mad_df_ludovic |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(mad_df_ludovic, 1, ttest, 
                  grp1=grep("KO", colnames(mad_df_ludovic)), 
                  grp2=grep("WT", colnames(mad_df_ludovic)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_mad_ludovic$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_mad_ludovic <- ggplot(data = pvalue_mad_ludovic, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed ludovic - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r fig.height=3, fig.width=5}
hist_p_mad
```

```{r}
ggarrange(
    hist_p_mad_imp,
    hist_p_mad_RF,
    hist_p_mad_kNN,
    hist_p_mad_ludovic,
    nrow=2,ncol=2
)
```

### Linear Regression

**Summary**<br>
Again the p-value plots look very bad, which means that we have again lost whole 
differences between the samples.

```{r p-value raw data lm}
lm_df <- data.frame(LFQ_KO.lm, LFQ_WT.lm)
lm_df[is.na(lm_df)] <- 0

pvalue_lm <- lm_df |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(lm_df, 1, ttest, 
                  grp1=grep("KO", colnames(lm_df)), 
                  grp2=grep("WT", colnames(lm_df)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_lm$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_lm <- ggplot(data = pvalue_lm, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Non-Imputed - p.value') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 4, color = "black")
```

```{r p-value imputed data lm}
lm_df_imp <- data.frame(LFQ_KO_imp.lm, LFQ_WT_imp.lm)

pvalue_lm_imp <- lm_df_imp |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(lm_df_imp, 1, ttest, 
                  grp1=grep("KO", colnames(lm_df_imp)), 
                  grp2=grep("WT", colnames(lm_df_imp)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_lm_imp$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_lm_imp <- ggplot(data = pvalue_lm_imp, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed Mati Code - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value rf data lm}
lm_df_RF <- data.frame(LFQ_KO_RF.lm, LFQ_WT_RF.lm)

pvalue_lm_RF <- lm_df_RF |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(lm_df_RF, 1, ttest, 
                  grp1=grep("KO", colnames(lm_df_RF)), 
                  grp2=grep("WT", colnames(lm_df_RF)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_lm_RF$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_lm_RF <- ggplot(data = pvalue_lm_RF, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed RF - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value kNN data lm}
lm_df_kNN <- data.frame(LFQ_KO_kNN.lm, LFQ_WT_kNN.lm)

pvalue_lm_kNN <- lm_df_kNN |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(lm_df_kNN, 1, ttest, 
                  grp1=grep("KO", colnames(lm_df_kNN)), 
                  grp2=grep("WT", colnames(lm_df_kNN)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_lm_kNN$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_lm_kNN <- ggplot(data = pvalue_lm_kNN, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed kNN - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value ludovic data lm}
lm_df_ludovic <- data.frame(LFQ_KO_ludovic.lm, LFQ_WT_ludovic.lm)

pvalue_lm_ludovic <- lm_df_ludovic |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(lm_df_ludovic, 1, ttest, 
                  grp1=grep("KO", colnames(lm_df_ludovic)), 
                  grp2=grep("WT", colnames(lm_df_ludovic)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_lm_ludovic$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_lm_ludovic <- ggplot(data = pvalue_lm_ludovic, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed ludovic - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r fig.height=3, fig.width=5}
hist_p_lm
```

```{r}
ggarrange(
    hist_p_lm_imp,
    hist_p_lm_RF,
    hist_p_lm_kNN,
    hist_p_lm_ludovic,
    nrow=2,ncol=2
)
```

### VSN

**Summary**<br>
Again the p-value plots look very bad, which means that we have again lost whole 
differences between the samples.

```{r p-value raw data vsn}
vsn_df <- data.frame(LFQ_KO.vsn, LFQ_WT.vsn)
vsn_df[is.na(vsn_df)] <- 0

pvalue_vsn <- vsn_df |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(vsn_df, 1, ttest, 
                  grp1=grep("KO", colnames(vsn_df)), 
                  grp2=grep("WT", colnames(vsn_df)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_vsn$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_vsn <- ggplot(data = pvalue_vsn, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Non-Imputed - p.value') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 4, color = "black")
```

```{r p-value imputed data vsn}
vsn_df_imp <- data.frame(LFQ_KO_imp.vsn, LFQ_WT_imp.vsn)

pvalue_vsn_imp <- vsn_df_imp |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(vsn_df_imp, 1, ttest, 
                  grp1=grep("KO", colnames(vsn_df_imp)), 
                  grp2=grep("WT", colnames(vsn_df_imp)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_vsn_imp$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_vsn_imp <- ggplot(data = pvalue_vsn_imp, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed Mati Code - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value rf data vsn}
vsn_df_RF <- data.frame(LFQ_KO_RF.vsn, LFQ_WT_RF.vsn)

pvalue_vsn_RF <- vsn_df_RF |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(vsn_df_RF, 1, ttest, 
                  grp1=grep("KO", colnames(vsn_df_RF)), 
                  grp2=grep("WT", colnames(vsn_df_RF)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_vsn_RF$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_vsn_RF <- ggplot(data = pvalue_vsn_RF, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed RF - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value kNN data vsn}
vsn_df_kNN <- data.frame(LFQ_KO_kNN.vsn, LFQ_WT_kNN.vsn)

pvalue_vsn_kNN <- vsn_df_kNN |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(vsn_df_kNN, 1, ttest, 
                  grp1=grep("KO", colnames(vsn_df_kNN)), 
                  grp2=grep("WT", colnames(vsn_df_kNN)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_vsn_kNN$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_vsn_kNN <- ggplot(data = pvalue_vsn_kNN, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed kNN - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value ludovic data vsn}
vsn_df_ludovic <- data.frame(LFQ_KO_ludovic.vsn, LFQ_WT_ludovic.vsn)

pvalue_vsn_ludovic <- vsn_df_ludovic |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(vsn_df_ludovic, 1, ttest, 
                  grp1=grep("KO", colnames(vsn_df_ludovic)), 
                  grp2=grep("WT", colnames(vsn_df_ludovic)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_vsn_ludovic$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_vsn_ludovic <- ggplot(data = pvalue_vsn_ludovic, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed ludovic - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r fig.height=3, fig.width=5}
hist_p_vsn
```

```{r}
ggarrange(
    hist_p_vsn_imp,
    hist_p_vsn_RF,
    hist_p_vsn_kNN,
    hist_p_vsn_ludovic,
    nrow=2,ncol=2
)
```

### EigenMS

**Summary**<br>
Finally, we have the expected p-value plots, which means that the EigenMS method 
again has the best results.

```{r p-value raw data eigen}
eigen_df <- data.frame(LFQ_KO.eigen, LFQ_WT.eigen)
eigen_df[is.na(eigen_df)] <- 0

pvalue_eigen <- eigen_df |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(eigen_df, 1, ttest, 
                  grp1=grep("KO", colnames(eigen_df)), 
                  grp2=grep("WT", colnames(eigen_df)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_eigen$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_eigen <- ggplot(data = pvalue_eigen, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Non-Imputed - p.value') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 4, color = "black")
```

```{r p-value imputed data eigen}
eigen_df_imp <- data.frame(LFQ_KO_imp.eigen, LFQ_WT_imp.eigen)

pvalue_eigen_imp <- eigen_df_imp |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(eigen_df_imp, 1, ttest, 
                  grp1=grep("KO", colnames(eigen_df_imp)), 
                  grp2=grep("WT", colnames(eigen_df_imp)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_eigen_imp$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_eigen_imp <- ggplot(data = pvalue_eigen_imp, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed Mati Code - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value rf data eigen}
eigen_df_RF <- data.frame(LFQ_KO_RF.eigen, LFQ_WT_RF.eigen)

pvalue_eigen_RF <- eigen_df_RF |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(eigen_df_RF, 1, ttest, 
                  grp1=grep("KO", colnames(eigen_df_RF)), 
                  grp2=grep("WT", colnames(eigen_df_RF)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_eigen$p_OCIAD1_TOTALS, na.rm=TRUE)
m <- mean(pvalue_eigen_RF$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_eigen_RF <- ggplot(data = pvalue_eigen_RF, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed RF - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value kNN data eigen}
# The EigenMS function doesn't work well with kNN predicted data. In a few rows 
# we didn't have a unique value for each sample. Sometimes there was only one 
# value for all columns and sometimes there were two values, one for KO and the 
# other for WT. That's why we had to remove duplicates from data.

eigen_df_kNN <- data.frame(LFQ_KO_kNN.eigen, LFQ_WT_kNN.eigen)

# handling with duplicates
duplicates <- which(lapply(1:nrow(eigen_df_kNN), function(i) length(unique(as.numeric(eigen_df_kNN[i,])))) != 6)
eigen_df_kNN <- eigen_df_kNN[-duplicates,]

pvalue_eigen_kNN <- eigen_df_kNN |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(eigen_df_kNN, 1, ttest, 
                  grp1=grep("KO", colnames(eigen_df_kNN)), 
                  grp2=grep("WT", colnames(eigen_df_kNN)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_eigen_kNN$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_eigen_kNN <- ggplot(data = pvalue_eigen_kNN, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed kNN - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r p-value ludovic data eigen}
eigen_df_ludovic <- data.frame(LFQ_KO_ludovic.eigen, LFQ_WT_ludovic.eigen)

pvalue_eigen_ludovic <- eigen_df_ludovic |>
    dplyr::mutate("p_OCIAD1_TOTALS" = apply(eigen_df_ludovic, 1, ttest, 
                  grp1=grep("KO", colnames(eigen_df_ludovic)), 
                  grp2=grep("WT", colnames(eigen_df_ludovic)))) |>
    dplyr::mutate(significant = ifelse(p_OCIAD1_TOTALS < p.cutoff, TRUE, FALSE))
m <- mean(pvalue_eigen_ludovic$p_OCIAD1_TOTALS, na.rm=TRUE)

hist_p_eigen_ludovic <- ggplot(data = pvalue_eigen_ludovic, aes(x = p_OCIAD1_TOTALS, fill=significant)) + 
    geom_histogram(bins = 100) + ggtitle('Imputed ludovic - p.value') + theme(legend.position = 'none') +
    annotate("text", x = Inf, y = Inf, label = paste("Mean p-value:", round(m, 3)), 
             hjust = 1.1, vjust = 1.3, size = 3.5, color = "black")
```

```{r fig.height=3, fig.width=5}
hist_p_eigen
```

```{r}
ggarrange(
    hist_p_eigen_imp,
    hist_p_eigen_RF,
    hist_p_eigen_kNN,
    hist_p_eigen_ludovic,
    nrow=2,ncol=2
)
```

# Conclusion

Summarizing everything above, we can safely say that the **EigenMS** method gives 
the best results. The distributions of the two normalization methods (VSN and EigenMS) 
were the best until the very end, but in the statistical metrics the EigenMS method 
won. It&nbsp;is worth noting that the correct shape of the *p-value* plot was only obtained 
for the data normalized with the **EigenMS** method.

About the imputation method. 

1. We can safely say that the **Random Forest** algorithm and the **Ludovic** method 
had the best performance. 
2. The **kNN** method corrects the shift in the distribution of the KO 24 replicates, 
but in the next calculations we discover a strange value in the imputed data. In fact, 
in some rows the values were the same, making it difficult to calculate the t-test 
and introducing an error in the CV calculations. 
3. The method that samples values from the distribution calculated on the raw data 
creates the additional peak of columns that doesn't appear in the raw non-imputed data.

---

# References

1. [A systematic evaluation of normalization methods in quantitative label-free proteomics](https://academic.oup.com/bib/article/19/1/1/2562889)
2. [How to Normalize Data in R for my Data: Methods and Examples](https://rpubs.com/zubairishaq9/how-to-normalize-data-r-my-data)
3. [EigenMS](https://sourceforge.net/projects/eigenms/)
4. [Normalization of peak intensities in bottom-up MS-based proteomics using singular value decomposition](https://pubmed.ncbi.nlm.nih.gov/19602524/)
5. [Metabolomics Data Normalization with EigenMS](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0116221)

```{r test ICC, include=FALSE}
# Maybe we should calculate the ICC like that? Don't know 🤔
# irr::icc(cbind(2^LFQ_KO,2^LFQ_WT))$value
# irr::icc(cbind(2^LFQ_KO.standard,2^LFQ_WT.standard))$value
# irr::icc(cbind(2^LFQ_KO.minmax,2^LFQ_WT.minmax))$value
# irr::icc(cbind(2^LFQ_KO.med,2^LFQ_WT.med))$value
# irr::icc(cbind(2^LFQ_KO.mad,2^LFQ_WT.mad))$value
# irr::icc(cbind(2^LFQ_KO.lm,2^LFQ_WT.lm))$value
# irr::icc(cbind(2^LFQ_KO.vsn,2^LFQ_WT.vsn))$value
# irr::icc(cbind(2^LFQ_KO.eigen,2^LFQ_WT.eigen))$value
```









