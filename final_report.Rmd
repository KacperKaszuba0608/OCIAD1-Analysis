---
title: "Normalization of the Mass Spec Data"
author: "Kacper Kaszuba"
date: "`r Sys.Date()`"
output: 
    html_document:
        toc: true
        toc_float: 
            collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, fig.align='center')

transform_M <- function(i, df) {
    if (df[i,2] == 22) {
        return(median(df[df[,2] == 22,1]))
    } else if (df[i,2] == 23) {
        return(median(df[df[,2] == 23,1]))
    } else {
        return(median(df[df[,2] == 24,1]))
    }
}

plothist <- function(df, title='', plot.title.and.legend=TRUE) {
    if (!is.data.frame(df)) { df <- as.data.frame(df)}
    
    # Assuming LFQ_KO is our data frame with at least 3 columns
    # Reshape the data to a long format
    df <- df %>%
        pivot_longer(cols = 1:ncol(df), names_to = "Rep", values_to = "LFQValue")
    
    # Plot all histograms on the same plot using ggplot
    if (plot.title.and.legend) {
        ret_plot <- ggplot(df, aes(x = LFQValue, fill = Rep)) +
            geom_histogram(alpha = 0.4, position = "identity", bins = 30) +
            labs(title = paste(title), x = "Values", y = "Frequency") +
            theme(legend.title = element_blank())
    } else {
        ret_plot <- ggplot(df, aes(x = LFQValue, fill = Rep)) +
            geom_histogram(alpha = 0.4, position = "identity", bins = 30) +
            labs(x = "Values", y = "Frequency") +
            theme(legend.title = element_blank(), legend.position = 'none')
    }
    return(ret_plot)
}

ordernorm <- function(df) {
    LFQ.22 <- bestNormalize::orderNorm(df[,1])$x.t
    LFQ.23 <- bestNormalize::orderNorm(df[,2])$x.t
    LFQ.24 <- bestNormalize::orderNorm(df[,3])$x.t
    
    if (startsWith(x = colnames(df)[1], 'KO')) {
        return(data.frame(KO_TOTALS_22 = LFQ.22,
                          KO_TOTALS_23 = LFQ.23,
                          KO_TOTALS_24 = LFQ.24))
    } else {
        return(data.frame(WT_TOTALS_22 = LFQ.22,
                          WT_TOTALS_23 = LFQ.23,
                          WT_TOTALS_24 = LFQ.24))
    }
}

plotviolin <- function(df, xlab='', ylab='LFQValue') {
    if (!is.data.frame(df)) { df <- as.data.frame(df)}
    
    df <- df %>%
        pivot_longer(1:ncol(df),names_to = 'Exp', values_to = 'LFQValue') %>%
        mutate(Sample = gsub("_TOTALS_", ".", Exp))
    
    violin <- ggplot(data=df, aes(x=Sample, y=LFQValue, fill=Sample))+
        geom_boxplot(width=0.3) +
        geom_violin(alpha=0.4)+
        theme(legend.position = 'none') +
        labs(x=xlab, y=ylab)
    
    return(violin)
}

plotoneviolin <- function(object, xlab) {
    object <- as.data.frame(object) %>%
        pivot_longer(everything(), names_to = 'Sample', values_to = 'LFQ_CV')
    
    ggplot(data=object, aes(x=Sample, y=LFQ_CV, fill=Sample))+
        geom_boxplot(width=0.2)+
        geom_violin(alpha=0.4)+
        labs(title='',x=xlab,y='LFQ CV[%]')+
        theme(legend.position = 'none',
              panel.background = element_rect(fill='white', colour = 'grey'),
              panel.grid = element_line(colour = 'grey'))
}

library(ggplot2)
library(ggpubr)
library(dplyr)
library(tidyr)
library(vsn)
source('EigenMS/EigenMS/EigenMS.R')
options(scipen=123)
```

# Load Data

```{r data loading}
# non-imputed data
lfq <- read.csv('./data/nonimputed_lfq.csv')
DT::datatable(lfq)
```
<br>
```{r imp data loading}
# imputed data
lfq_imp <- read.csv('./data/LFQ_raw_totals_imp.csv')
colnames(lfq_imp) <- gsub('_TOTALS_', '.I.', colnames(lfq_imp))
DT::datatable(lfq_imp)
```

# Data Preparation

```{r TOTALS data}
# Extracting only TOTALS data for knockout
LFQ_KO <- lfq %>% select(contains('KO'))

# Extracting only TOTALS data for wild type
LFQ_WT <- lfq %>% select(contains('WT'))
```

```{r imp TOTALS data}
# Extracting only TOTALS data for knockout
LFQ_KO_imp <- lfq_imp %>% select(contains('KO'))

# Extracting only TOTALS data for wild type
LFQ_WT_imp <- lfq_imp %>% select(contains('WT'))
```

# Data Mining

First of all, we have to check the distribution of our data.

## Knockout

```{r}
ggarrange(
    plothist(LFQ_KO, 'Non-Imputed'),
    plothist(LFQ_KO_imp, 'Imputed'),
    nrow = 2, ncol = 1
)
```

As we see on the historam of konckout columns, the data are slightly skewed to the right.

```{r}
moments::skewness(cbind(LFQ_KO,LFQ_KO_imp), na.rm=TRUE)
```

The asymmetry score tells use that we need to make some transformation of our data
and confirms the conclusions drawn from the histogram. 

## Wild type

```{r}
ggarrange(
    plothist(LFQ_WT, 'Non-Imputed'),
    plothist(LFQ_WT_imp, 'Imputed'),
    nrow = 2, ncol = 1
)
```

As we see on the historam of wild type columns, the data are slightly skewed to the right.

```{r}
moments::skewness(cbind(LFQ_WT,LFQ_WT_imp), na.rm=TRUE)
```

The asymmetry score tells use that we have to make some transformation of our data 
and confirms the conclusions drawn from the histogram. 

# Transformation - KO

Based on the graphic below, we will first use the natural logarithm.

<center><img src='drabinka_eng.png'></center>
    
## Logarithm

```{r KO logarithmic transformation}
LFQ_KO.log <- log(LFQ_KO) # Non-Imputed
LFQ_KO_imp.log <- log(LFQ_KO_imp) # Imputed
```

```{r echo=FALSE}
ggarrange(
    plothist(LFQ_KO.log, 'Non-Imputed Transformed by Logarithm'),
    plothist(LFQ_KO_imp.log, 'Imputed Transformed by Logarithm'),
    nrow=2,ncol=1
)
```

The data distributions looks better, but they are still skewed to the right side.

```{r}
moments::skewness(cbind(LFQ_KO.log,LFQ_KO_imp.log), na.rm=TRUE)
```

The skewness score is smaler. This is good, but check another transformation.

## Fraction $1/x$

```{r KO fraction trasnformation}
LFQ_KO.frac <- 1/LFQ_KO # Non-Imputed
LFQ_KO_imp.frac <- 1/LFQ_KO_imp # Imputed
```

```{r echo=FALSE}
ggarrange(
    plothist(LFQ_KO.frac, 'Non-Imputed Transformed by Fraction 1/x'),
    plothist(LFQ_KO_imp.frac, 'Imputed Transformed by Fraction 1/x'),
    nrow=2,ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_KO.frac, LFQ_KO_imp.frac), na.rm=TRUE)
```

As we see above the plot and scores of the skewness aren't close to normal distribution.
Our result shows us that with simple transformation we can't transform data and 
make their distribution closer to the Gaussian.

## Box-Cox Transformation

The transformation is based on the formula:
    
$$x' = \Bigg\{ \matrix{\frac{x^{\lambda}-1}{\lambda} & \lambda \neq0 \\
log(x) & \lambda = 0}$$

```{r KO lambda}
### Non-Imputed ###
# At first we have to calculate the lambda
lambda.22 <- car::powerTransform(LFQ_KO$KO.22)$lambda
lambda.23 <- car::powerTransform(LFQ_KO$KO.23)$lambda
lambda.24 <- car::powerTransform(LFQ_KO$KO.24)$lambda
```
<center>
```{r echo=FALSE}
DT::datatable(data.frame(lambda.22, lambda.23, lambda.24, row.names = 'lambda'), 
              options=list(searching=FALSE, paging=FALSE, info=FALSE))
```
</center>
    

<br>
The lambda is not 0, so the first transformation is performed.

```{r KO Box-Cox}
LFQ_KO.boxcox.22 <- car::bcPower(LFQ_KO$KO.22, lambda.22)
LFQ_KO.boxcox.23 <- car::bcPower(LFQ_KO$KO.23, lambda.23)
LFQ_KO.boxcox.24 <- car::bcPower(LFQ_KO$KO.24, lambda.24)
LFQ_KO.boxcox <- data.frame(LFQ_KO.boxcox.22, LFQ_KO.boxcox.23, LFQ_KO.boxcox.24)
colnames(LFQ_KO.boxcox) <- c('KO.22', 'KO.23', 'KO.24')
```

```{r echo=FALSE}
plots1 <- ggarrange(ggplot(LFQ_KO.boxcox) + 
                       geom_histogram(aes(x=KO.22), bins = 10, fill='red', alpha=0.4) + 
                       labs(x='Box-Cox.22',y='Frequency'),
                   ggplot(LFQ_KO.boxcox) + 
                       geom_histogram(aes(x=KO.23), bins = 10, fill='green', alpha=0.4) + 
                       labs(x='Box-Cox.23',y=''),
                   ggplot(LFQ_KO.boxcox) + 
                       geom_histogram(aes(x=KO.24), bins = 10, fill='blue', alpha=0.4) + 
                       labs(x='Box-Cox.24',y=''),
                   ncol=3)

plots1 <- annotate_figure(plots1, top=text_grob('Non-Imputed Transformed Data with Box-Cox',
                                                face='bold', size=13))
```

```{r imp KO lambda}
### Imputed ###
# At first we have to calculate the lambda
lambda.22 <- car::powerTransform(LFQ_KO_imp$KO.I.22)$lambda
lambda.23 <- car::powerTransform(LFQ_KO_imp$KO.I.23)$lambda
lambda.24 <- car::powerTransform(LFQ_KO_imp$KO.I.24)$lambda
```

<center>
```{r echo=FALSE}
DT::datatable(data.frame(lambda.22, lambda.23, lambda.24, row.names = 'lambda'), 
              options=list(searching=FALSE, paging=FALSE, info=FALSE))
```
</center>

<br>
The lambda is not 0, so the first transformation is performed.

```{r imp KO Box-Cox}
LFQ_KO.boxcox.22 <- car::bcPower(LFQ_KO_imp$KO.I.22, lambda.22)
LFQ_KO.boxcox.23 <- car::bcPower(LFQ_KO_imp$KO.I.23, lambda.23)
LFQ_KO.boxcox.24 <- car::bcPower(LFQ_KO_imp$KO.I.24, lambda.24)
LFQ_KO_imp.boxcox <- data.frame(LFQ_KO.boxcox.22, LFQ_KO.boxcox.23, LFQ_KO.boxcox.24)
colnames(LFQ_KO_imp.boxcox) <- c('KO.I.22', 'KO.I.23', 'KO.I.24')
```

```{r echo=FALSE}
plots2 <- ggarrange(ggplot(LFQ_KO_imp.boxcox) + 
                       geom_histogram(aes(x=KO.I.22), bins = 10, fill='red', alpha=0.4) + 
                       labs(x='Box-Cox.22',y='Frequency'),
                   ggplot(LFQ_KO_imp.boxcox) + 
                       geom_histogram(aes(x=KO.I.23), bins = 10, fill='green', alpha=0.4) + 
                       labs(x='Box-Cox.23',y=''),
                   ggplot(LFQ_KO_imp.boxcox) + 
                       geom_histogram(aes(x=KO.I.24), bins = 10, fill='blue', alpha=0.4) + 
                       labs(x='Box-Cox.24',y=''),
                   ncol=3)

plots2 <- annotate_figure(plots2, top=text_grob('Imputed Transformed Data with Box-Cox', 
                                     face='bold', size=13))
```


```{r echo=FALSE}
ggarrange(plots1, plots2, nrow=2, ncol=1)
```

```{r}
moments::skewness(cbind(LFQ_KO.boxcox, LFQ_KO_imp.boxcox), na.rm=TRUE)
```

Finally we have got expected results. The distributions are similar to a normal distribution. ðŸŽ‰

# Normalization - KO

## Z-score Normalization

Z-score normalization standardizes data by subtracting the mean and dividing 
by the standard deviation. This technique transforms data into a distribution 
with a mean of 0 and a standard deviation of 1.

Formula: $\tilde{y}_{ij} = \frac{y_ij - \bar{y_j}}{\theta_j}$, where:

* $y_ij$ - value of the LFQ;
* $\bar{y_j}$ - mean of the LFQ values;
* $\theta_j$ - standard deviation of the LFQ values.

```{r KO standardization}
LFQ_KO.standard <- as.data.frame(scale(LFQ_KO)) # Non-Imputed
LFQ_KO_imp.standard <- as.data.frame(scale(LFQ_KO_imp)) # Imputed
```

```{r echo=FALSE}
ggarrange(
    plothist(LFQ_KO.standard, 'Non-Imputed - Z-score Normalisation'),
    plothist(LFQ_KO_imp.standard, 'Imputed - Z-score Normalisation'),
    nrow=2,ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_KO.standard, LFQ_KO_imp.standard), na.rm=TRUE)
```

Unfortunately the standardization didn't help to bring the distribution closer to the normal dist.

## Min-max Normalization

Min-max normalization scales data to a specified range (usually [0, 1]) by 
subtracting the minimum value and dividing by the range of values.

```{r KO min-max normalization}
min_max_norm <- function(df) {
    df_no_na <- na.omit(df)
    ret <- scale(df_no_na, center = min(df_no_na), scale = max(df_no_na) - min(df_no_na))
    df[which(!is.na(df))] <- ret
    return(df)
}

LFQ_KO.minmax <- as.data.frame(lapply(LFQ_KO, function(col) min_max_norm(col)))  # Non-Imputed
LFQ_KO_imp.minmax <- as.data.frame(lapply(LFQ_KO_imp, function(col) min_max_norm(col)))  # Imputed
```

```{r echo=FALSE}
ggarrange(
    plothist(LFQ_KO.minmax, 'Non-Imputed - Min-Max Normalization'),
    plothist(LFQ_KO_imp.minmax, 'Imputed - Min-Max Normalization'),
    nrow=2, ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_KO.minmax, LFQ_KO_imp.minmax), na.rm=TRUE)
```

Unfortunately, again the normalization didn't help. ðŸ˜’

## Median Scaling

```{r KO median scaling}
LFQ_KO.med <- as.data.frame(DescTools::RobScale(LFQ_KO, scale=FALSE)) # Non-Imputed
LFQ_KO_imp.med <- as.data.frame(DescTools::RobScale(LFQ_KO_imp, scale=FALSE)) # Imputed
```

```{r echo=FALSE}
ggarrange(
    plothist(LFQ_KO.med, 'Non-Imputed - Median Scaling'),
    plothist(LFQ_KO_imp.med, 'Imputed - Median Scaling'),
    nrow=2,ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_KO.med,LFQ_KO_imp.med), na.rm=TRUE)
```

## MAD Scaling

```{r KO mad scaling}
LFQ_KO.mad <- as.data.frame(DescTools::RobScale(LFQ_KO)) # Non-Imputed
LFQ_KO_imp.mad <- as.data.frame(DescTools::RobScale(LFQ_KO_imp)) # Imputed
```

```{r echo=FALSE}
ggarrange(
    plothist(LFQ_KO.mad, 'Non-Imputed - MAD Scaling'),
    plothist(LFQ_KO_imp.mad, 'Imputed - MAD Scaling'),
    nrow=2,ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_KO.mad,LFQ_KO_imp.mad), na.rm=TRUE)
```

Bove Median and Mad Scaling didn't give us the expected result.

## Linear Regression Normalization

```{r KO lm, echo=FALSE}
LFQ_KO_long <- LFQ_KO %>%
    pivot_longer(cols = 1:3, names_to = "Rep", values_to = "LFQValue")

model <- lm(LFQValue ~ Rep, data=LFQ_KO_long)
LFQ_KO_long$normalized <- NA
LFQ_KO_long[which(!is.na(LFQ_KO_long$LFQValue)), 'normalized'] <- residuals(model)

LFQ_KO.lm <- LFQ_KO_long %>% 
    mutate(row = rep(1:nrow(LFQ_KO), each=3)) %>%
    reshape2::dcast(row ~ Rep, value.var='normalized') %>%
    select(-row)

LFQ_KO_long <- LFQ_KO_imp %>%
    pivot_longer(cols = 1:3, names_to = "Rep", values_to = "LFQValue")

model <- lm(LFQValue ~ Rep, data=LFQ_KO_long)
LFQ_KO_long$normalized <- residuals(model)

LFQ_KO_imp.lm <- LFQ_KO_long %>% 
    mutate(row = rep(1:nrow(LFQ_KO), each=3)) %>%
    reshape2::dcast(row ~ Rep, value.var='normalized') %>%
    select(-row)

ggarrange(
    plothist(LFQ_KO.lm, 'Non-Imputed - Linear Normalization'),
    plothist(LFQ_KO_imp.lm, 'Imputed - Linear Normalization'),
    nrow=2,ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_KO.lm, LFQ_KO_imp.lm), na.rm=TRUE)
```

Linear Regression also didn't give us expected result.

# Transformation - WT

Like earlier we will first use the logarithmic transformation.

## Logarithm

```{r WT logarightmic transformation}
LFQ_WT.log <- log(LFQ_WT) # Non-Imputed
LFQ_WT_imp.log <- log(LFQ_WT_imp) # Imputed
```

```{r echo=FALSE}
ggarrange(
    plothist(LFQ_WT.log, 'Non-Imputed Transformed by Logarithm'),
    plothist(LFQ_WT_imp.log, 'Imputed Transformed by Logarithm'),
    nrow=2,ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_WT.log, LFQ_WT_imp.log), na.rm=TRUE)
```

Unfortunantely the result isn't closer to the Gaussian distribution.

## Fraction $1/x$

```{r WT fraction transformation}
LFQ_WT.frac <- 1/LFQ_WT # Non-Imputed
LFQ_WT_imp.frac <- 1/LFQ_WT_imp # Imputed
```

```{r echo=FALSE}
ggarrange(
    plothist(LFQ_WT.frac, 'Non-Imputed Transformed by Fraction 1/x'),
    plothist(LFQ_WT_imp.frac, 'Imputed Transformed by Fraction 1/x'),
    nrow=2,ncol=1
)
```

As we can see above, the results are the same like for the knockout cell type.
With simple methods we can't make the distribution closer to Gaussian dist.


## Box-Cox Transformation

The transformation is based on the formula:

$$x' = \Bigg\{ \matrix{\frac{x^{\lambda}-1}{\lambda} & \lambda \neq0 \\
    log(x) & \lambda = 0}$$
        
```{r WT lambda}
### Non-Imputed ###
# At first we have to calculate the lambda
lambda.22 <- car::powerTransform(LFQ_WT$WT.22)$lambda
lambda.23 <- car::powerTransform(LFQ_WT$WT.23)$lambda
lambda.24 <- car::powerTransform(LFQ_WT$WT.24)$lambda
```
<center>
```{r echo=FALSE}
DT::datatable(data.frame(lambda.22, lambda.23, lambda.24, row.names = 'lambda'), 
              options=list(searching=FALSE, paging=FALSE, info=FALSE))
```
</center>
    
<br>
The lambda is not 0, so the first transformation is performed.

```{r WT Box-Cox}
LFQ_WT.boxcox.22 <- car::bcPower(LFQ_WT$WT.22, lambda.22)
LFQ_WT.boxcox.23 <- car::bcPower(LFQ_WT$WT.23, lambda.23)
LFQ_WT.boxcox.24 <- car::bcPower(LFQ_WT$WT.24, lambda.24)
LFQ_WT.boxcox <- data.frame(LFQ_WT.boxcox.22, LFQ_WT.boxcox.23, LFQ_WT.boxcox.24)
colnames(LFQ_WT.boxcox) <- c('WT.22', 'WT.23', 'WT.24')
```

```{r echo=FALSE}
plots3 <- ggarrange(ggplot(LFQ_WT.boxcox) + 
                        geom_histogram(aes(x=WT.22), bins = 10, fill='red', alpha=0.4) + 
                        labs(x='Box-Cox.22',y='Frequency'),
                    ggplot(LFQ_WT.boxcox) + 
                        geom_histogram(aes(x=WT.23), bins = 10, fill='green', alpha=0.4) + 
                        labs(x='Box-Cox.23',y=''),
                    ggplot(LFQ_WT.boxcox) + 
                        geom_histogram(aes(x=WT.24), bins = 10, fill='blue', alpha=0.4) + 
                        labs(x='Box-Cox.24',y=''),
                    ncol=3)

plots3 <- annotate_figure(plots3, top=text_grob('Non-Imputed Transformed Data with Box-Cox',
                                                face='bold', size=13))
```

Let's check the imputed data.

```{r imp WT lambda}
### Imputed ###
# At first we have to calculate the lambda
lambda.22 <- car::powerTransform(LFQ_WT_imp$WT.I.22)$lambda
lambda.23 <- car::powerTransform(LFQ_WT_imp$WT.I.23)$lambda
lambda.24 <- car::powerTransform(LFQ_WT_imp$WT.I.24)$lambda
```
<center>
```{r echo=FALSE}
DT::datatable(data.frame(lambda.22, lambda.23, lambda.24, row.names = 'lambda'), 
              options=list(searching=FALSE, paging=FALSE, info=FALSE))
```
</center>

<br>
The lambda is not 0, so the first transformation is performed.

```{r imp WT Box-Cox}
LFQ_WT.boxcox.22 <- car::bcPower(LFQ_WT_imp$WT.I.22, lambda.22)
LFQ_WT.boxcox.23 <- car::bcPower(LFQ_WT_imp$WT.I.23, lambda.23)
LFQ_WT.boxcox.24 <- car::bcPower(LFQ_WT_imp$WT.I.24, lambda.24)
LFQ_WT_imp.boxcox <- data.frame(LFQ_WT.boxcox.22, LFQ_WT.boxcox.23, LFQ_WT.boxcox.24)
colnames(LFQ_WT_imp.boxcox) <- c('WT.I.22', 'WT.I.23', 'WT.I.24')
```

```{r echo=FALSE}
plots4 <- ggarrange(ggplot(LFQ_WT_imp.boxcox) + 
                       geom_histogram(aes(x=WT.I.22), bins = 10, fill='red', alpha=0.4) + 
                       labs(x='Box-Cox.22',y='Frequency'),
                   ggplot(LFQ_WT_imp.boxcox) + 
                       geom_histogram(aes(x=WT.I.23), bins = 10, fill='green', alpha=0.4) + 
                       labs(x='Box-Cox.23',y=''),
                   ggplot(LFQ_WT_imp.boxcox) + 
                       geom_histogram(aes(x=WT.I.24), bins = 10, fill='blue', alpha=0.4) + 
                       labs(x='Box-Cox.24',y=''),
                   ncol=3)

plots4 <- annotate_figure(plots4, top=text_grob('Imputed Transformed Data with Box-Cox', 
                                     face='bold', size=13))
```


```{r echo=FALSE}
ggarrange(plots3, plots4, nrow=2, ncol=1)
```

```{r}
moments::skewness(cbind(LFQ_WT.boxcox, LFQ_WT_imp.boxcox), na.rm=TRUE)
```

And again the Box-Cox Tranformation gave us expected result. 
The distributions are similar to a normal distribution.

# Normalization - WT

## Z-score Normalization

```{r WT standardization}
LFQ_WT.standard <- as.data.frame(scale(LFQ_WT)) # Non-Imputed
LFQ_WT_imp.standard <- as.data.frame(scale(LFQ_WT_imp)) # Imputed
```

```{r echo=FALSE}
ggarrange(
    plothist(LFQ_WT.standard, 'Non-Imputed Transformed by Z-score'),
    plothist(LFQ_WT_imp.standard, 'Imputed Transformed by Z-score'),
    nrow=2, ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_WT.standard, LFQ_WT_imp.standard), na.rm=TRUE)
```

Unfortunately the standardization didn't help to bring the distribution closer to the normal dist.

## Min-max Normalization

```{r WT min-max normalization}
LFQ_WT.minmax <- as.data.frame(lapply(LFQ_WT, function(col) min_max_norm(col))) # Non-Imputed
LFQ_WT_imp.minmax <- as.data.frame(lapply(LFQ_WT_imp, function(col) min_max_norm(col))) # Imputed
```

```{r echo=FALSE}
ggarrange(
    plothist(LFQ_WT.minmax, 'Non-Imputed - Min-Max Normalization'),
    plothist(LFQ_WT_imp.minmax, 'imputed - Min-Max Normalization'),
    nrow=2,ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_WT.minmax, LFQ_WT_imp.minmax), na.rm=TRUE)
```

Unfortunately, again the normalization didn't help. ðŸ˜’

## Median Scaling

```{r WT median scaling}
LFQ_WT.med <- as.data.frame(DescTools::RobScale(LFQ_WT, scale = FALSE)) # Non-Imputed
LFQ_WT_imp.med <- as.data.frame(DescTools::RobScale(LFQ_WT_imp, scale = FALSE)) # Imputed
```

```{r echo=FALSE}
ggarrange(
    plothist(LFQ_WT.med, 'Non-Imputed - Median Scaling'),
    plothist(LFQ_WT_imp.med, 'Imputed - Median Scaling'),
    nrow=2, ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_WT.med,LFQ_WT_imp.med), na.rm=TRUE)
```

## MAD Scaling

```{r WT mad scaling}
LFQ_WT.mad <- as.data.frame(DescTools::RobScale(LFQ_WT)) # Non-Imputed
LFQ_WT_imp.mad <- as.data.frame(DescTools::RobScale(LFQ_WT_imp)) # Imputed
```

```{r echo=FALSE}
ggarrange(
    plothist(LFQ_WT.mad, 'Non-Imputed - MAD Scaling'),
    plothist(LFQ_WT_imp.mad,'Imputed - MAD Scaling'),
    nrow=2, ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_WT.mad,LFQ_WT_imp.mad), na.rm=TRUE)
```

## Linear Regression Normalization

```{r WT lm, echo=FALSE}
LFQ_WT_long <- LFQ_WT %>%
    pivot_longer(cols = 1:3, names_to = "Sample", values_to = "LFQValue")
#%>% mutate(celltype = rep(c(rep('WT', 3), rep('WT', 3)), nrow(lfq)))

model2 <- lm(LFQValue ~ Sample, data=LFQ_WT_long)
LFQ_WT_long$normalized <- NA
LFQ_WT_long[which(!is.na(LFQ_WT_long$LFQValue)), 'normalized'] <- residuals(model2)

LFQ_WT.lm <- LFQ_WT_long %>% 
    mutate(row = rep(1:nrow(LFQ_WT), each=3)) %>%
    reshape2::dcast(row ~ Sample, value.var='normalized') %>%
    select(-row)

LFQ_WT_long <- LFQ_WT_imp %>%
    pivot_longer(cols = 1:3, names_to = "Sample", values_to = "LFQValue") 
#%>% mutate(celltype = rep(c(rep('WT', 3), rep('WT', 3)), nrow(lfq)))

model2 <- lm(LFQValue ~ Sample, data=LFQ_WT_long)
LFQ_WT_long$normalized <- residuals(model2)

LFQ_WT_imp.lm <- LFQ_WT_long %>% 
    mutate(row = rep(1:nrow(LFQ_WT_imp), each=3)) %>%
    reshape2::dcast(row ~ Sample, value.var='normalized') %>%
    select(-row)

ggarrange(
    plothist(LFQ_WT.lm, 'Non-Imputed - Linear Normalization'),
    plothist(LFQ_WT_imp.lm, 'imputed - Linear Normalization'),
    nrow=2,ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_WT.lm, LFQ_WT_imp.lm), na.rm=TRUE)
```


# VSN 
VSN - Variance Stabilization Normalization

```{r vsn}
lfq_rglist<- new('RGList', list(
    R = as.matrix(LFQ_KO),
    G = as.matrix(LFQ_WT)))

lfq_rglist_imp<- new('RGList', list(
    R = as.matrix(LFQ_KO_imp),
    G = as.matrix(LFQ_WT_imp)))

LFQ.vsn <- justvsn(lfq_rglist)@assayData  # Non-Imputed
LFQ_imp.vsn <- justvsn(lfq_rglist_imp)@assayData # Imputed
```

```{r echo=FALSE}
ggarrange(
    ggarrange(plothist(LFQ.vsn$R, 'Non-Imputed Transformed by VSN')+
                  theme(legend.position = 'none')+xlab(''), 
              plothist(LFQ.vsn$G, '')+
                  theme(legend.position = 'none')+xlab('WT'),
              nrow=1,ncol=2),
    ggarrange(plothist(LFQ_imp.vsn$R, 'Imputed Transformed by VSN')+
                  theme(legend.position = 'none')+xlab('KO'), 
              plothist(LFQ_imp.vsn$G, '')+
                  theme(legend.position = 'none')+xlab('WT'),
              nrow=1,ncol=2),
    nrow=2,ncol=1
)
```

This transformation didn't help us too. Furthermore we lost differences between 
knockout and wild type intensity.

```{r}
# Non-imputed
moments::skewness(cbind(LFQ.vsn$R, LFQ.vsn$G), na.rm=TRUE)
```

```{r}
# Imputed
moments::skewness(cbind(LFQ_imp.vsn$R, LFQ_imp.vsn$G), na.rm=TRUE)
```

# `bestNormalize` Package for Automatic Normalization

## KO

**Experiment 22**

```{r autonorm KO 22}
best.KO.22 <- bestNormalize::bestNormalize(LFQ_KO$KO.22) # Non-Imputed
best.KO_imp.22 <- bestNormalize::bestNormalize(LFQ_KO_imp$KO.I.22) # Imputed
```

**Experiment 23**

```{r autonorm KO 23}
best.KO.23 <- bestNormalize::bestNormalize(LFQ_KO$KO.23) # Non-Imputed
best.KO_imp.23 <- bestNormalize::bestNormalize(LFQ_KO_imp$KO.I.23) # Imputed
```

**Experiment 24**

```{r autonorm KO 24}
best.KO.24 <- bestNormalize::bestNormalize(LFQ_KO$KO.24) # Non-Imputed
best.KO_imp.24 <- bestNormalize::bestNormalize(LFQ_KO_imp$KO.I.24) # Imputed
```

**Summary**

```{r echo=FALSE}
autonorm_ko <- data.frame(Rep_22=environment(best.KO.22[["norm_stat_fn"]])[["best_idx"]],
                          Rep_22_imp=environment(best.KO_imp.22[["norm_stat_fn"]])[["best_idx"]],
                          Rep_23=environment(best.KO.23[["norm_stat_fn"]])[["best_idx"]],
                          Rep_23_imp=environment(best.KO_imp.23[["norm_stat_fn"]])[["best_idx"]],
                          Rep_24=environment(best.KO.24[["norm_stat_fn"]])[["best_idx"]],
                          Rep_24_imp=environment(best.KO_imp.24[["norm_stat_fn"]])[["best_idx"]])

rownames(autonorm_ko) <- 'Method'
autonorm_ko
```

Based on the result for each column, the best method to normalize the data is 
Order Quntile Normalization for all of the samples:

```{r echo=FALSE}
# Non-Imputed
LFQ_KO.bestNorm <- data.frame('KO.22'=best.KO.22$x.t,'KO.23'=best.KO.23$x.t,'KO.24'=best.KO.24$x.t)
# Imputed
LFQ_KO_imp.bestNorm <- data.frame('KO.I.22'=best.KO_imp.22$x.t,'KO.I.23'=best.KO_imp.23$x.t,'KO.I.24'=best.KO_imp.24$x.t)

ggarrange(
    plothist(LFQ_KO.bestNorm, 'Non-Imputed - BestNormalize'),
    plothist(LFQ_KO_imp.bestNorm, 'Imputed - BestNormalize'),
    nrow=2,ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_KO.bestNorm, LFQ_KO_imp.bestNorm), na.rm = TRUE)
```

## WT

**Experiment 22**

```{r autonorm WT 22}
best.WT.22 <- bestNormalize::bestNormalize(LFQ_WT$WT.22) # Non-Imputed
best.WT_imp.22 <- bestNormalize::bestNormalize(LFQ_WT_imp$WT.I.22) # Imputed
```

**Experiment 23**

```{r autonorm WT 23}
best.WT.23 <- bestNormalize::bestNormalize(LFQ_WT$WT.23) # Non-Imputed
best.WT_imp.23 <- bestNormalize::bestNormalize(LFQ_WT_imp$WT.I.23) # Imputed
```

**Experiment 24**

```{r autonorm WT 24}
best.WT.24 <- bestNormalize::bestNormalize(LFQ_WT$WT.24) # Non-Imputed
best.WT_imp.24 <- bestNormalize::bestNormalize(LFQ_WT_imp$WT.I.24) # Imputed
```

**Summary**

```{r echo=FALSE}
autonorm_wt <- data.frame(Rep_22=environment(best.WT.22[["norm_stat_fn"]])[["best_idx"]],
                          Rep_22_imp=environment(best.WT_imp.22[["norm_stat_fn"]])[["best_idx"]],
                          Rep_23=environment(best.WT.23[["norm_stat_fn"]])[["best_idx"]],
                          Rep_23_imp=environment(best.WT_imp.23[["norm_stat_fn"]])[["best_idx"]],
                          Rep_24=environment(best.WT.24[["norm_stat_fn"]])[["best_idx"]],
                          Rep_24_imp=environment(best.WT_imp.24[["norm_stat_fn"]])[["best_idx"]])

rownames(autonorm_wt) <- 'Method'
autonorm_wt
```

Like earlier the best method for each column is Order Quantile Normalization.

```{r echo=FALSE}
# Non-Imputed
LFQ_WT.bestNorm <- data.frame('WT.22'=best.WT.22$x.t,'WT.23'=best.WT.23$x.t,'WT.24'=best.WT.24$x.t)
# Imputed
LFQ_WT_imp.bestNorm <- data.frame('WT.I.22'=best.WT_imp.22$x.t,'WT.I.23'=best.WT_imp.23$x.t,'WT.I.24'=best.WT_imp.24$x.t)

ggarrange(
    plothist(LFQ_WT.bestNorm, 'Non-Imputed - BestNormalize'),
    plothist(LFQ_WT_imp.bestNorm, 'Imputed - BestNormalize'),
    nrow=2,ncol=1
)
```

```{r}
moments::skewness(cbind(LFQ_WT.bestNorm, LFQ_WT_imp.bestNorm), na.rm = TRUE)
```


As we can see both KO and WT data have a normal distribution, but we have lost 
the differences between samples.

# EigenMS Non-Imputed

```{r EigenMS 1, results='hide', fig.keep='all'}
protein.groups <- readr::read_tsv('data/proteinGroups.txt', show_col_types = FALSE)
protein.groups <- protein.groups %>% filter(is.na(`Only identified by site`),
                         is.na(Reverse),
                         is.na(`Potential contaminant`))

# I needed unique values for each peptide, so I create artificial names `prot_1:5905`
prot.info <- data.frame(protein.groups[,"Peptide sequences"], paste('prot_', 1:nrow(lfq), sep = ''))
LFQ.eig1 <- eig_norm1(lfq, treatment = as.factor(c('KO', 'KO', 'KO', 'WT', 'WT', 'WT')), prot.info = prot.info)
```

```{r EigenMS 2, results='hide', fig.keep='all'}
# Performing eig normalization
LFQ.eig_norm <- eig_norm2(LFQ.eig1)
```

```{r boxplot EigenNorm}
par(mfcol=c(1,2))
boxplot(lfq, las=2, main='Raw intensities')
boxplot(LFQ.eig_norm$norm_m, las=2, main='Normalized intensities')
```

```{r}
ggarrange(
    plothist(LFQ.eig_norm$norm_m[,1:3], 'Non-Imputed EigenMS'),
    plothist(LFQ.eig_norm$norm_m[,4:6], ''),
    nrow=2,ncol=1
)
```


```{r}
moments::skewness(LFQ.eig_norm$norm_m, na.rm=TRUE)
```

The eigen normalization perfectly shows the differences between cell type, but
distributions are skewed all the time.

# EigenMS Imputed

```{r EigenMS 1 imp, results='hide', fig.keep='all'}
# I needed unique values for each peptide, so I create artificial names `prot_1:5905`
prot.info <- data.frame(protein.groups[,"Peptide sequences"], paste('prot_', 1:nrow(lfq_imp), sep = ''))
LFQ_imp.eig1 <- eig_norm1(lfq_imp, treatment = as.factor(c('KO', 'KO', 'KO', 'WT', 'WT', 'WT')), prot.info = prot.info)
```

```{r EigenMS 2 imp, results='hide', fig.keep='all'}
# Performing eig normalization
LFQ_imp.eig_norm <- eig_norm2(LFQ_imp.eig1)
```

```{r boxplot EigenNorm imp}
par(mfcol=c(1,2))
boxplot(lfq_imp, las=2, main='Raw intensities')
boxplot(LFQ_imp.eig_norm$norm_m, las=2, main='Normalized intensities')
```

```{r}
ggarrange(
    plothist(LFQ_imp.eig_norm$norm_m[,1:3], 'Imputed EigenMS'),
    plothist(LFQ_imp.eig_norm$norm_m[,4:6], ''),
    nrow=2,ncol=1
)
```


```{r}
moments::skewness(LFQ_imp.eig_norm$norm_m, na.rm=TRUE)
```


# Comparison

## Visualizations

### Non-Imputed

---

<center><b style='font-size:26px'>TRANSFORMATIONS</b></center>

---

```{r comparison trans KO, echo=FALSE}
plots_tKO <- ggarrange(ggarrange(plothist(LFQ_KO, '', FALSE) + xlab('Original'),
                                 plothist(LFQ_KO.log, '', FALSE) + labs(x='Logarithm',y=''),
                                 plothist(LFQ_KO.frac, '', FALSE) + labs(x='Fraction 1/x',y=''),
                                 nrow=1, ncol=3),
                       plots1, nrow=2,ncol=1)

annotate_figure(plots_tKO, top=text_grob('Comparison of Transformation Methods - KO',
                                         face='bold', size=13))
```

---

```{r comparison trans WT, echo=FALSE}
plots_tWT <- ggarrange(ggarrange(plothist(LFQ_WT, '', FALSE) + xlab('Original'),
                                 plothist(LFQ_WT.log, '', FALSE) + labs(x='Logarithm',y=''),
                                 plothist(LFQ_WT.frac, '', FALSE) + labs(x='Fraction 1/x',y=''),
                                 nrow=1, ncol=3),
                       plots3, nrow=2,ncol=1)

annotate_figure(plots_tWT, top=text_grob('Comparison of Transformation Methods - WT',
                                         face='bold', size=13))
```

---

<center><b style='font-size:26px'>NORMALIZATIONS</b></center>

---

```{r violin1, echo=FALSE}
plotviolin(lfq, 'Original') + 
    ggtitle('Comparison of Normalization Methods on Non-Imputed Data')
```

```{r violin2, echo=FALSE}
ggarrange(
    plotviolin(cbind(LFQ_KO.standard,LFQ_WT.standard), 'Z-score'),
    plotviolin(cbind(LFQ_KO.minmax,LFQ_WT.minmax), 'Min-Max', ''),
    plotviolin(cbind(LFQ_KO.med,LFQ_WT.med), 'Median Scaling'),
    plotviolin(cbind(LFQ_KO.mad,LFQ_WT.mad), 'MAD Scaling', ''),
    nrow=2, ncol=2
)
```

```{r violin3, echo=FALSE}
ggarrange(
    plotviolin(cbind(LFQ_KO.lm,LFQ_WT.lm), 'Linear Normalisation'),
    plotviolin(cbind(LFQ.vsn$R,LFQ.vsn$G), 'VSN', ''),
    plotviolin(cbind(LFQ_KO.bestNorm,LFQ_WT.bestNorm), '`bestNormalize` Package'),
    plotviolin(LFQ.eig_norm$norm_m, 'EigenMS', ''),
    nrow=2, ncol=2
)
```

---

<center><b style='font-size:26px'>HISTOGRAMS</b></center>

---

```{r comparison KO, echo=FALSE}
plots_KO <- ggarrange(plothist(LFQ_KO, '', FALSE) + xlab('Original'),
                      plothist(LFQ_KO.standard, '', FALSE) + labs(x='Z-score', y=''), 
                      plothist(LFQ_KO.minmax, '', FALSE) + labs(x='Min-Max', y=''),
                      plothist(LFQ_KO.med, '', FALSE) + xlab('Median Scaling'),
                      plothist(LFQ_KO.mad, '', FALSE) + labs(x='MAD Scaling', y=''),
                      plothist(LFQ_KO.lm, '', FALSE) + labs(x='Linear Norm', y=''),
                      plothist(LFQ.vsn$R, '', FALSE) + xlab('VSN'),
                      plothist(LFQ_KO.bestNorm, '', FALSE) + labs(x='bestNormalize Pkg', y=''),
                      plothist(LFQ.eig_norm$norm_m[,1:3], '', FALSE) + labs(x='EigenMS', y=''),
                      nrow=3, ncol=3)

annotate_figure(plots_KO, top=text_grob('Comparison of Normalization Methods - KO', 
                                     face='bold', size=13))
```

---

```{r comparison WT, echo=FALSE}
plots_WT <- ggarrange(plothist(LFQ_WT, '', FALSE) + xlab('Original'),
                      plothist(LFQ_WT.standard, '', FALSE) + labs(x='Z-score', y=''), 
                      plothist(LFQ_WT.minmax, '', FALSE) + labs(x='Min-Max', y=''),
                      plothist(LFQ_WT.med, '', FALSE) + xlab('Median Scaling'),
                      plothist(LFQ_WT.mad, '', FALSE) + labs(x='MAD Scaling', y=''),
                      plothist(LFQ_WT.lm, '', FALSE) + labs(x='Linear Norm', y=''),
                      plothist(LFQ.vsn$G, '', FALSE) + xlab('VSN'),
                      plothist(LFQ_WT.bestNorm, '', FALSE) + labs(x='bestNormalize Pkg', y=''),
                      plothist(LFQ.eig_norm$norm_m[,4:6], '', FALSE) + labs(x='EigenMS', y=''),
                      nrow=3, ncol=3)

annotate_figure(plots_WT, top=text_grob('Comparison of Normalization Methods - WT', 
                                     face='bold', size=13))
```

### Imputed

---

<center><b style='font-size:26px'>TRANSFORMATIONS</b></center>

---

```{r comparison trans KO imp, echo=FALSE}
plots_tKO_imp <- ggarrange(ggarrange(plothist(LFQ_KO_imp, '', FALSE) + xlab('Original'),
                                     plothist(LFQ_KO_imp.log, '', FALSE) + labs(x='Logarithm',y=''),
                                     plothist(LFQ_KO_imp.frac, '', FALSE) + labs(x='Fraction 1/x',y=''),
                                     nrow=1, ncol=3),
                           plots2, nrow=2,ncol=1)

annotate_figure(plots_tKO_imp, top=text_grob('Comparison of Transformation Methods - KO',
                                             face='bold', size=13))
```

---

```{r comparison trans WT imp, echo=FALSE}
plots_tWT_imp <- ggarrange(ggarrange(plothist(LFQ_WT_imp, '', FALSE) + xlab('Original'),
                                     plothist(LFQ_WT_imp.log, '', FALSE) + labs(x='Logarithm',y=''),
                                     plothist(LFQ_WT_imp.frac, '', FALSE) + labs(x='Fraction 1/x',y=''),
                                     nrow=1, ncol=3),
                           plots4, nrow=2,ncol=1)

annotate_figure(plots_tWT_imp, top=text_grob('Comparison of Transformation Methods - WT',
                                             face='bold', size=13))
```

---

<center><b style='font-size:26px'>NORMALIZATIONS</b></center>

---

```{r violin1 imp, echo=FALSE}
plotviolin(lfq_imp, 'Original') + 
    ggtitle('Comparison of Normalization Methods on Imputed Data')
```

```{r violin2 imp, echo=FALSE}
ggarrange(
    plotviolin(cbind(LFQ_KO_imp.standard,LFQ_WT_imp.standard), 'Z-score'),
    plotviolin(cbind(LFQ_KO_imp.minmax,LFQ_WT_imp.minmax), 'Min-Max', ''),
    plotviolin(cbind(LFQ_KO_imp.med,LFQ_WT_imp.med), 'Median Scaling'),
    plotviolin(cbind(LFQ_KO_imp.mad,LFQ_WT_imp.mad), 'MAD Scaling', ''),
    nrow=2, ncol=2
)
```

```{r violin3 imp, echo=FALSE}
ggarrange(
    plotviolin(cbind(LFQ_KO_imp.lm,LFQ_WT_imp.lm), 'Linear Normalisation'),
    plotviolin(cbind(LFQ_imp.vsn$R,LFQ_imp.vsn$G), 'VSN', ''),
    plotviolin(cbind(LFQ_KO_imp.bestNorm,LFQ_WT_imp.bestNorm), '`bestNormalize` Package'),
    plotviolin(LFQ_imp.eig_norm$norm_m, 'EigenMS', ''),
    nrow=2, ncol=2
)
```

---

<center><b style='font-size:26px'>HISTOGRAMS</b></center>

---

```{r comparison KO imp, echo=FALSE}
plots_KO_imp <- ggarrange(plothist(LFQ_KO_imp, '', FALSE) + xlab('Original'),
                      plothist(LFQ_KO_imp.standard, '', FALSE) + labs(x='Z-score', y=''), 
                      plothist(LFQ_KO_imp.minmax, '', FALSE) + labs(x='Min-Max', y=''),
                      plothist(LFQ_KO_imp.med, '', FALSE) + xlab('Median Scaling'),
                      plothist(LFQ_KO_imp.mad, '', FALSE) + labs(x='MAD Scaling', y=''),
                      plothist(LFQ_KO_imp.lm, '', FALSE) + labs(x='Linear Norm', y=''),
                      plothist(LFQ_imp.vsn$R, '', FALSE) + xlab('VSN'),
                      plothist(LFQ_KO_imp.bestNorm, '', FALSE) + labs(x='bestNormalize Pkg', y=''),
                      plothist(LFQ_imp.eig_norm$norm_m[,1:3], '', FALSE) + labs(x='EigenMS', y=''),
                      nrow=3, ncol=3)

annotate_figure(plots_KO_imp, top=text_grob('Comparison of Normalization Methods - KO', 
                                     face='bold', size=13))
```

---

```{r comparison WT imp, echo=FALSE}
plots_WT_imp <- ggarrange(plothist(LFQ_WT_imp, '', FALSE) + xlab('Original'),
                      plothist(LFQ_WT_imp.standard, '', FALSE) + labs(x='Z-score', y=''), 
                      plothist(LFQ_WT_imp.minmax, '', FALSE) + labs(x='Min-Max', y=''),
                      plothist(LFQ_WT_imp.med, '', FALSE) + xlab('Median Scaling'),
                      plothist(LFQ_WT_imp.mad, '', FALSE) + labs(x='MAD Scaling', y=''),
                      plothist(LFQ_WT_imp.lm, '', FALSE) + labs(x='Linear Norm', y=''),
                      plothist(LFQ_imp.vsn$G, '', FALSE) + xlab('VSN'),
                      plothist(LFQ_WT_imp.bestNorm, '', FALSE) + labs(x='bestNormalize Pkg', y=''),
                      plothist(LFQ_imp.eig_norm$norm_m[,4:6], '', FALSE) + labs(x='EigenMS', y=''),
                      nrow=3, ncol=3)

annotate_figure(plots_WT_imp, top=text_grob('Comparison of Normalization Methods - WT', 
                                     face='bold', size=13))
```

## Statistical Metrics

**COEFFICIENT OF VARIATION (CV) / RELATIVE STANDARD DEVIATION (RSD)**

This is a way to measure how spread out values are in a dataset relative to the mean.
A lower RSD/CV indicates better normalization. It is calculated as:

$CV = \frac{\sigma}{\mu}$

where:

* $\sigma$: The standard deviation of dataset
* $\mu$: The mean of dataset

### Non-Imputed CV

```{r cv, echo=FALSE}
cv <- function (x) sd(x) / mean(x) * 100
exp <- c('KO.22', 'KO.23', 'KO.24', 'WT.22', 'WT.23', 'WT.24')

cv.before <- data.frame(KO=apply(2^LFQ_KO, 1, cv), WT=apply(2^LFQ_WT,1,cv))
cv.standard <- data.frame(KO=apply(2^LFQ_KO.standard, 1, cv), WT=apply(2^LFQ_WT.standard, 1, cv))
cv.minmax <- data.frame(KO=apply(2^LFQ_KO.minmax, 1, cv), WT= apply(2^LFQ_WT.minmax, 1, cv))
cv.med <- data.frame(KO=apply(2^LFQ_KO.med, 1, cv), WT=apply(2^LFQ_WT.med, 1, cv))
cv.mad <- data.frame(KO=apply(2^LFQ_KO.mad, 1, cv), WT=apply(2^LFQ_WT.mad, 1, cv))
cv.lm <- data.frame(KO=apply(2^LFQ_KO.lm, 1, cv), WT=apply(2^LFQ_WT.lm, 1, cv))
cv.vsn <- data.frame(KO=apply(2^LFQ.vsn$R, 1, cv), WT=apply(2^LFQ.vsn$G, 1, cv))
cv.bestnorm <- data.frame(KO=apply(2^LFQ_KO.bestNorm, 1, cv), WT=apply(2^LFQ_WT.bestNorm, 1, cv))
cv.eig <- data.frame(KO=apply(2^LFQ.eig_norm$norm_m[,1:3], 1, cv), WT=apply(2^LFQ.eig_norm$norm_m[,4:6], 1, cv))

vplots1 <- ggarrange(
    plotoneviolin(cv.before, 'Original'),
    plotoneviolin(cv.minmax, 'MinMax'),
    plotoneviolin(cv.vsn, 'VSN'),
    plotoneviolin(cv.eig, 'EigenMS'),
    nrow=2,ncol=2
)

annotate_figure(vplots1, 
                top=text_grob('Violin plots of LFQ CV [%] - comparison of normalization methods', 
                              face = 'bold'))
```

```{r echo=FALSE}
ggarrange(
    plotoneviolin(cv.standard, 'Z-score'),
    plotoneviolin(cv.med, 'Median'),
    plotoneviolin(cv.mad, 'MAD'),
    plotoneviolin(cv.lm, 'Linear'),
    plotoneviolin(cv.bestnorm, 'bestNormalize Package'),
    nrow=2, ncol=3
)
```

<center>
**SUMMARY OF COEFFICIENT OF VARIATION**
</center>

```{r cv summary, echo=FALSE}
cv_all <- data.frame(cv.before, cv.standard, cv.minmax, cv.med, cv.mad, cv.lm, cv.vsn, cv.bestnorm)
colnames(cv_all) <- c('Before_KO', 'Before_WT', 'Z-score_KO', 'Z-score_WT',
                      'MinMax_KO', 'MinMax_WT', 'Median_KO', 'Median_WT', 
                      'MAD_KO', 'MAD_WT', 'Linear_KO', 'Linear_WT', 
                      'VSN_KO', 'VSN_WT', 'bestNormalize_KO', 'bestNormalize_WT')
colnames(cv.eig) <- c('EigenMS_KO', 'EigenMS_WT')

cv_summary_KO <- cv_all %>%
    select(contains('KO')) %>%
    pivot_longer(everything(), names_to = 'method KO', values_to = 'LFQ_CV') %>%
    group_by(`method KO`) %>%
    summarise('mean_KO'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_KO'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_KO'=round(sd(LFQ_CV, na.rm=TRUE),5))

summary_eigen_KO <- tibble(cv.eig) %>%
    select(contains('KO')) %>%
    pivot_longer(everything(), names_to = 'method KO', values_to = 'LFQ_CV') %>%
    group_by(`method KO`) %>%
    summarise('mean_KO'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_KO'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_KO'=round(sd(LFQ_CV, na.rm=TRUE),5))

cv_summary_WT <- cv_all %>%
    select(contains('WT')) %>%
    pivot_longer(everything(), names_to = 'method WT', values_to = 'LFQ_CV') %>%
    group_by(`method WT`) %>%
    summarise('mean_WT'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_WT'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_WT'=round(sd(LFQ_CV, na.rm=TRUE),5))

summary_eigen_WT <- tibble(cv.eig) %>%
    select(contains('WT')) %>%
    pivot_longer(everything(), names_to = 'method WT', values_to = 'LFQ_CV') %>%
    group_by(`method WT`) %>%
    summarise('mean_WT'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_WT'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_WT'=round(sd(LFQ_CV, na.rm=TRUE),5))
```

<center>
```{r echo=FALSE}
cv_imp_summary <- as.data.frame(cbind(rbind(cv_summary_KO, summary_eigen_KO),
                                      rbind(cv_summary_WT, summary_eigen_WT)))
colnames(cv_imp_summary)[1] <- 'method'
cv_imp_summary <- cv_imp_summary[,-5]
cv_imp_summary$method <- gsub('_KO', '', cv_imp_summary$method)

DT::datatable(cv_imp_summary, rownames = FALSE, 
              options=list(searching=FALSE, paging=FALSE, info=FALSE)) %>%
    DT::formatStyle(columns = 'mean_KO', 
                    background = DT::styleInterval(c(-5,5), c('white','lightgreen', 'white'))) %>%
    DT::formatStyle(columns = 'mean_WT', 
                    background = DT::styleInterval(c(-5,5), c('white','lightgreen', 'white'))) %>%
    DT::formatStyle(columns = 'median_KO', 
                    background = DT::styleInterval(c(-5,5), c('white','lightgreen', 'white'))) %>%
    DT::formatStyle(columns = 'median_WT', 
                    background = DT::styleInterval(c(-5,5), c('white','lightgreen', 'white')))
   

```
</center>

<br>As we can see above, the best methods were:

* Min-Max Normalization
* VSN
* EigenMS

Coefficient of Variation for the rest of the methods have a poor performance.

### Imputed CV

```{r cv imp, echo=FALSE}
cv_imp.before <- data.frame(KO=apply(2^LFQ_KO_imp, 1, cv), WT=apply(2^LFQ_WT_imp,1,cv))
cv_imp.standard <- data.frame(KO=apply(2^LFQ_KO_imp.standard, 1, cv), WT=apply(2^LFQ_WT_imp.standard, 1, cv))
cv_imp.minmax <- data.frame(KO=apply(2^LFQ_KO_imp.minmax, 1, cv), WT= apply(2^LFQ_WT_imp.minmax, 1, cv))
cv_imp.med <- data.frame(KO=apply(2^LFQ_KO_imp.med, 1, cv), WT=apply(2^LFQ_WT_imp.med, 1, cv))
cv_imp.mad <- data.frame(KO=apply(2^LFQ_KO_imp.mad, 1, cv), WT=apply(2^LFQ_WT_imp.mad, 1, cv))
cv_imp.lm <- data.frame(KO=apply(2^LFQ_KO_imp.lm, 1, cv), WT=apply(2^LFQ_WT_imp.lm, 1, cv))
cv_imp.vsn <- data.frame(KO=apply(2^LFQ_imp.vsn$R, 1, cv), WT=apply(2^LFQ_imp.vsn$G, 1, cv))
cv_imp.bestnorm <- data.frame(KO=apply(2^LFQ_KO_imp.bestNorm, 1, cv), WT=apply(2^LFQ_WT_imp.bestNorm, 1, cv))
cv_imp.eig <- data.frame(KO=apply(2^LFQ_imp.eig_norm$norm_m[,1:3], 1, cv), WT=apply(2^LFQ_imp.eig_norm$norm_m[,4:6], 1, cv))

vplots2 <- ggarrange(
    plotoneviolin(cv_imp.before, 'Original'),
    plotoneviolin(cv_imp.minmax, 'MinMax'),
    plotoneviolin(cv_imp.vsn, 'VSN'),
    plotoneviolin(cv_imp.eig, 'EigenMS'),
    nrow=2,ncol=2
)

annotate_figure(vplots2, 
                top=text_grob('Violin plots of LFQ CV [%] - comparison of normalization methods', 
                              face = 'bold'))
```


```{r echo=FALSE}
ggarrange(
    plotoneviolin(cv_imp.standard, 'Z-score'),
    plotoneviolin(cv_imp.med, 'Median'),
    plotoneviolin(cv_imp.mad, 'MAD'),
    plotoneviolin(cv_imp.lm, 'Linear'),
    plotoneviolin(cv_imp.bestnorm, 'bestNormalize Package'),
    nrow=2, ncol=3
)
```

<center>
**SUMMARY OF COEFFICIENT OF VARIATION**
</center>

```{r cv summary imp,  echo=FALSE}
cv_imp_all <- data.frame(cv_imp.before, cv_imp.standard, cv_imp.minmax, cv_imp.med,
                         cv_imp.mad, cv_imp.lm, cv_imp.vsn, cv_imp.bestnorm)
colnames(cv_imp_all) <- c('Before_KO', 'Before_WT', 'Z-score_KO', 'Z-score_WT',
                          'MinMax_KO', 'MinMax_WT', 'Median_KO', 'Median_WT', 
                          'MAD_KO', 'MAD_WT', 'Linear_KO', 'Linear_WT', 
                          'VSN_KO', 'VSN_WT', 'bestNormalize_KO', 'bestNormalize_WT')
colnames(cv_imp.eig) <- c('EigenMS_KO', 'EigenMS_WT')

cv_imp_summary_KO <- cv_imp_all %>%
    select(contains('KO')) %>%
    pivot_longer(everything(), names_to = 'method KO', values_to = 'LFQ_CV') %>%
    group_by(`method KO`) %>%
    summarise('mean_KO'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_KO'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_KO'=round(sd(LFQ_CV, na.rm=TRUE),5))

summary_imp_eigen_KO <- tibble(cv_imp.eig) %>%
    select(contains('KO')) %>%
    pivot_longer(everything(), names_to = 'method KO', values_to = 'LFQ_CV') %>%
    group_by(`method KO`) %>%
    summarise('mean_KO'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_KO'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_KO'=round(sd(LFQ_CV, na.rm=TRUE),5))

cv_imp_summary_WT <- cv_imp_all %>%
    select(contains('WT')) %>%
    pivot_longer(everything(), names_to = 'method WT', values_to = 'LFQ_CV') %>%
    group_by(`method WT`) %>%
    summarise('mean_WT'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_WT'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_WT'=round(sd(LFQ_CV, na.rm=TRUE),5))

summary_imp_eigen_WT <- tibble(cv_imp.eig) %>%
    select(contains('WT')) %>%
    pivot_longer(everything(), names_to = 'method WT', values_to = 'LFQ_CV') %>%
    group_by(`method WT`) %>%
    summarise('mean_WT'=round(mean(LFQ_CV, na.rm=TRUE),5), 
              'median_WT'=round(median(LFQ_CV, na.rm=TRUE),5), 
              'sd_WT'=round(sd(LFQ_CV, na.rm=TRUE),5))
```

<center>
```{r echo=FALSE}
cv_imp_summary <- as.data.frame(cbind(rbind(cv_imp_summary_KO, summary_imp_eigen_KO),
                                      rbind(cv_imp_summary_WT, summary_imp_eigen_WT)))
colnames(cv_imp_summary)[1] <- 'method'
cv_imp_summary <- cv_imp_summary[,-5]
cv_imp_summary$method <- gsub('_KO', '', cv_imp_summary$method)

DT::datatable(cv_imp_summary, rownames = FALSE, 
              options=list(searching=FALSE, paging=FALSE, info=FALSE)) %>%
    DT::formatStyle(columns = 'mean_KO', 
                    background = DT::styleInterval(c(-5,5), c('white','lightgreen', 'white'))) %>%
    DT::formatStyle(columns = 'mean_WT', 
                    background = DT::styleInterval(c(-5,5), c('white','lightgreen', 'white'))) %>%
    DT::formatStyle(columns = 'median_KO', 
                    background = DT::styleInterval(c(-5,5), c('white','lightgreen', 'white'))) %>%
    DT::formatStyle(columns = 'median_WT', 
                    background = DT::styleInterval(c(-5,5), c('white','lightgreen', 'white')))
    
```
</center>

<br>
**REPRODUCIBILITY OF BIOLOGICAL REPLICATES**

After normalization, biological replicates should group more tightly. You can 
assess this by measuring the intraclass correlation coefficient (ICC) to see if 
replicates cluster together.

A guidelines for interpretation by [Koo and Li (2016)](https://doi.org/10.1016%2Fj.jcm.2016.02.012):

* below 0.50: poor <span style="color:#D3D3D3;font-size:16px">â– </span>
* between 0.50 and 0.75: moderate <span style="color:#aefda1;font-size:16px">â– </span>
* between 0.75 and 0.90: good <span style="color:#6dff54;font-size:16px">â– </span>
* above 0.90: excellent <span style="color:#1bb400;font-size:16px">â– </span>

### Non-Imputed ICC

```{r icc KO, echo=FALSE}
icc.KO.before <- irr::icc(2^LFQ_KO)$value
icc.KO.standard <- irr::icc(2^LFQ_KO.standard)$value
icc.KO.minmax <- irr::icc(2^LFQ_KO.minmax)$value
icc.KO.med <- irr::icc(2^LFQ_KO.med)$value
icc.KO.mad <- irr::icc(2^LFQ_KO.mad)$value
icc.KO.lm <- irr::icc(2^LFQ_KO.lm)$value
icc.KO.vsn <- irr::icc(2^LFQ.vsn$R)$value
icc.KO.ordernorm <- irr::icc(2^LFQ_KO.bestNorm)$value
icc.KO.eig <- irr::icc(2^LFQ.eig_norm$norm_m[,1:3])$value

icc.KO.all <- data.frame(`ICC KO`=c(icc.KO.before, icc.KO.standard, icc.KO.minmax, icc.KO.med, 
                            icc.KO.mad, icc.KO.lm, icc.KO.vsn, icc.KO.ordernorm, icc.KO.eig))
rownames(icc.KO.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'bestNormalize', 'EigenMS')
```

```{r icc WT, echo=FALSE}
icc.WT.before <- irr::icc(2^LFQ_WT)$value
icc.WT.standard <- irr::icc(2^LFQ_WT.standard)$value
icc.WT.minmax <- irr::icc(2^LFQ_WT.minmax)$value
icc.WT.med <- irr::icc(2^LFQ_WT.med)$value
icc.WT.mad <- irr::icc(2^LFQ_WT.mad)$value
icc.WT.lm <- irr::icc(2^LFQ_WT.lm)$value
icc.WT.vsn <- irr::icc(2^LFQ.vsn$G)$value
icc.WT.ordernorm <- irr::icc(2^LFQ_WT.bestNorm)$value
icc.WT.eig <- irr::icc(2^LFQ.eig_norm$norm_m[,4:6])$value

icc.WT.all <- data.frame(`ICC WT`=c(icc.WT.before, icc.WT.standard, icc.WT.minmax, icc.WT.med, 
                            icc.WT.mad, icc.WT.lm, icc.WT.vsn, icc.WT.ordernorm, icc.WT.eig))
rownames(icc.WT.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'bestNormalize', 'EigenMS')
```

<center>
```{r echo=FALSE}
DT::datatable(cbind(icc.KO.all, icc.WT.all), 
              options=list(searching=FALSE, paging=FALSE, info=FALSE)) %>%
    DT::formatStyle(columns = 'ICC.KO', 
                    background = DT::styleInterval(c(0.5,0.75,0.9), c('#D3D3D3', '#aefda1', '#6dff54', '#1bb400'))) %>%
    DT::formatStyle(columns = 'ICC.WT', 
                    background = DT::styleInterval(c(0.5,0.75,0.9), c('#D3D3D3', '#aefda1', '#6dff54', '#1bb400')))
```
</center>

### Imputed ICC

```{r icc imp KO, echo=FALSE}
icc.KO.before <- irr::icc(2^LFQ_KO_imp)$value
icc.KO.standard <- irr::icc(2^LFQ_KO_imp.standard)$value
icc.KO.minmax <- irr::icc(2^LFQ_KO_imp.minmax)$value
icc.KO.med <- irr::icc(2^LFQ_KO_imp.med)$value
icc.KO.mad <- irr::icc(2^LFQ_KO_imp.mad)$value
icc.KO.lm <- irr::icc(2^LFQ_KO_imp.lm)$value
icc.KO.vsn <- irr::icc(2^LFQ_imp.vsn$R)$value
icc.KO.ordernorm <- irr::icc(2^LFQ_KO_imp.bestNorm)$value
icc.KO.eig <- irr::icc(2^LFQ_imp.eig_norm$norm_m[,1:3])$value

icc_imp.KO.all <- data.frame(`ICC KO`=c(icc.KO.before, icc.KO.standard, icc.KO.minmax, icc.KO.med,
                                   icc.KO.mad, icc.KO.lm, icc.KO.vsn, icc.KO.ordernorm, icc.KO.eig))
rownames(icc_imp.KO.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 
                              'Linear', 'VSN', 'bestNormalize', 'EigenMS')
```

```{r icc imp WT, echo=FALSE}
icc.WT.before <- irr::icc(2^LFQ_WT_imp)$value
icc.WT.standard <- irr::icc(2^LFQ_WT_imp.standard)$value
icc.WT.minmax <- irr::icc(2^LFQ_WT_imp.minmax)$value
icc.WT.med <- irr::icc(2^LFQ_WT_imp.med)$value
icc.WT.mad <- irr::icc(2^LFQ_WT_imp.mad)$value
icc.WT.lm <- irr::icc(2^LFQ_WT_imp.lm)$value
icc.WT.vsn <- irr::icc(2^LFQ_imp.vsn$G)$value
icc.WT.ordernorm <- irr::icc(2^LFQ_WT_imp.bestNorm)$value
icc.WT.eig <- irr::icc(2^LFQ_imp.eig_norm$norm_m[,4:6])$value

icc_imp.WT.all <- data.frame(`ICC WT`=c(icc.WT.before, icc.WT.standard, icc.WT.minmax, icc.WT.med, 
                            icc.WT.mad, icc.WT.lm, icc.WT.vsn, icc.WT.ordernorm, icc.WT.eig))
rownames(icc_imp.WT.all) <- c('Before', 'Z-score', 'MinMax', 'Median', 'MAD', 'Linear', 
                       'VSN', 'bestNormalize', 'EigenMS')
```

<center>
```{r echo=FALSE}
DT::datatable(cbind(icc_imp.KO.all, icc_imp.WT.all), 
              options=list(searching=FALSE, paging=FALSE, info=FALSE)) %>%
    DT::formatStyle(columns = 'ICC.KO', 
                    background = DT::styleInterval(c(0.5,0.75,0.9), c('#D3D3D3', '#aefda1', '#6dff54', '#1bb400'))) %>%
    DT::formatStyle(columns = 'ICC.WT', 
                    background = DT::styleInterval(c(0.5,0.75,0.9), c('#D3D3D3', '#aefda1', '#6dff54', '#1bb400')))
```
</center>

# Conclusion

All normalization methods worked quite well. The skewness problem was solved 
with the Box-Cox transformation, and with transformed data we can normalize 
it with all methods.

The method chosen with `bestNormalize` gave us a perfect shape of a distribution,
but we lost the main differences between samples. In my opinion the best result
gave the **Eigen Normalization** method, because this method kept a characteristic
of a sample and this is very important in the analysis of biological data.

---

# References

1. [A systematic evaluation of normalization methods in quantitative label-free proteomics](https://academic.oup.com/bib/article/19/1/1/2562889)
2. [bestNormalize R Package](https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html#the-ordered-quantile-technique)
3. [How to Normalize Data in R for my Data: Methods and Examples](https://rpubs.com/zubairishaq9/how-to-normalize-data-r-my-data)
4. [EigenMS](https://sourceforge.net/projects/eigenms/)
5. [Normalization of peak intensities in bottom-up MS-based proteomics using singular value decomposition](https://pubmed.ncbi.nlm.nih.gov/19602524/)
